# Transformer前传

* [返回上层目录](../transformer.md)



# Transformer的诞生

你可能无法相信这个辍学放弃读博的年轻人。2012年加入谷歌，命运指引他成为**Transformer**的作者之一。也就是这一年，**诺贝尔和图灵奖双料得主**的**辛顿**，带着两位爱徒，用英伟达的GPU拿下视觉识别比赛的第一名，并加入谷歌阵营。自此深度学习技术AlexNet，让人工智能有实现AGI的可能性。

早期研究者使用当时主流的深度学习技术在邮件自动完成和客服聊天机器人等领域。直到2014年，一种名为**注意力机制**的出现，注意力模型可能更快、更高效，更适合GPU。于是他说服了几位同事一起探索这个新方向，之后研究小有成果，在2016年发表了相关研究论文。

不过**Uszkoreit**认为注意力机制远不止如此。16年的某一天，在午餐时偶遇了Ilya Polosukhin，后者正在烦恼如何直接为搜索问题提供答案。

之后Yoshkarite向Polosukhin推荐注意力机制，Polosukhin一看好家伙，成败在此一举，顺带邀请三哥Ishis Veswani一同加入，三人一起写了设计文档。

因为Paulusukin喜欢变形金刚，名字草率的就叫**Transformer**。2017年，三哥Niki Palmer和英国的Lion Jones加入了团队，之后Lakesh Kaiser带着他的实习生Aidan Gomez也加入了进来。接着团队决定将Transformer重点用在翻译上。起初Transformer和LSTM差不多，没什么突出，但命运推着它往前走。

在谷歌工作十几年的老油条Norm Shazier，偶然经过Kaiser的工位时，听到Transformer的未来，心想好家伙，这种流芳百世的事不叫我。Kaiser顺势给了他个台阶，就这样Shazier加入了团队。**Transformer八大金刚终于集齐，打开人工智能通往AGI之路**。

Shazier加入后，一改以往养老文化，玩命重写整个代码，把系统提升到了一个新的高度。在2017年的NeurIPS会议前夕，离提交论文的最后两周，团队靠打鸡血、喝咖啡续着命。直到论文提交截止前的几分钟，他们还在收集实验结果。论文在最后两分钟提交成功。因为Jones喜欢披头士，标题也草率来自歌曲《All You Need Is Love》。不负众望。

这篇论文在12月的NIPS会议上引起了轰动。事后几位作者回忆，现场的科学家就像送鸡蛋的超市，那疯狂程度此生未见。

他们嗓子都喊哑了，最后还是保安清场的。这篇论文后来被引用超过了11万次。**Transformer的成功离不开辛顿对神经网络的贡献**，加速人工智能的起点，让AI有机会实现AGI。

更是一句话，让AI军火商黄仁勋见到都要两眼泪汪汪的恩人。尽管这些创新在谷歌孕育而生，最终却没有在谷歌开花结果。**奥特曼曾说，谷歌当时没有人意识到Transformer意味着什么**，就这样旧的成功因素一点一点失去，新机会鬼使神差地错过。

就像谷歌前CEO施密特说的，**谷歌养老文化导致又佛又拉胯**，一周到公司一天，以致Transformer在谷歌开花，却在小公司的OpenAI结果，甚至被马斯克这种997的公司卷没了。



# 参考资料

* [除了 Transformer，还有哪些值得关注的 NLP 模型架构或技术路线？ 楚歌的回答](https://www.zhihu.com/question/1957039735286535291/answer/1957182944918606984)

“Transformer的诞生”一节参考此知乎回答。

