# 模型压缩概述

- [返回上层目录](../model-compression.md)



模型压缩- 剪枝/量化/蒸馏/AutoML
原因：深度学习计算复杂度高，参数冗余。
解决方式：
（1）线性或非线性量化。
（2）结构或非结构剪枝。
（3）网络结构搜索。
（4）权重矩阵的低秩分解。（蒸馏）
目的：
优化精度、性能、存储……使得可以在一些场景和设备上进行相应模型的部署。

1. 剪枝
（1）剪枝位置的判定一般根据权重。权重越小，证明该神经元的作用越小。
（2）剪枝的方式：删去网络层上的权重的向量/整个神经元/单个像素（数据）。由于矩阵操作的并行化，减去单个像素或者向量并不能减少计算量。即有的硬件并不支持稀疏矩阵的运输，所以一般剪枝操作是直接减去整个神经元。
（3）判别剪神经元的位置，神经元激活后的数值，越接近0越没用。
（4）剪枝流程：训练、剪枝、得到权重、再训练。
（5）训练技巧：由于要进行剪枝，训练过程的优化器不能太剧烈，也不能太温柔。否则会破坏已有学到的东西。常用SGD优化器（较温和）。Adam为比较距离的优化器。
（6）剪枝方式：
① 根据某种规则，按像素位置随机剪枝。
② 根据某种规则，按向量随机剪枝。
③ 再卷积核上做剪枝。（根据卷积核模的大小，判定剪枝位置）
④ 直接随机减去通道。
⑥ 剪枝分为：结构式剪枝和非结构式剪枝。
（7）实现原理：压低权重，根据权重大的数据进行保留，没用的数值权重越来越小，逐渐消失。L1正则化，对数据的净输出做正则化。（净输出做归一化（norm）：使得一部分权重压低后，另一部分的权重会升高。）

2. 量化
（1）基础理论：
精度：常规精度一般为FP32，存储模型权重；低精度一般为FP16,INT8……计算速度快。
混合精度：在模型中使用FP32和FP16。FP16减小了一半内存，但是有些参数和操作符必须用INT8。
量化的原理：量化一般值INT8，即把权重映射到INT8的范围之间，计算速度快。（量化的映射范围一般是不等分的，由于权重一般较小，在原点处可以近似看成等分。）
（2）根据权重存储分为：二值神经网络、三元权重网络、XNOR网络。
（3）在工业上一般用FP32对模型进行训练（追求精度），对推理部分用INT8（提高性能）。
（4）代码基本步骤：详细可通过官方文档进行学习
① 网络打包分块（将一个网络子块进行打包后，一起量化。例如：conv + BN + Relu糅合成一个模块。）——这里注意要使用量化支持的板块。
② 准备评估工具。（例如：top1、top5精确度、耗时、存储大小……）
③ 对原始网络进行训练。（先训练，后量化）
④ 开始量化。（量化前和量化后的评估指标作对比）
⑤ 做QAT伪训练。（量化后可能精确度下降，根据性能要求，再做训练）

3. 蒸馏
    （1）蒸馏，又叫做老师学生模型，属于迁移学习。
    蒸馏的原理：先预训练一个大模型，用大模型教小模型（大模型的结果在神经元的级别上作为小模型的先验），使得小模型有大模型的精度，性能又比大模型高。
    （2）常用方法：博主这篇写的很好
    ————————————————
    版权声明：本文为CSDN博主「什么都一般的咸鱼」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
    原文链接：https://blog.csdn.net/weixin_41809530/article/details/106377921


