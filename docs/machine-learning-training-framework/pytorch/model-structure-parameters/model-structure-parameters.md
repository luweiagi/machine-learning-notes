#  æ¨¡å‹ç»“æ„å‚æ•°åŠæ˜¾ç¤º

* [è¿”å›ä¸Šå±‚ç›®å½•](../pytorch.md)

æˆ‘æ¥åˆ†åˆ«æ¼”ç¤ºä¸€ä¸‹ `model.parameters()`ã€`model.named_parameters()` å’Œ `optimizer.param_groups` çš„ç»“æ„å’Œæ‰“å°ç»“æœï¼Œè¿™æ ·ä½ ä¹‹åè°ƒè¯•ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ã€å‚æ•°ç»„ã€å†»ç»“å‚æ•°ç­‰ä¼šæ›´æ¸…æ™°ï¼

æˆ‘ä»¬ç”¨ä¸€ä¸ªç®€å•æ¨¡å‹æ¥åšæ¼”ç¤ºï¼š

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.base = nn.Linear(4, 8)
        self.actor = nn.Linear(8, 2)
        self.critic = nn.Linear(8, 1)

    def forward(self, x):
        x = self.base(x)
        return self.actor(x), self.critic(x)

model = MyModel()
```

**ï¼ˆï¼ˆ1ï¼‰`model.parameters()`**

è¿™æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä¼šè¿”å›æ‰€æœ‰å¯è®­ç»ƒçš„å‚æ•°ï¼ˆä¸å¸¦åå­—ï¼‰ï¼š

```python
for p in model.parameters():
    print(p)
```

è¾“å‡ºï¼ˆæ¯ä¸ªéƒ½æ˜¯ `torch.nn.Parameter`ï¼‰ï¼š

```css
Parameter containing:
tensor([[...]], requires_grad=True)
Parameter containing:
tensor([...], requires_grad=True)
... å…±6ä¸ªï¼šbase(æƒé‡+bias)ï¼Œactor(æƒé‡+bias)ï¼Œcritic(æƒé‡+bias)
```

**ï¼ˆ2ï¼‰`model.named_parameters()`**

è¿™ä¸ªç‰ˆæœ¬å¸¦åå­—ï¼ˆæ¯”å¦‚ `base.weight`, `actor.bias`ï¼‰ï¼Œæ–¹ä¾¿ä½ è°ƒè¯•ï¼š

```python
for name, p in model.named_parameters():
    print(name, p.shape)
```

è¾“å‡ºç¤ºä¾‹ï¼š

```css
base.weight torch.Size([8, 4])
base.bias torch.Size([8])
actor.weight torch.Size([2, 8])
actor.bias torch.Size([2])
critic.weight torch.Size([1, 8])
critic.bias torch.Size([1])
```

**ï¼ˆ3ï¼‰`optimizer.param_groups`**

è¿™ä¸ªéå¸¸é‡è¦ï¼Œå’Œä½ åŠ¨æ€è°ƒèŠ‚å­¦ä¹ ç‡å¯†åˆ‡ç›¸å…³ã€‚æ¯ä¸ª param_group æ˜¯ä¸€ä¸ªå­—å…¸ï¼š

```python
optimizer = torch.optim.Adam([
    {"params": model.base.parameters(), "lr": 1e-3},
    {"params": model.actor.parameters(), "lr": 5e-4},
    {"params": model.critic.parameters(), "lr": 1e-4},
])

for i, group in enumerate(optimizer.param_groups):
    print(f"Group {i}: lr = {group['lr']}")
    for param in group["params"]:
        print(param.shape)
```

è¾“å‡ºç»“æ„ç±»ä¼¼ï¼š

```css
Group 0: lr = 0.001
torch.Size([8, 4])
torch.Size([8])
Group 1: lr = 0.0005
torch.Size([2, 8])
torch.Size([2])
Group 2: lr = 0.0001
torch.Size([1, 8])
torch.Size([1])
```

**ï¼ˆ4ï¼‰æ€»ç»“ä¸€ä¸‹ä¸‰è€…åŒºåˆ«ï¼š**

| æ–¹æ³•                       | è¿”å›å†…å®¹                            | æ˜¯å¦åŒ…å«å‚æ•°å          | ç”¨é€”                       |
| -------------------------- | ----------------------------------- | ----------------------- | -------------------------- |
| `model.parameters()`       | æ‰€æœ‰å‚æ•°ï¼ˆæŒ‰å®šä¹‰é¡ºåºï¼‰              | âŒ                       | ä¼˜åŒ–å™¨åˆå§‹åŒ–               |
| `model.named_parameters()` | å‚æ•° + åå­—                         | âœ…                       | è°ƒè¯•ã€å†»ç»“æŸäº›å‚æ•°ã€æ‰“å°ç­‰ |
| `optimizer.param_groups`   | åˆ†ç»„å­—å…¸ï¼Œæ¯ç»„æœ‰ lrã€paramsã€eps ç­‰ | âŒï¼ˆparamså†…éƒ¨æ²¡æœ‰åå­—ï¼‰ | å­¦ä¹ ç‡è°ƒèŠ‚ã€æƒé‡åˆ†ç»„è®¾ç½®ç­‰ |

------

å¦‚æœä½ æƒ³è¦ç»“åˆparam_groupçš„é¡ºåºå’Œåå­—ï¼Œå¯ä»¥ç”¨ä¸€ä¸ªåŠæ³•ï¼š**åœ¨è®¾ç½®optimizerä¹‹å‰ï¼Œæ‰‹åŠ¨æ‰“å°æ¯ä¸ªç»„çš„åå­—å’Œå¯¹åº”å‚æ•°ã€‚**

æˆ‘æ¥å¸®ä½ å°è£…ä¸€ä¸ªå¸¦åå­—å’Œç»„åˆ«çš„å¯è§†åŒ–å·¥å…·ï¼Œæ¯”å¦‚ä½ ä¼ å…¥modelå’Œoptimizerï¼Œå®ƒè‡ªåŠ¨æ‰“å°æ¯ç»„çš„å­¦ä¹ ç‡ã€ç»„åã€åŒ…å«å“ªäº›å±‚ã€‚è¿™æ ·æŸ¥bugç‰¹åˆ«èˆ’æœã€‚

é‚£æˆ‘æ¥ç»™ä½ å°è£…ä¸€ä¸ªå°å·¥å…·å‡½æ•°ï¼Œèƒ½æ¸…æ™°åœ°**å¯è§†åŒ–æ¯ä¸ªparam groupçš„ç»“æ„**ï¼ŒåŒ…æ‹¬ï¼š

- å­¦ä¹ ç‡ (`lr`)
- epsï¼ˆå¦‚æœæœ‰ï¼‰
- å‚æ•°å¼ é‡çš„å½¢çŠ¶
- å‚æ•°åï¼ˆå¯é€‰ï¼‰

```python
def visualize_optimizer_param_groups(model, optimizer):
    # è·å–å¸¦åå­—çš„å‚æ•°ï¼Œç”¨äºåŒ¹é… param group å†…çš„å‚æ•°
    name_map = {p: n for n, p in model.named_parameters()}

    for i, group in enumerate(optimizer.param_groups):
        print(f"\nğŸŸ¢ Param Group {i}:")
        print(f"  â†ª learning rate (lr): {group.get('lr', 'N/A')}")
        print(f"  â†ª epsilon (eps): {group.get('eps', 'N/A')}")
        print(f"  â†ª weight_decay: {group.get('weight_decay', 'N/A')}")
        print("  â†ª Parameters:")

        for param in group["params"]:
            name = name_map.get(param, "âš ï¸ unnamed")
            print(f"     - {name:30} | shape: {tuple(param.shape)}")
           
if __name__ == '__main__':
    import torch

    model_A = torch.nn.Linear(4, 3)
    model_B = torch.nn.Linear(3, 2)
    model_C = torch.nn.Linear(2, 1)
    model = torch.nn.Sequential(model_A, model_B, model_C)

    lr_A = 0.001
    lr_B = 0.002
    lr_C = 0.003

    optimizer = torch.optim.Adam([
        {"params": model_A.parameters(), "lr": lr_A, "eps": 1e-8},
        {"params": model_B.parameters(), "lr": lr_B, "eps": 1e-8},
        {"params": model_C.parameters(), "lr": lr_C, "eps": 1e-8},
    ])
    for i, group in enumerate(optimizer.param_groups):
        print(f"Param group {i} learning rate: {group['lr']}")

    # è®¾ç½®åŸºç¡€å­¦ä¹ ç‡
    scheduler = BatchSizeLRScheduler(
        optimizer,
        base_lrs=[lr_A, lr_B, lr_C],
        base_batch_size=64  # é»˜è®¤åŸºç¡€batch size
    )

    # æ¯è½®è®­ç»ƒåæ›´æ–°å­¦ä¹ ç‡
    for _ in range(1):
        current_batch_size = 128
        scheduler.step(current_batch_size)  # ä¼ å…¥å½“å‰å®é™…çš„batch size

    visualize_optimizer_param_groups(model, optimizer)
```

ä¼šæ˜¾ç¤ºï¼š

```
ğŸŸ¢ Param Group 0:
  â†ª learning rate (lr): 0.0014142135623730952
  â†ª epsilon (eps): 1e-08
  â†ª weight_decay: 0
  â†ª Parameters:
     - 0.weight                       | shape: (3, 4)
     - 0.bias                         | shape: (3,)

ğŸŸ¢ Param Group 1:
  â†ª learning rate (lr): 0.0028284271247461905
  â†ª epsilon (eps): 1e-08
  â†ª weight_decay: 0
  â†ª Parameters:
     - 1.weight                       | shape: (2, 3)
     - 1.bias                         | shape: (2,)

ğŸŸ¢ Param Group 2:
  â†ª learning rate (lr): 0.004242640687119286
  â†ª epsilon (eps): 1e-08
  â†ª weight_decay: 0
  â†ª Parameters:
     - 2.weight                       | shape: (1, 2)
     - 2.bias                         | shape: (1,)
```

