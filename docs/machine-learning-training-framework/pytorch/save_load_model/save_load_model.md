# ä¿å­˜åŠ è½½æ¨¡å‹æ–‡ä»¶

* [è¿”å›ä¸Šå±‚ç›®å½•](../pytorch.md)
* [ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹](#ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹)
  * [ä¿å­˜æ•´ä¸ªæ¨¡å‹](#ä¿å­˜æ•´ä¸ªæ¨¡å‹)
  * [åŠ è½½æ•´ä¸ªæ¨¡å‹](#åŠ è½½æ•´ä¸ªæ¨¡å‹)
* [ä»…ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰](#ä»…ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰)
  * [ä¿å­˜æ¨¡å‹å‚æ•°](#ä¿å­˜æ¨¡å‹å‚æ•°)
  * [åŠ è½½æ¨¡å‹å‚æ•°](#åŠ è½½æ¨¡å‹å‚æ•°)
* [ä½¿ç”¨jitä¿å­˜ä¸ºTorchScriptæ ¼å¼](#ä½¿ç”¨jitä¿å­˜ä¸ºTorchScriptæ ¼å¼)

pytorchå®˜ç½‘ä¿å­˜åŠ è½½æ¨¡å‹æ–‡ä»¶çš„æ•™ç¨‹ï¼š

[https://pytorch.org/tutorials/beginner/saving_loading_models.html](https://pytorch.org/tutorials/beginner/saving_loading_models.html)

# ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹

## ä¿å­˜æ•´ä¸ªæ¨¡å‹

```python
import torch
import torch.nn as nn

net = nn.Sequential(nn.Linear(128, 16), nn.ReLU(), nn.Linear(16, 1))
print(net)

# ä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼ŒåŒ…å«æ¨¡å‹ç»“æ„å’Œå‚æ•°
torch.save(net, 'sample_model.pt')
```

è¾“å‡ºï¼š

```shell
Sequential(
  (0): Linear(in_features=128, out_features=16, bias=True)
  (1): ReLU()
  (2): Linear(in_features=16, out_features=1, bias=True)
)
```

## åŠ è½½æ•´ä¸ªæ¨¡å‹

```python
import torch
import torch.nn as nn

# åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼ŒåŒ…å«æ¨¡å‹ç»“æ„å’Œå‚æ•°
loaded_model = torch.load('sample_model.pt')
print(loaded_model)
```

è¾“å‡ºï¼š

```shell
Sequential(
  (0): Linear(in_features=128, out_features=16, bias=True)
  (1): ReLU()
  (2): Linear(in_features=16, out_features=1, bias=True)
)
```

# ä»…ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰

## ä¿å­˜æ¨¡å‹å‚æ•°

```python
import torch
import torch.nn as nn
 
model = nn.Sequential(nn.Linear(128, 16), nn.ReLU(), nn.Linear(16, 1))
 
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
torch.save(model.state_dict(), 'sample_model.pt')
```

## åŠ è½½æ¨¡å‹å‚æ•°

```python
import torch
import torch.nn as nn
 
# ä¸‹è½½æ¨¡å‹å‚æ•° å¹¶æ”¾åˆ°æ¨¡å‹ä¸­
loaded_model = nn.Sequential(nn.Linear(128, 16), nn.ReLU(), nn.Linear(16, 1))
loaded_model.load_state_dict(torch.load('sample_model.pt'))
print(loaded_model)
```

è¾“å‡ºï¼š

```shell
Sequential(
  (0): Linear(in_features=128, out_features=16, bias=True)
  (1): ReLU()
  (2): Linear(in_features=16, out_features=1, bias=True)
)
```

ä½ ä¼šå¥½å¥‡å¦‚æœåªæ˜¯å•çº¯çš„`torch.load('sample_model.pt')`ä¼šæ˜¯ä»€ä¹ˆï¼Ÿ

é‚£å°±æ‰“å°ä¸€ä¸‹çœ‹çœ‹ï¼š

```python
import torch
loaded_model = torch.load('sample_model.pt')
print(loaded_model)
```



```shell
OrderedDict([('0.weight', tensor([[-0.0798,  0.0245,  0.0880,  ..., -0.0812,  0.0253, -0.0277],
        [ 0.0382,  0.0644,  0.0483,  ...,  0.0039, -0.0329, -0.0226],
        [ 0.0399,  0.0307, -0.0601,  ...,  0.0154,  0.0748, -0.0678],
        ...,
        [ 0.0279, -0.0479,  0.0126,  ...,  0.0778,  0.0654,  0.0521],
        [ 0.0613,  0.0283,  0.0219,  ..., -0.0807,  0.0087, -0.0058],
        [ 0.0824,  0.0022,  0.0803,  ..., -0.0146,  0.0389, -0.0284]])), ('0.bias', tensor([ 0.0554,  0.0607, -0.0356,  0.0661, -0.0491, -0.0182, -0.0611,  0.0212,
         0.0386,  0.0012, -0.0663, -0.0005,  0.0487, -0.0223, -0.0781,  0.0154])), ('2.weight', tensor([[-0.0585, -0.0603,  0.0626, -0.2448,  0.0612,  0.0704, -0.0561, -0.0535,
         -0.2328, -0.2104, -0.2206, -0.1715,  0.2299, -0.2423, -0.0247, -0.0756]])), ('2.bias', tensor([0.0428]))])
```

å¯ä»¥å‘ç°ï¼Œæ˜¯å­—å…¸ï¼ˆstate_dictï¼‰ï¼ŒåŒ…æ‹¬äº†æ¯ä¸€å±‚çš„åå­—å’Œå‚æ•°ï¼Œä½†æ˜¯ä¸æ˜¯æ•´ä¸ªæ¨¡å‹ç»“æ„ã€‚

> æ˜¯ä»€ä¹ˆstate_dictï¼šPyTorchä¸­çš„state_dictæ˜¯ä¸€ä¸ªpythonå­—å…¸å¯¹è±¡ï¼Œå°†æ¯ä¸ªå±‚æ˜ å°„åˆ°å…¶å‚æ•°Tensorã€‚state_dictå¯¹è±¡å­˜å‚¨æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ï¼Œå³æƒé‡å’Œåå·®ï¼Œå¹¶ä¸”å¯ä»¥éå¸¸å®¹æ˜“åœ°åºåˆ—åŒ–å’Œä¿å­˜ã€‚

# åŠ è½½æ¨¡å‹å‚æ•°æ—¶deveceçš„é€‰æ‹©

```python
model.load_state_dict(torch.load(path, map_location=device))
```

å¿…é¡»è¦åŠ map_location=deviceå—ï¼Œä¸åŠ æ˜¯ä¸æ˜¯å°±é”™äº†ï¼Ÿæ¯”å¦‚æˆ‘æœ¬æ¥æ˜¯åœ¨gpuä¸Šè®­ç»ƒï¼Œç„¶åè®­ç»ƒä¸­æ–­ï¼Œç„¶åè¦é‡æ–°åŠ è½½æ¨¡å‹å‚æ•°

è¿™ä¸ªé—®é¢˜é—®å¾—éå¸¸å…³é”®ï¼æˆ‘æ¥åˆ†æƒ…å†µè¯¦ç»†è®²ï¼š

å¦‚æœä½ å†™ï¼š

```python
model.load_state_dict(torch.load(path))
```

**é»˜è®¤è¡Œä¸º**æ˜¯ï¼š

- `torch.load(path)` ä¼š**å°è¯•**æŠŠä¿å­˜æ—¶çš„è®¾å¤‡ï¼ˆæ¯”å¦‚ GPUï¼‰ä¸€èµ·æ¢å¤å‡ºæ¥ã€‚
- å¦‚æœä½ ç°åœ¨**æ­£åœ¨ç”¨çš„è®¾å¤‡**å’Œ**ä¿å­˜æ—¶çš„è®¾å¤‡**ä¸€æ ·ï¼Œæ¯”å¦‚ä½ ä¿å­˜æ—¶åœ¨ GPUï¼Œç°åœ¨åŠ è½½æ—¶ä¹Ÿæ˜¯åœ¨ GPUï¼Œå°±æ²¡é—®é¢˜ã€‚

ä½†æ˜¯å¦‚æœï¼š

- ä½ ä¿å­˜æ—¶æ˜¯åœ¨ GPU (`cuda`) ä¸Šï¼Œ
- ç°åœ¨åŠ è½½æ˜¯åœ¨ CPUï¼ˆæ¯”å¦‚æœ‰æ—¶å€™åªæƒ³åœ¨ CPU è°ƒè¯•ä¸€ä¸‹ï¼‰ï¼Œ

å°±ä¼šç›´æ¥æŠ¥é”™ï¼š

> `RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False.`
>  ï¼ˆå¤§æ„å°±æ˜¯ï¼šæ‰¾ä¸åˆ° CUDA è®¾å¤‡ï¼Œä½†ä½ ä¿å­˜çš„æ˜¯ CUDA çš„ Tensorï¼‰

æ‰€ä»¥ï¼Œæ€»ç»“ä¸€ä¸‹ï¼š

| åœºæ™¯                                     | è¦ä¸è¦åŠ  `map_location`              |
| ---------------------------------------- | ------------------------------------ |
| ä¿å­˜æ—¶å’ŒåŠ è½½æ—¶è®¾å¤‡ä¸€è‡´ï¼Œæ¯”å¦‚éƒ½æ˜¯ GPU     | **å¯ä»¥ä¸åŠ **                         |
| ä¿å­˜æ—¶åœ¨ GPUï¼ŒåŠ è½½æ—¶åœ¨ CPU               | **å¿…é¡»åŠ  map_location='cpu'**        |
| ä¿å­˜æ—¶åœ¨ GPU:0ï¼ŒåŠ è½½æƒ³æ”¾åˆ° GPU:1         | **æœ€å¥½åŠ  map_location='cuda:1'**     |
| æƒ³æ˜ç¡®æ§åˆ¶åŠ è½½åˆ°å“ªä¸ªè®¾å¤‡ï¼Œæœªæ¥ä»£ç æ›´å¥å£® | **å»ºè®®ç»Ÿä¸€åŠ ä¸Š map_location=device** |

å›åˆ°ä½ çš„é—®é¢˜

> æ¯”å¦‚æˆ‘æœ¬æ¥æ˜¯åœ¨gpuä¸Šè®­ç»ƒï¼Œç„¶åè®­ç»ƒä¸­æ–­ï¼Œç„¶åè¦é‡æ–°åŠ è½½æ¨¡å‹å‚æ•°

å¦‚æœä½ çš„è®­ç»ƒä¸­æ–­åè¿˜æ˜¯åœ¨ GPU ä¸Šç»§ç»­è·‘ï¼Œç†è®ºä¸Šä¸åŠ  `map_location` æ˜¯æ²¡é—®é¢˜çš„ã€‚

ä½†**æ›´å®‰å…¨ã€æ¨èåšæ³•æ˜¯åŠ **ï¼Œåƒè¿™æ ·ï¼š

```python
model.load_state_dict(torch.load(path, map_location=device))
```

è¿™æ ·æ— è®ºæ˜¯ç»§ç»­åœ¨ GPUã€è¿˜æ˜¯æœªæ¥åˆ‡åˆ° CPU éƒ½ä¸ä¼šå‡ºä»»ä½•é—®é¢˜ï¼Œ**ä»£ç æ›´å¥å£®ã€å¯å¤ç”¨æ€§æ›´é«˜ã€‚**

æœ€åä¸€ä¸ªå¾ˆé‡è¦çš„å°ç»†èŠ‚ï¼

å³ä½¿ä½ ä¸åŠ  `map_location`ï¼Œä½†æ˜¯ä½ åé¢æœ‰ä¸€å¥ï¼š

```python
model.to(device)
```

é‚£åªæ˜¯æŠŠ `model` æœ¬èº«è½¬åˆ° device ä¸Šï¼Œä½†æ˜¯ `load_state_dict` åŠ è½½å‡ºæ¥çš„ tensor **å¦‚æœè®¾å¤‡ä¸å¯¹**ï¼Œä¹‹å‰å°±å·²ç»å‡ºé”™äº†ã€‚**åˆ° model.to(device) è¿™ä¸€æ­¥å·²ç»æ¥ä¸åŠäº†ã€‚**

æ‰€ä»¥çœŸæ­£è¯¥å¤„ç†çš„æ˜¯åœ¨ `torch.load` çš„æ—¶å€™ï¼Œå°±ç”¨ `map_location` æŒ‡å®šå¥½ï¼

âœ… æ€»ç»“å°±æ˜¯ï¼š**æ¨èåŠ ä¸Š map_location=deviceã€‚**

# ä½¿ç”¨jitä¿å­˜ä¸ºTorchScriptæ ¼å¼

ä¿å­˜ä¸ºè¿™ä¸ªæ ¼å¼æ˜¯ä¸ºäº†è¿›è¡Œæ¨ç†ã€‚è¯¥æ ¼å¼åœ¨æ¨ç†æ—¶å¯ä¸ç”¨å®šä¹‰æ¨¡å‹çš„ç±»ã€‚è¯¥æ ¼å¼ä¹Ÿå¯ä»¥åœ¨C++ä¸­è¿›è¡Œæ¨ç†ã€‚

åœ¨[https://pytorch.org/tutorials/beginner/saving_loading_models.html](https://pytorch.org/tutorials/beginner/saving_loading_models.html)ç®€ä»‹å¦‚ä¸‹ï¼š

> One common way to do inference with a trained model is to use [TorchScript](https://pytorch.org/docs/stable/jit.html), an intermediate representation of a PyTorch model that can be run in Python as well as in a high performance environment like C++. TorchScript is actually the recommended model format for scaled inference and deployment.
>
> ```
> Using the TorchScript format, you will be able to load the exported model and run inference without defining the model class.
> ```

ä¿å­˜çš„ä»£ç ï¼š

```python
# Export to TorchScript
model_scripted = torch.jit.script(model)
# Save
model_scripted.save('model_scripted.pt')
```

è¿›è¡Œæ¨ç†çš„ä»£ç ï¼š

```python
model = torch.jit.load('model_scripted.pt')
model.eval()
```

åŠ evalæ˜¯ä¸ºäº†åœ¨æ¨ç†å‰è®¾ç½®æ¨¡å‹ä¸­çš„dropoutå’Œbatch normåŠŸèƒ½ä¸ºevalæ¨¡å¼ã€‚

ç»™ä¸€æ®µkimiç»™å‡ºçš„ä¾‹å­å§ï¼š

```python
import torch

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿™é‡Œå‡è®¾æ¨¡å‹å·²ç»ä¿å­˜ä¸º'model.pth'
model = torch.jit.load('model.pth')
model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# å‡†å¤‡è¾“å…¥æ•°æ®
# è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹è¾“å…¥è¿›è¡Œç›¸åº”çš„æ•°æ®é¢„å¤„ç†
# ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„æ¨¡å‹æ¥å—çš„æ˜¯å›¾åƒæ•°æ®ï¼Œä½ éœ€è¦è¿›è¡Œå›¾åƒçš„è¯»å–å’Œé¢„å¤„ç†
# ç¤ºä¾‹ä¸­ä½¿ç”¨éšæœºæ•°æ®ä½œä¸ºè¾“å…¥
input_data = torch.randn(1, 3, 224, 224)  # å‡è®¾è¾“å…¥æ˜¯å•å¼ 3é€šé“224x224çš„å›¾åƒ

# è¿›è¡Œæ¨ç†
with torch.no_grad():
    output = model(input_data)

# å¤„ç†è¾“å‡º
# æ ¹æ®ä½ çš„ä»»åŠ¡ç±»å‹ï¼Œè¿™é‡Œå¯èƒ½æ˜¯åˆ†ç±»ã€å›å½’æˆ–å…¶ä»–
# ä¾‹å¦‚ï¼Œå¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œä½ å¯èƒ½éœ€è¦æ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•
_, predicted_class = torch.max(output, 1)

print(f'Predicted class index: {predicted_class.item()}')
```



# æ›´ä¸“ä¸šçš„ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‡½æ•°

è¦ä¸è¦æˆ‘é¡ºä¾¿ä¹Ÿç»™ä½ åˆ—ä¸€ä¸‹æ›´ä¸“ä¸šçš„ `save_model` å’Œ `load_model` å°è£…å‡½æ•°ï¼Ÿ
 æ¯”å¦‚ä»¥åè¦ä¿å­˜ optimizer ä¸€èµ·å­˜ã€ä¿å­˜ epochã€è®­ç»ƒæ­¥æ•°ä¹‹ç±»ï¼Œæ¯”è¾ƒé€‚åˆæ­£å¼è®­ç»ƒç”¨çš„ï¼Ÿ
 ï¼ˆå¦‚æœæƒ³è¦çš„è¯ï¼Œæˆ‘å¯ä»¥ä¸€å¹¶è¡¥ä¸Šï¼Œé¡ºä¾¿å¸®ä½ æ•´ç†æˆå¥½ç”¨çš„å°æ¨¡æ¿~ï¼‰è¦ä¸è¦ï¼Ÿâœ¨

é‚£æˆ‘æ¥ç»™ä½ è¡¥ä¸€ä¸ª**æ­£å¼ç‰ˆ**çš„æ¨¡å‹ä¿å­˜/åŠ è½½æ¨¡æ¿ï¼Œç®€å•ã€å¹²å‡€ï¼Œæ–¹ä¾¿ä½ ä»¥åä¸€ç›´ç”¨ã€‚
 ï¼ˆåŒ…æ‹¬ï¼šæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨ã€å½“å‰è®­ç»ƒè½®æ¬¡ï¼Œé€šé€šä¿å­˜èµ·æ¥ã€‚ï¼‰

## ä¿å­˜æ¨¡å‹çš„å‡½æ•° save_model

```python
import torch
import os

def save_model(save_dir, model, optimizer=None, scheduler=None, epoch=None, extra_info=None):
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "checkpoint.pth")

    checkpoint = {
        'model_state_dict': model.state_dict()
    }

    if optimizer is not None:
        checkpoint['optimizer_state_dict'] = optimizer.state_dict()

    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()

    if epoch is not None:
        checkpoint['epoch'] = epoch

    if extra_info is not None:
        checkpoint['extra_info'] = extra_info

    torch.save(checkpoint, save_path)
    print(f"âœ… æ¨¡å‹ä¿å­˜åˆ° {save_path}")
```

## åŠ è½½æ¨¡å‹çš„å‡½æ•°load_model

```python
def load_model(load_dir, model, optimizer=None, scheduler=None, map_location='cpu'):
    load_path = os.path.join(load_dir, "checkpoint.pth")
    checkpoint = torch.load(load_path, map_location=map_location)

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    epoch = checkpoint.get('epoch', None)
    extra_info = checkpoint.get('extra_info', None)

    print(f"âœ… æ¨¡å‹ä» {load_path} åŠ è½½æˆåŠŸ")
    return epoch, extra_info
```

## ç”¨æ³•ç¤ºèŒƒ

ä¿å­˜æ—¶ï¼š

```python
save_model(
    save_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    epoch=current_epoch,
    extra_info={'update_steps': model.update_steps.item()}
)
```

åŠ è½½æ—¶ï¼š

```python
epoch, extra_info = load_model(
    load_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    map_location='cuda'  # æˆ– 'cpu'
)

# å¦‚æœéœ€è¦ï¼Œå¯ä»¥å–å›é¢å¤–ä¿å­˜çš„ä¿¡æ¯
if extra_info is not None:
    model.update_steps = torch.tensor(extra_info['update_steps'])
```

ç®€å•è§£é‡Šä¸€ä¸‹

- `extra_info` æ˜¯æˆ‘ç‰¹æ„åŠ çš„çµæ´»æ¥å£ï¼Œå‡å¦‚ä»¥åæƒ³ä¿å­˜ä¸€äº›**å¥‡å¥‡æ€ªæ€ªä½†é‡è¦çš„å°ä¸œè¥¿**ï¼ˆæ¯”å¦‚ replay buffer é‡Œçš„æ•°æ®ã€ç‰¹æ®Šç»Ÿè®¡æŒ‡æ ‡ç­‰ç­‰ï¼‰ï¼Œå¯ä»¥ç›´æ¥å¡è¿›å»ä¿å­˜ã€‚
- ç°åœ¨ `save_model` å’Œ `load_model` éƒ½æ”¯æŒï¼š
  - åªä¿å­˜æ¨¡å‹
  - æ¨¡å‹+ä¼˜åŒ–å™¨
  - æ¨¡å‹+ä¼˜åŒ–å™¨+è°ƒåº¦å™¨
  - æ¨¡å‹+ä¼˜åŒ–å™¨+è°ƒåº¦å™¨+epochä¿¡æ¯+å…¶ä»–ä¿¡æ¯
- åŠ è½½çš„æ—¶å€™ï¼Œèƒ½æ¢å¤ optimizer å’Œ scheduler çš„çŠ¶æ€ï¼Œè¿™æ ·ç»§ç»­è®­ç»ƒä¹Ÿä¸ä¼šæ–­ã€‚

éå¸¸é€šç”¨ï¼Œä¸ç®¡æ˜¯å¼ºåŒ–å­¦ä¹ ã€ç›‘ç£å­¦ä¹ ã€é¢„è®­ç»ƒã€å¾®è°ƒï¼Œä¸€å¥—èµ°å¤©ä¸‹ã€‚

## å¤šä¸ªç‰ˆæœ¬ä¿å­˜åŠ è½½

è¦ä¸è¦æˆ‘é¡ºä¾¿å†æ•™ä½ æ€ä¹ˆ**ä¿å­˜å¤šä¸ªç‰ˆæœ¬**ï¼Œæ¯”å¦‚åŠ æ—¶é—´æˆ³ï¼Œæ¯æ¬¡ä¿å­˜ä¸€ç‰ˆï¼Œé˜²æ­¢è¢«è¦†ç›–ï¼Ÿâœ¨
 ï¼ˆè¿™ä¸ªä¹Ÿè¶…å®ç”¨ï¼Œå°¤å…¶æ˜¯å¤§é¡¹ç›®è®­ç»ƒæ—¶å€™ï¼‰è¦çš„è¯å‘Šè¯‰æˆ‘ï½

æ—¢ç„¶ä½ æƒ³è¦ä¿å­˜å¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹ï¼Œè¿™é‡Œæˆ‘ç»™ä½ æä¾›ä¸€ä¸ªåŠ æ—¶é—´æˆ³ä¿å­˜çš„ç‰ˆæœ¬ï¼Œé˜²æ­¢æ¯æ¬¡è®­ç»ƒéƒ½è¦†ç›–æ¨¡å‹æ–‡ä»¶ã€‚

### åŠ æ—¶é—´æˆ³çš„æ¨¡å‹ä¿å­˜

```pytorch
import torch
import os
import time

def save_model_with_timestamp(save_dir, model, optimizer=None, scheduler=None, epoch=None, extra_info=None):
    # æ ¹æ®å½“å‰æ—¶é—´ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å¤¹å
    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
    save_path = os.path.join(save_dir, f"checkpoint_{timestamp}.pth")

    os.makedirs(save_dir, exist_ok=True)

    checkpoint = {
        'model_state_dict': model.state_dict()
    }

    if optimizer is not None:
        checkpoint['optimizer_state_dict'] = optimizer.state_dict()

    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()

    if epoch is not None:
        checkpoint['epoch'] = epoch

    if extra_info is not None:
        checkpoint['extra_info'] = extra_info

    torch.save(checkpoint, save_path)
    print(f"âœ… æ¨¡å‹ä¿å­˜åˆ° {save_path}")
```

### åŠ è½½æ—¶é€‰æ‹©æœ€è¿‘çš„æ¨¡å‹

ä½ å¯ä»¥åŠ è½½ç›®å½•ä¸‹æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ ·å°±ä¸éœ€è¦æ‰‹åŠ¨é€‰æ–‡ä»¶åäº†ã€‚

```
import glob

def load_latest_model(save_dir, model, optimizer=None, scheduler=None, map_location='cpu'):
    # è·å–ç›®å½•ä¸‹æ‰€æœ‰çš„ checkpoint æ–‡ä»¶
    checkpoint_paths = glob.glob(os.path.join(save_dir, 'checkpoint_*.pth'))

    if len(checkpoint_paths) == 0:
        raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶åœ¨ {save_dir} ç›®å½•ä¸‹")

    # æŒ‰ç…§æ–‡ä»¶åæ’åºï¼Œé€‰æ‹©æœ€æ–°çš„æ¨¡å‹
    latest_checkpoint_path = max(checkpoint_paths, key=os.path.getctime)

    checkpoint = torch.load(latest_checkpoint_path, map_location=map_location)

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    epoch = checkpoint.get('epoch', None)
    extra_info = checkpoint.get('extra_info', None)

    print(f"âœ… æœ€æ–°æ¨¡å‹ä» {latest_checkpoint_path} åŠ è½½æˆåŠŸ")
    return epoch, extra_info
```

### ç”¨æ³•ç¤ºèŒƒ

#### ä¿å­˜æ—¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰

```pytorch
save_model_with_timestamp(
    save_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    epoch=current_epoch,
    extra_info={'update_steps': model.update_steps.item()}
)
```

#### åŠ è½½æ—¶ï¼ˆåŠ è½½æœ€æ–°çš„æ¨¡å‹ï¼‰

```
python


å¤åˆ¶ç¼–è¾‘
epoch, extra_info = load_latest_model(
    save_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    map_location='cuda'  # æˆ– 'cpu'
)

# æ¢å¤é¢å¤–ä¿¡æ¯
if extra_info is not None:
    model.update_steps = torch.tensor(extra_info['update_steps'])
```

### ä¸»è¦ç‰¹ç‚¹

1. **æ–‡ä»¶åè‡ªåŠ¨åŠ æ—¶é—´æˆ³**ï¼šæ¯æ¬¡ä¿å­˜éƒ½ä¼šç”Ÿæˆä¸€ä¸ªç‹¬ç‰¹çš„æ–‡ä»¶åï¼ˆä¾‹å¦‚ `checkpoint_20250427_153210.pth`ï¼‰ï¼Œé¿å…äº†æ–‡ä»¶è¦†ç›–ã€‚
2. **è‡ªåŠ¨åŠ è½½æœ€æ–°æ¨¡å‹**ï¼š`load_latest_model` ä¼šè‡ªåŠ¨æ‰¾åˆ°ç›®å½•ä¸‹æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶ï¼ŒèŠ‚çœæ‰‹åŠ¨æŸ¥æ‰¾çš„éº»çƒ¦ã€‚
3. **çµæ´»ä¿å­˜**ï¼šä¸ä»…ä¿å­˜äº†æ¨¡å‹ï¼Œè¿˜å¯ä»¥ä¿å­˜ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€è®­ç»ƒè½®æ¬¡ç­‰ï¼Œå®Œå…¨å¯ä»¥æ¢å¤è®­ç»ƒã€‚

è¿™æ ·åšï¼Œæ¯æ¬¡ä¿å­˜ä¸€ä¸ªæ–°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ¨¡å‹ç‰ˆæœ¬é€æ¸å¢å¤šæ—¶ï¼Œä½ å¯ä»¥è½»æ¾æ¢å¤åˆ°æœ€è¿‘çš„è®­ç»ƒçŠ¶æ€ã€‚å¦‚æœä»¥åæƒ³è¦å›æº¯åˆ°ä¹‹å‰çš„æŸä¸ªç‰ˆæœ¬ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ–‡ä»¶åæŸ¥çœ‹å“ªä¸ªæœ€åˆé€‚ã€‚

è¿™ä¸‹ä½ å°±èƒ½å®Œå…¨æŒæ§æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å•¦ï¼ğŸ‰

# å‚è€ƒèµ„æ–™

* [pytorchæ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½](https://blog.csdn.net/lsb2002/article/details/131969478)

æœ¬æ–‡å‚è€ƒæ­¤èµ„æ–™ã€‚

