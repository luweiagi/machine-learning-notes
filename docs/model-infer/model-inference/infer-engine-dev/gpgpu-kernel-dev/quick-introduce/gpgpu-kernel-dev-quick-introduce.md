# GPGPUç®—å­å¼€å‘å¿«é€Ÿå…¥é—¨

* [è¿”å›ä¸Šå±‚ç›®å½•](../gpgpu-kernel-dev.md)
* [å¿«é€Ÿå…¥é—¨](#å¿«é€Ÿå…¥é—¨)
  * [å¼€å‘GPGPUç®—å­çš„æœ€å¿«è·¯å¾„](#å¼€å‘GPGPUç®—å­çš„æœ€å¿«è·¯å¾„)
  * [CUDAèƒŒæ™¯çŸ¥è¯†](#CUDAèƒŒæ™¯çŸ¥è¯†)
    * [Relu-CUDA-kernelæºç ](#Relu-CUDA-kernelæºç )
    * [ä»£ç è§£é‡Š](#ä»£ç è§£é‡Š)
      * [ä»£ç æ•´ä½“è¯¦ç»†è§£é‡Š](#ä»£ç æ•´ä½“è¯¦ç»†è§£é‡Š)
      * [relu_kernelä¸­çš„næ˜¯ä»€ä¹ˆ](#relu_kernelä¸­çš„næ˜¯ä»€ä¹ˆ)
      * [é€»è¾‘blockä¼šåˆ†é…åˆ°ç‰©ç†SMå»æ‰§è¡Œ](#é€»è¾‘blockä¼šåˆ†é…åˆ°ç‰©ç†SMå»æ‰§è¡Œ)
      * [ä¸åŒä»£ç çš„å†™æ³•çš„åŒºåˆ«](#ä¸åŒä»£ç çš„å†™æ³•çš„åŒºåˆ«)
* [å®Œæ•´å¯ç”¨çš„PyTorch-CUDA-Extensionå·¥ç¨‹](#å®Œæ•´å¯ç”¨çš„PyTorch-CUDA-Extensionå·¥ç¨‹)
  * [ä»£ç æ¶æ„](#ä»£ç æ¶æ„)
  * [é€æ–‡ä»¶å®Œæ•´ä»£ç ](#é€æ–‡ä»¶å®Œæ•´ä»£ç )
    * [relu_kernel.cuï¼ˆCUDAå®ç°ï¼‰](#relu_kernel.cuï¼ˆCUDAå®ç°ï¼‰)
    * [relu.cppï¼ˆC++å‰ç«¯+PyTorchç»‘å®šï¼‰](#relu.cppï¼ˆC++å‰ç«¯+PyTorchç»‘å®šï¼‰)
    * [setup.pyï¼ˆç”¨PyTorchçš„cpp_extensionç¼–è¯‘ï¼‰](#setup.pyï¼ˆç”¨PyTorchçš„cpp_extensionç¼–è¯‘ï¼‰)
    * [test.pyï¼ˆæµ‹è¯•ï¼‰](#test.pyï¼ˆæµ‹è¯•ï¼‰)
  * [ç¼–è¯‘](#ç¼–è¯‘)
    * [æŠ¥é”™è§£å†³](#æŠ¥é”™è§£å†³)
      * [æœªå®‰è£…nvcc](#æœªå®‰è£…nvcc)
      * [ç¯å¢ƒå˜é‡TORCH_CUDA_ARCH_LISTè®¾ç½®é”™è¯¯](#ç¯å¢ƒå˜é‡TORCH_CUDA_ARCH_LISTè®¾ç½®é”™è¯¯)
    * [ç¼–è¯‘å®Œçš„æ–‡ä»¶ç»“æ„åˆ†æ](#ç¼–è¯‘å®Œçš„æ–‡ä»¶ç»“æ„åˆ†æ)
    * [ç¼–è¯‘åŸç†](#ç¼–è¯‘åŸç†)
      * [relu_kernel.cuæ€ä¹ˆç¼–è¯‘](#relu_kernel.cuæ€ä¹ˆç¼–è¯‘)
      * [python-setup.pyç±»ä¼¼cmake](#python-setup.pyç±»ä¼¼cmake)
      * [PyTorch-CUDA-Extensionçš„ç¼–è¯‘æµç¨‹](#PyTorch-CUDA-Extensionçš„ç¼–è¯‘æµç¨‹)
      * [relu_kernel.cuæœ¬è´¨æ˜¯c++ä»£ç ](#relu_kernel.cuæœ¬è´¨æ˜¯c++ä»£ç )
      * [relu_kernelå¯çœ‹ä½œæ˜¯cudaå¯¹c++çš„æ‹“å±•ï¼Œç±»ä¼¼qtå¯¹c++çš„æ‹“å±•](#relu_kernelå¯çœ‹ä½œæ˜¯cudaå¯¹c++çš„æ‹“å±•ï¼Œç±»ä¼¼qtå¯¹c++çš„æ‹“å±•)
      * [nvccç¼–è¯‘å™¨æ›¿ä»£g++ç¼–è¯‘cuæ–‡ä»¶](#nvccç¼–è¯‘å™¨æ›¿ä»£g++ç¼–è¯‘cuæ–‡ä»¶)
      * [importèƒ½ç›´æ¥å¯¼å…¥soè¿›python](#importèƒ½ç›´æ¥å¯¼å…¥soè¿›python)
    * [nvccä»‹ç»](#nvccä»‹ç»)
  * [è¿è¡Œæµ‹è¯•](#è¿è¡Œæµ‹è¯•)

# å¿«é€Ÿå…¥é—¨

## å¼€å‘GPGPUç®—å­çš„æœ€å¿«è·¯å¾„

ç”¨**å·¥ç¨‹è§’åº¦ + æœ€ä½èƒŒæ™¯å‡è®¾**çš„æ–¹å¼ï¼Œç»™ä½ ä¸€æ¡ä» 0 åˆ°èƒ½ä¸Šæ‰‹å¼€å‘ GPGPU ç®—å­çš„æœ€å¿«è·¯å¾„ï¼Œå°¤å…¶é€‚åˆåŒæ ·ä»â€œè®­ç»ƒæ¨¡å‹â€è·¨åˆ°â€œç®—å­å¼€å‘â€çš„äººã€‚

ğŸš€ ä¸€ã€ä½ ç°åœ¨æœ€éœ€è¦çŸ¥é“çš„æ ¸å¿ƒäº‹å®ï¼ˆ10 åˆ†é’Ÿç†è§£ GPGPU ç®—å­ï¼‰

1. **ä½ ä»¥å‰æ˜¯â€œè°ƒç”¨ç®—å­çš„äººâ€**

è®­ç»ƒæ¨¡å‹æ—¶ä½ å†™çš„æ˜¯ï¼š

```
x = torch.nn.Conv2d(...)
y = torch.matmul(A, B)
```

è¿™äº›éƒ½æ˜¯**åˆ«äººå†™å¥½çš„ GPU ç®—å­**ã€‚

2. **ç°åœ¨ä½ æ˜¯â€œç®—å­çš„æä¾›è€…â€**

ä½ çš„ä»»åŠ¡æ˜¯ï¼š

- å®ç°ä¸€ä¸ª**é«˜æ€§èƒ½** GPU ç®—æ³•

- è®©åˆ«äººå¯ä»¥åƒ

  ```
  y = my_op(x)
  ```

  é‚£æ ·ä½¿ç”¨ä½ å†™çš„ç®—å­

3. â€œç®—å­å¼€å‘â€çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿ

**å†™ GPU å†…æ ¸ï¼ˆkernelï¼‰ + æŠŠå®ƒæ³¨å†Œåˆ°æ¡†æ¶ï¼ˆPyTorch/ONNX/TensorRT/CUDA Runtimeï¼‰é‡Œ**ã€‚

æ ¸å¿ƒèƒ½åŠ›å°±æ˜¯ä¸¤ä¸ªï¼š

- å†™ GPU kernelï¼ˆCUDAã€HIPã€æˆ–æ¡†æ¶ DSLï¼Œå¦‚ Tritonï¼‰
- è®©æ¡†æ¶è®¤è¯†ä½ çš„ kernelï¼ˆå‰ç«¯åŒ…è£… + åç«¯è°ƒåº¦ï¼‰

ğŸš€ äºŒã€GPGPU ç®—å­çš„æœ€å°æ•°å­¦æ¨¡å‹ï¼š100 ç§’è®²æ˜ç™½

ä½ å†™çš„ GPU kernel æœ¬è´¨ä¸Šéƒ½æ˜¯ï¼š

**(1) ä»¥å¹¶è¡Œçš„æ–¹å¼è¿è¡Œæ¯ä¸ª thread çš„ä¸€å°æ®µä»£ç **

ä¸€ä¸ª kernel é€šå¸¸é•¿è¿™æ ·ï¼š

```
__global__ void add_kernel(float* A, float* B, float* C, int N) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if(idx < N) C[idx] = A[idx] + B[idx];
}
```

**(2) GPU é€šè¿‡ thread block / warp è°ƒåº¦æˆåƒä¸Šä¸‡ä¸ªçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ**

ä½ è¦ç†è§£ 3 ä¸ªå…³é”®è¯ï¼ˆæ˜¯æ‰€æœ‰ç®—å­æ€§èƒ½çš„æ ¸å¿ƒï¼‰ï¼š

| GPU æ¦‚å¿µ              | è¯´æ˜                                |
| --------------------- | ----------------------------------- |
| **thread**            | æœ€å°æ‰§è¡Œå•ä½                        |
| **warpï¼ˆ32 ä¸ªçº¿ç¨‹ï¼‰** | GPU æœ€å°è°ƒåº¦å•ä½ï¼ˆæœ€é‡è¦ï¼‰          |
| **block**             | ä¸€ç»„ warpï¼ˆä¾‹å¦‚ 128 æˆ– 256 ä¸ªçº¿ç¨‹ï¼‰ |
| **grid**              | æ‰€æœ‰ block ç»„æˆ                     |

**(3) æ€§èƒ½çš„å…³é”®å› ç´ **

ç®—å­æ€§èƒ½çš„ä¸»è¦ç“¶é¢ˆæ¥è‡ªï¼š

1. **å†…å­˜å¸¦å®½ï¼ˆæœ€å¸¸è§ï¼‰**
   - global memoryï¼ˆæ…¢ï¼‰
   - shared memoryï¼ˆå¿«ï¼‰
   - registerï¼ˆæœ€å¿«ï¼‰
2. **å¹¶è¡Œåº¦æ˜¯å¦è¶³å¤Ÿ**
3. **warp divergenceï¼ˆçº¿ç¨‹åˆ†æ”¯ä¸ä¸€è‡´ï¼Œæ€§èƒ½å¤§å¹…ä¸‹é™ï¼‰**

ä½ åªè¦ç†è§£è¿™ä¸‰ç‚¹ï¼Œå·²ç»å…·å¤‡ 70% ç®—å­ä¼˜åŒ–æ€ç»´æ¡†æ¶ã€‚

ğŸš€ ä¸‰ã€ä½ çœŸæ­£éœ€è¦æŒæ¡çš„æŠ€èƒ½ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰

â­ Level 1ï¼ˆæœ€é‡è¦ï¼Œå¿…é¡»é©¬ä¸ŠæŒæ¡ï¼‰

1. CUDA åŸºç¡€ç¼–ç¨‹ï¼ˆkernelã€è°ƒåº¦ã€å†…å­˜æ‹·è´ï¼‰

åªéœ€è¦æŒæ¡ï¼š

- `__global__` kernel
- threadIdx / blockIdx
- global/shared/register memory
- launch kernel

è¿™å°±èƒ½å†™å¤§éƒ¨åˆ†åˆçº§ç®—å­ã€‚

2. PyTorch è‡ªå®šä¹‰ CUDA extension

è¿™æ˜¯ä½ æœ€å¸¸è§çš„ç›®æ ‡ä»»åŠ¡ï¼Œæµç¨‹æ˜¯ï¼š

```
C++ æ¥å£ï¼ˆpybind11ï¼‰
â†“
CUDA kernelï¼ˆ.cuï¼‰
â†“
ç¼–è¯‘åˆ° PyTorch extension
â†“
Python ä¸­ä½¿ç”¨
```

è¿™ä¸ªé“¾è·¯æ˜¯å·¥ç¨‹ä¸­æœ€å®é™…çš„ã€‚

3. ç”¨ Nsight Systems / Nsight Compute åšæ€§èƒ½åˆ†æ

ç®—å­å¼€å‘ä¸€å®šä¼šç”¨åˆ°ã€‚

â­ Level 2ï¼ˆè¿›é˜¶ï¼‰

4. ç®—å­å¸¸è§ä¼˜åŒ–ç­–ç•¥

- block/warp tilingï¼ˆçŸ©é˜µã€å·ç§¯ã€å½’çº¦å¸¸ç”¨ï¼‰
- shared memory ç¼“å†²
- register tiling
- é¿å… bank conflict
- é¿å… warp divergence
- loop unrolling
- ä½¿ç”¨ Tensor Coreï¼ˆCUTLASSï¼‰

â­ Level 3ï¼ˆé«˜çº§ï¼‰

5. CUDA Graph / Stream / Multi-GPU å¹¶å‘

6. TensorRT plugin å¼€å‘

7. CUTLASS / cuBLAS / cuDNN çš„å†…æ ¸åŸç†è§£æ

è¿™äº›ä¸»è¦ç”¨äºå¤§æ¨¡å‹æ¨ç†ä¼˜åŒ–ã€‚

ğŸš€ å››ã€ä»é›¶åˆ°èƒ½å†™ä¸€ä¸ªGPUç®—å­çš„æœ€å¿«è·¯çº¿å›¾ï¼ˆæˆ‘å¼ºçƒˆæ¨èï¼‰

ä¸‹é¢æ˜¯**æœ€å¿«ä¸Šæ‰‹è·¯çº¿**ï¼ˆä¹Ÿæ˜¯æˆ‘ç»™æ–°åŒäº‹åŸ¹è®­çš„æ ‡å‡†è·¯çº¿ï¼‰ã€‚

**ç¬¬ 1 å¤©ï¼šä¼šå†™ä½ çš„ç¬¬ä¸€ä¸ª GPU Kernelï¼ˆCUDA åŸºç¡€ï¼‰**

ä½ è¦ä¼šå†™æœ€åŸºæœ¬çš„ kernelï¼š

- å‘é‡åŠ æ³•
- çŸ©é˜µåŠ æ³•
- Reduceï¼ˆsumï¼‰
- GEMMï¼ˆä¸ä¼˜åŒ–ç‰ˆå³å¯ï¼‰

ä½ ä¼šæ˜ç™½ thread block æ˜¯å¹²å•¥çš„ï¼Œå…±äº«å†…å­˜æ€ä¹ˆç”¨ã€‚

**ç¬¬ 3 å¤©ï¼šPyTorch CUDA Extension**

å†™ä¸€ä¸ª PyTorch ç®—å­ï¼Œä¾‹å¦‚ï¼š

âœ¨ ä½ èƒ½åšåˆ°ï¼š

```
import my_cuda_extension
out = my_cuda_extension.my_op(x)
```

æµç¨‹åŒ…æ‹¬ï¼š

- C++ å‰ç«¯ï¼ˆtorch::Tensor wrapperï¼‰
- CUDA kernel
- setup.py + ç¼–è¯‘
- Python import

åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½ å·²ç»èƒ½æ¥ 70% çš„ç®—å­å¼€å‘ä»»åŠ¡ã€‚

**ç¬¬ 1â€“2 å‘¨ï¼šå¼€å§‹åšçœŸå®ä¸šåŠ¡ç®—å­**

å…¸å‹ç®—å­åŒ…æ‹¬ï¼š

- layernorm
- softmax
- RMSNorm
- GELU
- attention çš„éƒ¨åˆ†kernelï¼ˆå¦‚ QK^Tï¼‰

è¿™äº›éƒ½æœ‰ç»å…¸å®ç°æ–¹å¼ï¼Œå¯ä»¥å¿«é€Ÿæ¨¡ä»¿ã€‚

ğŸš€ äº”ã€ä½ è‚¯å®šå…³å¿ƒçš„é—®é¢˜ï¼šå¼€å‘ç®—å­éœ€è¦å¤šå°‘æ•°å­¦ï¼Ÿ

å‡ ä¹ä¸éœ€è¦é«˜ç­‰æ•°å­¦ã€‚ç®—å­å¼€å‘ä¸»è¦å…³å¿ƒï¼š

1. **å¼ é‡å½¢çŠ¶ï¼ˆshapeï¼‰**
2. **è®¿å­˜æ¨¡å¼ï¼ˆmemory access patternï¼‰**
3. **çº¿ç¨‹å¹¶è¡Œæ–¹å¼**

ç®—å­çš„æ•°å­¦éš¾åº¦è¿œè¿œå°äºæ¨¡å‹è®­ç»ƒã€‚

ğŸš€ å…­ã€æˆ‘å¯ä»¥ä¸ºä½ åšä»€ä¹ˆï¼ˆä½ åªè¦å‘Šè¯‰æˆ‘ä½ çš„æ–¹å‘ï¼‰

åªè¦ä½ å‘Šè¯‰æˆ‘ï¼š

â¶ ä½ è¦å¼€å‘çš„ç®—å­æ˜¯ä»€ä¹ˆï¼ˆå¦‚ softmax / layernorm / conv / matmulï¼‰
 â· ä½ ç›®æ ‡å¹³å°æ˜¯ä»€ä¹ˆï¼ˆCUDA / HIP / Triton / TensorRT pluginï¼‰
 â¸ ä½ ç°åœ¨çš„ç¼–ç¨‹æ°´å¹³ï¼ˆæ¯”å¦‚ C++ è¿˜è¡Œï¼ŒCUDA å®Œå…¨ä¸ä¼šï¼‰

æˆ‘å¯ä»¥ç«‹å³ç»™ä½ ï¼š

- ä¸€ä»½**é’ˆå¯¹è¯¥ç®—å­çš„å®Œæ•´æ•™å­¦è·¯çº¿**
- æœ€çŸ­å¯è¡Œ **å·¥ç¨‹çº§ CUDA kernel æ¨¡æ¿**
- ä¸€å¥—å¯ç›´æ¥ç¼–è¯‘è¿è¡Œçš„ **PyTorch CUDA extension å·¥ç¨‹éª¨æ¶**
- æ€§èƒ½ä¼˜åŒ– checklist
- å¯¹åº”çš„ Nsight åˆ†ææŒ‡å—

ç”šè‡³æˆ‘å¯ä»¥å¸¦ä½ æ‰‹æŠŠæ‰‹ä»ç©ºç™½æ–‡ä»¶å¼€å§‹å†™ä¸€ä¸ªç®—å­ã€‚

ğŸš€ ä¸ƒã€æƒ³ç°åœ¨é©¬ä¸Šå…¥é—¨ï¼Œåªéœ€å‘Šè¯‰æˆ‘ï¼š

ğŸ‘‰ **ä½ è¢«å®‰æ’å¼€å‘çš„ç¬¬ä¸€ä¸ªç®—å­æ˜¯ä»€ä¹ˆï¼Ÿ**

æ˜¯ï¼šä»¥reluç®—å­ä¸ºä¾‹å¼€å‘ç®—å­ï¼Œç›®æ ‡å¹³å°æ˜¯CUDAï¼Œç°åœ¨çš„ç¼–ç¨‹æ°´å¹³æ˜¯C++ä¼šï¼ŒCUDAå®Œå…¨ä¸ä¼š

æˆ‘ç›´æ¥ç»™ä½ é‚£ä¸€ä¸ªçš„å¼€å‘æ•™ç¨‹å’Œå¯å¤åˆ¶ä»£ç ã€‚

å¥½ï¼æˆ‘ä»¬å°±ä» **â€œæœ€é€‚åˆä½œä¸ºå…¥é—¨ç¬¬ä¸€ä¸ª CUDA ç®—å­â€â€”â€”ReLU** å¼€å§‹ã€‚

æˆ‘ä¼šå¸¦ä½ å®Œæˆï¼š

ğŸ¯ ç›®æ ‡ï¼ˆåŠå°æ—¶ä½ å°±èƒ½è·‘èµ·æ¥ï¼‰

1. å†™ä¸€ä¸ª **CUDA kernel å®ç° ReLU**

2. ç”¨ **C++ + CUDA** å°è£…ä¸º PyTorch è‡ªå®šä¹‰ç®—å­

3. åœ¨ Python ä¸­è°ƒç”¨ï¼š

   ```
   import myrelu
   y = myrelu.relu(x)
   ```

è¿™ä¸€å¥—æ˜¯ä½ è¿›å…¥ **GPGPU + PyTorch ç®—å­å¼€å‘** çš„æœ€å°å·¥ç¨‹é—­ç¯ã€‚

## CUDAèƒŒæ™¯çŸ¥è¯†

### Relu-CUDA-kernelæºç 

ğŸ§± ä¸€ã€å…ˆè®²ä½ å¿…é¡»ç†è§£çš„æœ€å° CUDA çŸ¥è¯†ï¼ˆ5 åˆ†é’Ÿï¼‰

ReLUï¼š
$$
y_i = \max(x_i, 0)
$$
GPU å¹¶è¡Œéå¸¸å‹å¥½ â€”â€” **æ¯ä¸ªå…ƒç´ ç‹¬ç«‹è®¡ç®—ï¼Œä¸€ä¸ª thread è´Ÿè´£ä¸€ä¸ªå…ƒç´ **ã€‚

CUDA kernel çš„æ ¸å¿ƒï¼š

```shell
__global__ void relu_kernel(const float* x, float* y, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) y[idx] = fmaxf(x[idx], 0.0f);
}
```

ç†è§£ï¼š

- GPU ç”± **grid**ï¼ˆå¾ˆå¤š blockï¼‰ç»„æˆ
- æ¯ä¸ª block ä¸­æœ‰å¾ˆå¤š **thread**
- æ¯ä¸ª thread æœ‰è‡ªå·±çš„ **idx**
- ä½ è®©æ¯ä¸ª thread è´Ÿè´£ä¸€ä¸ªå…ƒç´ å³å¯

è¿™å°±æ˜¯ 90% çš„å…¥é—¨ç®—å­é•¿ç›¸ã€‚

### ä»£ç è§£é‡Š

#### ä»£ç æ•´ä½“è¯¦ç»†è§£é‡Š

èƒ½å¸®æˆ‘è®²ä¸‹è¿™ä¸ªä»£ç æ‰€éœ€çš„èƒŒæ™¯çŸ¥è¯†å’Œä»£ç çš„å…·ä½“å«ä¹‰å—ï¼Œè®©æˆ‘èƒ½å®Œå…¨ç†è§£ã€‚

ä¸‹é¢æˆ‘ä¼š **ä»é›¶å¼€å§‹**ï¼ˆå‡è®¾ä½ ä»æ¥æ²¡å†™è¿‡ CUDAï¼‰ï¼Œä¸€æ­¥ä¸€æ­¥è®²æ¸…æ¥š CUDA èƒŒæ™¯ã€çº¿ç¨‹/ç½‘æ ¼æ¦‚å¿µã€å†…æ ¸å‡½æ•°çš„è¿è¡Œæœºåˆ¶ï¼Œç„¶åè§£é‡Šä½ çœ‹åˆ°çš„é‚£æ®µ ReLU kernel åˆ°ä½ èƒ½ **å®Œå…¨ç†è§£å¹¶èƒ½è‡ªå·±å†™** çš„ç¨‹åº¦ã€‚

ğŸ§± 1. ä½ å¿…é¡»å…ˆçŸ¥é“ï¼šä»€ä¹ˆæ˜¯ GPU å¹¶è¡Œï¼Ÿ

CPUï¼ˆ1â€“8 æ ¸ï¼‰ â‰ˆ é€‚åˆä¸²è¡Œã€é€»è¾‘å¤æ‚çš„è®¡ç®—

GPUï¼ˆå‡ åƒæ ¸å¿ƒï¼‰ â‰ˆ é€‚åˆ **å¤§é‡å®Œå…¨ä¸€æ ·çš„è®¡ç®—ä»»åŠ¡å¹¶è¡Œæ‰§è¡Œ**

æ¯”å¦‚ ReLUï¼š
$$
y_i = \max(x_i, 0)
$$
æ¯ä¸ªå…ƒç´ è®¡ç®—äº’ä¸å½±å“ï¼Œå®Œç¾é€‚åˆ GPU â€”â€” ç»™æ¯ä¸ªå…ƒç´ åˆ†é…ä¸€ä¸ªçº¿ç¨‹å°±è¡Œã€‚

ğŸ§© 2. CUDA ç¨‹åºç”±ä¸¤éƒ¨åˆ†ç»„æˆ

| éƒ¨åˆ†           | åœ¨å“ªé‡Œæ‰§è¡Œ | ä»£ç å½¢æ€                         |
| -------------- | ---------- | -------------------------------- |
| Hostï¼ˆä¸»æœºï¼‰   | CPU        | æ™®é€š C/C++                       |
| Deviceï¼ˆè®¾å¤‡ï¼‰ | GPU        | CUDA kernelï¼ˆ`__global__` å‡½æ•°ï¼‰ |

ä½ åœ¨ CPU ä¸Šå†™ä»£ç ï¼Œç„¶åè°ƒç”¨ GPU kernel è®© GPU å¹²æ´»ã€‚

ğŸ§© 3. ä»€ä¹ˆæ˜¯ kernelï¼Ÿ

kernel å°±æ˜¯**åœ¨ GPU ä¸Šæ‰§è¡Œçš„å‡½æ•°**ã€‚

```
__global__ void relu_kernel(const float* x, float* y, int n)
```

å…³é”®ç‚¹ï¼š

- `__global__` = **è¿™ä¸ªå‡½æ•°è¿è¡Œåœ¨ GPU ä¸Šï¼Œæ˜¯ kernel**
- CPU ä¸èƒ½ç›´æ¥è°ƒç”¨æ™®é€šæ–¹å¼ï¼Œéœ€è¦ç‰¹æ®Šè¯­æ³•ï¼š

```
relu_kernel<<<gridSize, blockSize>>>(x, y, n);
```

ğŸ§© 4. GPU å¦‚ä½•å¹¶è¡Œæ‰§è¡Œ kernelï¼Ÿ

GPU ç¨‹åºä¼šå¯åŠ¨ **å¾ˆå¤šçº¿ç¨‹ï¼ˆthreadï¼‰** æ¥ä¸€èµ·æ‰§è¡Œ kernelã€‚

å®ƒä»¬ç»„æˆï¼š

Gridï¼ˆç½‘æ ¼ï¼‰

- ç”±å¤šä¸ª Blockï¼ˆçº¿ç¨‹å—ï¼‰ç»„æˆ

Blockï¼ˆçº¿ç¨‹å—ï¼‰

- ç”±å¤šä¸ª Threadï¼ˆçº¿ç¨‹ï¼‰ç»„æˆ

Threadï¼ˆçº¿ç¨‹ï¼‰

- æ¯ä¸ªçº¿ç¨‹éƒ½ä¼šæ‰§è¡Œ kernel å¹¶å¤„ç†ä¸€ä¸ªå…ƒç´ ï¼ˆæœ¬ä¾‹ä¸­ï¼‰

CUDA è‡ªåŠ¨è®©æ•°åƒä¸ªçº¿ç¨‹å¹¶è¡Œè¿è¡Œã€‚

ğŸ§© 5. é‚£è¿™ä¸‰ä¸ªå˜é‡æ˜¯è°ï¼Ÿ

åœ¨ kernel å†…éƒ¨ï¼Œä½ å¯ä»¥ç›´æ¥è®¿é—® CUDA ç»™ä½ çš„ä¸‰ä¸ªå˜é‡ï¼š

| å˜é‡          | å«ä¹‰                        |
| ------------- | --------------------------- |
| `blockIdx.x`  | å½“å‰ block åœ¨ grid ä¸­çš„ç¼–å· |
| `blockDim.x`  | æ¯ä¸ª block æœ‰å¤šå°‘çº¿ç¨‹       |
| `threadIdx.x` | çº¿ç¨‹åœ¨è‡ªå·± block ä¸­çš„ç¼–å·   |

ğŸ’¡ **ä¸ºäº†å¾—åˆ°å½“å‰çº¿ç¨‹å¯¹åº”çš„å…¨å±€å…ƒç´ ç¼–å·ï¼Œå¿…é¡»æŠŠ block ç¼–å· + thread ç¼–å·ç»“åˆèµ·æ¥ï¼š**

```
int idx = blockIdx.x * blockDim.x + threadIdx.x;
```

æ¯”å¦‚ï¼š

- blockDim.x = 128ï¼ˆæ¯ä¸ª block æœ‰ 128 ä¸ªçº¿ç¨‹ï¼‰
- blockIdx.x = 2ï¼ˆè¿™æ˜¯ç¬¬ 3 ä¸ª blockï¼‰
- threadIdx.x = 17ï¼ˆè¿™æ˜¯ block é‡Œçš„ç¬¬ 18 ä¸ªçº¿ç¨‹ï¼‰

é‚£ä¹ˆå®ƒå¤„ç†çš„å…ƒç´  index=

```
idx = 2 * 128 + 17 = 273
```

å®ƒå°±ä¼šè®¡ç®—ï¼š

```
y[273] = max(x[273], 0)
```

ğŸ§© 6. ä¸‹é¢æˆ‘ä»¬ç»ˆäºæ¥çœ‹ ReLU kernel

```
__global__ void relu_kernel(const float* x, float* y, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) y[idx] = fmaxf(x[idx], 0.0f);
}
```

æˆ‘æ¥é€è¡Œè§£é‡Šï¼š

ç¬¬ 1 è¡Œï¼šå®šä¹‰ GPU å†…æ ¸

```
__global__ void relu_kernel(const float* x, float* y, int n)
```

- `__global__` å‘Šè¯‰ CUDAï¼šè¿™æ˜¯ä¸€ä¸ª GPU kernel
- è¿è¡Œåœ¨ GPU ç«¯ï¼Œä¸æ˜¯ CPU
- å‚æ•° `x`ã€`y` éœ€è¦æ˜¯ GPU ä¸Šçš„å†…å­˜åœ°å€ï¼ˆdevice memoryï¼‰

ç¬¬ 2 è¡Œï¼šè®¡ç®—æ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„å…ƒç´  idx

```
int idx = blockIdx.x * blockDim.x + threadIdx.x;
```

å«ä¹‰ï¼š

**æ ¹æ®çº¿ç¨‹æ‰€åœ¨çš„ block å’Œçº¿ç¨‹åœ¨ block å†…çš„ç¼–å·ï¼Œç®—å‡ºè¿™æ˜¯å…¨å±€ç¬¬ idx ä¸ªçº¿ç¨‹ã€‚**

æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œè¿™ä¸ªå…¬å¼ä¼šå¾—åˆ°ä¸åŒçš„ idxã€‚

ç¬¬ 3â€“4 è¡Œï¼šè¾¹ç•Œæ£€æŸ¥ + ReLU è®¡ç®—

```
if (idx < n) 
    y[idx] = fmaxf(x[idx], 0.0f);
```

ä¸ºä»€ä¹ˆéœ€è¦ `if (idx < n)`ï¼Ÿ

- grid å’Œ block çš„å°ºå¯¸é€šå¸¸å‘ä¸Šå–æ•´ï¼Œçº¿ç¨‹æ•°å¯èƒ½ > n
- é˜²æ­¢è¶Šç•Œè®¿é—®

`fmaxf(a, b)` = è¿”å› `max(a, b)` çš„ float ç‰ˆæœ¬ã€‚

ğŸ§© 7. å®Œæ•´æµç¨‹å›¾ï¼ˆä½ æœ€å¥½ç†è§£å®ƒï¼‰

```
CPU:  åˆ†é… GPU å†…å­˜ï¼Œå¤åˆ¶ x åˆ° GPU
CPU:  å¯åŠ¨ kernel <<<grid, block>>>
GPU:  åˆ›å»º grid
GPU:  grid ä¸­åˆ›å»ºå¤šä¸ª block
GPU:  æ¯ä¸ª block åˆ›å»ºå¤šä¸ª thread
GPU:  æ¯ä¸ª thread è®¡ç®—è‡ªå·±çš„ idx
GPU:  æ¯ä¸ª thread æ‰§è¡Œ ReLU
CPU:  æŠŠ y ä» GPU æ‹·å›
```

ğŸ§© 8. ä¸»æœºè°ƒç”¨ä»£ç é•¿ä»€ä¹ˆæ ·ï¼Ÿ

è®©ä½ æ›´å®Œæ•´ç†è§£ï¼š

```c++
int n = 1000000;
int blockSize = 256;
int gridSize = (n + blockSize - 1) / blockSize;

// GPU å†…å­˜åˆ†é…
float *d_x, *d_y;
cudaMalloc(&d_x, n * sizeof(float));
cudaMalloc(&d_y, n * sizeof(float));

// æ•°æ®æ‹·è´åˆ° GPU
cudaMemcpy(d_x, host_x, n * sizeof(float), cudaMemcpyHostToDevice);

// å¯åŠ¨ kernel
relu_kernel<<<gridSize, blockSize>>>(d_x, d_y, n);

// æ‹·è´ç»“æœå› CPU
cudaMemcpy(host_y, d_y, n * sizeof(float), cudaMemcpyDeviceToHost);
```

è¿™æ ·ä½ å°±å®Œå…¨æ‡‚é‚£ä¸ª kernel çš„æ„ä¹‰äº†ã€‚

ğŸ“Œ ä½ ç°åœ¨åº”è¯¥å·²ç»ç†è§£äº†è¿™æ®µä»£ç ï¼š

```
__global__ void relu_kernel(const float* x, float* y, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) y[idx] = fmaxf(x[idx], 0.0f);
}
```

ä½ çŸ¥é“ï¼š

- å®ƒæ˜¯ GPU ä¸Šæ‰§è¡Œçš„å‡½æ•°
- å®ƒå¯¹æ¯ä¸ªå…ƒç´ å¼€ä¸€ä¸ªçº¿ç¨‹
- æ¯ä¸ªçº¿ç¨‹é€šè¿‡ idx æ‰¾åˆ°è‡ªå·±è´Ÿè´£çš„å…ƒç´ 
- å¹¶è¡Œæ‰§è¡Œ ReLU

#### relu_kernelä¸­çš„næ˜¯ä»€ä¹ˆ

```c++
__global__ void relu_kernel(const float* x, float* y, int n)
```

ä¸­çš„næ˜¯ä»€ä¹ˆï¼Ÿè¿™ä¸ªnå®Œå…¨ä¸æ¸…æ¥š

éå¸¸å¥½ï¼Œä½ èƒ½æŠ“åˆ°æœ€å…³é”®çš„ç–‘é—® ğŸ‘

 **`n` æ˜¯ä»€ä¹ˆï¼Ÿå®ƒä»å“ªæ¥ï¼Ÿä¸ºä»€ä¹ˆè¦ä¼ ç»™ GPUï¼Ÿ**

æˆ‘æ¥æŠŠè¿™ä¸ªé—®é¢˜å½»åº•è®²æ¸…æ¥šï¼Œè®©ä½  100% æ˜ç™½ã€‚

âœ… ç»“è®ºå…ˆè¯´ï¼š`n` æ˜¯å‘é‡/æ•°ç»„çš„é•¿åº¦ï¼ˆå…ƒç´ ä¸ªæ•°ï¼‰

ä¹Ÿå°±æ˜¯ä½ æœ‰å¤šå°‘ä¸ªè¾“å…¥æ•°æ®éœ€è¦åš ReLUã€‚

æ¯”å¦‚ï¼š

ä½ æœ‰ä¸€ä¸ªå¼ é‡ flatten åé•¿åº¦æ˜¯ï¼š

```
x = [0.5, -1.3, 2.0, 0.0, -0.8]   --> å…± n = 5 ä¸ªå…ƒç´ 
```

é‚£ `n = 5`

GPU æ‰çŸ¥é“å®ƒéœ€è¦åš 5 æ¬¡ ReLUã€‚

ğŸ“Œ ä¸ºä»€ä¹ˆå¿…é¡»ä¼  `n` ç»™ kernelï¼Ÿ

å› ä¸º GPU å†…æ ¸å‡½æ•°å†…éƒ¨ä¸ä¼šè‡ªåŠ¨çŸ¥é“æ•°ç»„æœ‰å¤šé•¿ã€‚

å®ƒåªçœ‹åˆ°ä¸€ä¸ªæŒ‡é’ˆï¼š

```
const float *x
```

æŒ‡é’ˆé‡Œæ²¡æœ‰æ•°ç»„é•¿åº¦ä¿¡æ¯ï¼ ï¼ˆè¿™æ˜¯ C/C++ çš„ç‰¹æ€§ï¼‰

æ‰€ä»¥ä½ å¿…é¡»å‘Šè¯‰ GPUï¼š

> â€œä½ è¦å¤„ç† n ä¸ªå…ƒç´ ï¼Œè¶…è¿‡ n çš„ä¸è¦å¤„ç†ã€‚â€

å¦åˆ™ GPU å¯èƒ½è¶Šç•Œè®¿é—®å†…å­˜ï¼Œç›´æ¥ç‚¸æ‰ã€‚

ğŸ“Œ `n` åœ¨ä¸»æœºï¼ˆCPUï¼‰ç«¯å®šä¹‰ï¼Œå†ä¼ ç»™ GPU

å…¸å‹çš„ CPU ç«¯ä»£ç ï¼š

```
int n = 1000000;   // æ•°æ®é•¿åº¦

relu_kernel<<<gridSize, blockSize>>>(d_x, d_y, n);
```

å†…æ ¸æ”¶åˆ°è¿™ä¸ªå‚æ•°ï¼š

```
__global__ void relu_kernel(const float* x, float* y, int n)
```

ğŸ“Œ kernel å†…éƒ¨ `n` çš„ç”¨é€”ï¼š**è¾¹ç•Œæ£€æŸ¥**

```
int idx = blockIdx.x * blockDim.x + threadIdx.x;

if (idx < n)
    y[idx] = fmaxf(x[idx], 0.0f);
```

ä¸ºä»€ä¹ˆè¦ `if(idx<n)`ï¼Ÿ

å› ä¸º grid/block çš„çº¿ç¨‹æ•°ç»å¸¸æ˜¯å‘ä¸Šå–æ•´çš„ï¼Œæ•°é‡å¯èƒ½å¤§äº nã€‚æ¯”å¦‚ï¼š

- n = 1000
- blockSize = 256
- gridSize = ceil(1000/256)=4 â†’ ä¸€å…±äº§ç”Ÿ 4Ã—256 = 1024 ä¸ªçº¿ç¨‹

åé¢å¤šå‡ºæ¥çš„ 24 ä¸ªçº¿ç¨‹å¿…é¡»å•¥ä¹Ÿä¸å¹²ï¼Œå¦åˆ™è¶Šç•Œè®¿é—®ã€‚

ğŸ“Œ å†ç»™ä½ ä¸€ä¸ªå½¢è±¡åŒ–ä¾‹å­ï¼ˆæœ€å®¹æ˜“ç†è§£ï¼‰

å‡è®¾ï¼š

```
x æœ‰ 10 ä¸ªå…ƒç´ 
```

ä½ å¯åŠ¨ kernelï¼š

```
blockDim.x = 4
gridDim.x = 3
```

ä¸€å…±åˆ›å»ºï¼š

```
3 * 4 = 12 ä¸ªçº¿ç¨‹
```

ä½†ä½ åªæœ‰ 10 ä¸ªå…ƒç´ è¦ç®—ã€‚

è¿™æ—¶ï¼š

```
idx = 0~9 â†’ æœ‰æ•ˆ
idx = 10,11 â†’ æ— æ•ˆï¼ˆè¶Šç•Œï¼‰
```

æ‰€ä»¥æ¯ä¸ªçº¿ç¨‹éƒ½æ£€æŸ¥ï¼š

```
if (idx < n)
```

åªæœ‰å‰ 10 ä¸ªçº¿ç¨‹ä¼šæ‰§è¡Œ ReLUã€‚

ğŸ“Œ æœ€é‡è¦ä¸€å¥æ€»ç»“ï¼š

**n å°±æ˜¯ä½ æƒ³è®© GPU å¤„ç†çš„å…ƒç´ æ•°é‡ï¼Œç”¨äºå‘Šè¯‰æ¯ä¸ªçº¿ç¨‹æ˜¯å¦å‚ä¸è®¡ç®—ã€‚**

#### é€»è¾‘blockä¼šåˆ†é…åˆ°ç‰©ç†SMå»æ‰§è¡Œ

é—®é¢˜ï¼š

> è¿™é‡Œå‡è®¾xå’Œyçš„æ•°ç»„çš„é•¿åº¦æ˜¯10ï¼Œä½†æ˜¯ï¼Œidxæ˜¯å½“å‰blockä¸­çš„å½“å‰threadçš„é¡ºåºä½ç½®ï¼Œè¿™ä¸¤ä¸ªæ€ä¹ˆèƒ½æ¯”è¾ƒå‘¢ï¼Ÿé™¤éidxçš„èµ·å§‹ä½ç½®æ˜¯0ï¼Œæˆ–è€…ä¼ å…¥ä¸€ä¸ªèµ·å§‹ä½ç½®idx_startï¼Œç„¶åidx-idx_startå’Œnæ¯”è¾ƒã€‚å°±æ˜¯ï¼Œæˆ‘çŸ¥é“idx æ˜¯ block + thread ç»„åˆå¾—åˆ°çš„â€œå…¨å±€ thread ç´¢å¼•â€ï¼Œä½†æ˜¯ï¼Œä½ æ€ä¹ˆç¡®å®šå¤„ç†è¿™ä¸ªæ•°ç»„çš„èµ·å§‹çš„threadå°±æ˜¯ç¬¬0ä¸ªblockçš„ç¬¬0ä¸ªthreadå‘¢ï¼Ÿå¦‚æœä¸æ˜¯ï¼Œé‚£å°±æ— æ³•å’Œnæ¯”è¾ƒå•Šï¼Œæ¯”å¦‚å‰10ä¸ªblockéƒ½åœ¨å¤„ç†å…¶ä»–çš„äº‹ï¼ŒçœŸæ­£å¤„ç†è¿™ä¸ªæ•°ç»„çš„reluæ˜¯ä»ç¬¬11ä¸ªblockæ‰å¼€å§‹çš„ï¼Œè¿™æ ·çš„è¯ï¼Œidxå°±æ— æ³•å’Œnæ¯”è¾ƒäº†å‘€

å¥½é—®é¢˜ â€” ä½ æŠ“åˆ°äº†å¹¶è¡Œæ‰§è¡Œä¸­æœ€å®¹æ˜“æ··æ·†çš„ä¸€ç‚¹ã€‚ç­”æ¡ˆåˆ†ä¸¤å±‚ï¼š**CUDA çš„æ‰§è¡Œæ¨¡å‹ï¼ˆé€»è¾‘ä¸Šï¼‰** ä¸ **ä½ å¦‚ä½•ç»„ç»‡ / å¯åŠ¨ kernelï¼ˆç¨‹åºä¸Šï¼‰**ã€‚æˆ‘æŠŠä¸¤å±‚è®²æ¸…æ¥šï¼Œå¹¶ç»™å‡ºå®æˆ˜å¯ç”¨çš„æ–¹æ¡ˆã€‚

1) CUDA çš„æ‰§è¡Œæ¨¡å‹ï¼ˆå…³é”®äº‹å®ï¼‰

- **ä¸€æ¬¡ kernel å¯åŠ¨ä¼šåˆ›å»ºä¸€ä¸ªé€»è¾‘ grid**ï¼ˆ`gridDim.x` Ã— `blockDim.x` çš„è¿ç»­ç¼–å·ç©ºé—´ï¼‰ã€‚åœ¨è¿™ä¸ªé€»è¾‘ç©ºé—´é‡Œï¼Œ`blockIdx.x` å’Œ `threadIdx.x` çš„ç»„åˆ**ç¡®å®šæ¯ä¸ªçº¿ç¨‹çš„å…¨å±€ç¼–å· `idx`ï¼Œä¸”ç¼–å·ä» 0 å¼€å§‹åˆ° `gridDim.x \* blockDim.x - 1`ã€‚**
- è¿™äº›é€»è¾‘ç¼–å·**ä¸å®é™…ç‰©ç† SMï¼ˆstreaming multiprocessorï¼‰ä¸Šçš„è°ƒåº¦æ— å…³**ï¼šCUDA è¿è¡Œæ—¶ä¼šæŠŠé€»è¾‘ä¸Šçš„ block åˆ†é…åˆ°ç‰©ç† SM å»æ‰§è¡Œï¼Œä½†è¿™åªæ˜¯è°ƒåº¦ç»†èŠ‚ï¼Œ**ä¸ä¼šæ”¹å˜ `blockIdx` çš„å€¼**ã€‚
- æ¢å¥è¯è¯´ï¼šå³ä¾¿æŸäº› block æš‚æ—¶è¿˜æ²¡è¢«è°ƒåº¦åˆ° SMï¼Œå®ƒä»¬é€»è¾‘ä¸Šä»ç„¶å­˜åœ¨ï¼Œå…¶ `blockIdx` å°±æ˜¯é‚£æ ·ç¡®å®šçš„ï¼›æ‰€æœ‰çº¿ç¨‹éƒ½â€œå±äºâ€è¿™ä¸ª kernel çš„ç¼–å·ç©ºé—´ã€‚

å› æ­¤ **ä¸ä¼šå‡ºç°â€œå‰ 10 ä¸ª block æ­£åœ¨å¹²åˆ«çš„äº‹ï¼Œå¯¼è‡´å¤„ç†æ•°ç»„çš„èµ·å§‹çº¿ç¨‹ä¸æ˜¯ 0â€** çš„æƒ…å†µ â€”â€” å¦‚æœä½ æ˜¯é€šè¿‡ä¸€æ¬¡ kernel è°ƒç”¨æ¥å¤„ç†è¯¥æ•°ç»„ï¼Œé‚£ä¹ˆè¿™ä¸ª kernel çš„æ¯ä¸ªé€»è¾‘çº¿ç¨‹éƒ½å±äºè¯¥ä»»åŠ¡ã€‚

2) å…³äºå¹¶å‘ï¼ˆå¤šä¸ª kernelï¼‰ä¸ stream

- å¦‚æœä½ åœ¨**ä¸åŒ stream** å¯åŠ¨å¤šä¸ª kernelï¼Œå®ƒä»¬å¯ä»¥å¹¶è¡Œè°ƒåº¦æ‰§è¡Œï¼ˆå—ç¡¬ä»¶å’Œèµ„æºé™åˆ¶ï¼‰ï¼Œä½†**æ¯ä¸ª kernel çš„ `blockIdx`/`threadIdx` ç©ºé—´ä»ç‹¬ç«‹**ï¼ˆéƒ½ä» 0 å¼€å§‹ï¼‰ã€‚

3) å°ç»“ï¼ˆä¸€å¥è¯ï¼‰

> ä¸€æ¬¡ kernel å¯åŠ¨å¯¹åº”ä¸€æ¬¡é€»è¾‘è¿ç»­çš„çº¿ç¨‹ç¼–å·ç©ºé—´ï¼ˆ`idx` ä» 0 å¼€å§‹ï¼‰ã€‚å¦‚æœä½ è¦å¤„ç†æ•°ç»„çš„å­åŒºé—´ï¼Œç”¨æŒ‡é’ˆåç§»æˆ–ä¼ å…¥ offset å°±è¡Œï¼›ä¸ä¼šæœ‰â€œå‰é¢ blocks åœ¨åšåˆ«çš„äº‹å¯¼è‡´ idx æ— æ³•å’Œæ•°ç»„ä¸‹æ ‡å¯¹é½â€çš„é—®é¢˜ï¼Œé™¤éä½ è‡ªå·±ç”¨æ›´å¤æ‚çš„å¹¶å‘/æŒä¹…åŒ–ç­–ç•¥ã€‚

---

é—®é¢˜ï¼š

> > ä¸€æ¬¡ kernel å¯åŠ¨å¯¹åº”ä¸€æ¬¡é€»è¾‘è¿ç»­çš„çº¿ç¨‹ç¼–å·ç©ºé—´ï¼ˆidx ä» 0 å¼€å§‹ï¼‰ã€‚ 
>
> è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ 
>
> > è¿™äº›é€»è¾‘ç¼–å·ä¸å®é™…ç‰©ç† SMï¼ˆstreaming multiprocessorï¼‰ä¸Šçš„è°ƒåº¦æ— å…³ï¼šCUDA è¿è¡Œæ—¶ä¼šæŠŠé€»è¾‘ä¸Šçš„ block åˆ†é…åˆ°ç‰©ç† SM å»æ‰§è¡Œï¼Œä½†è¿™åªæ˜¯è°ƒåº¦ç»†èŠ‚ï¼Œä¸ä¼šæ”¹å˜ blockIdx çš„å€¼ã€‚ 
>
> ä½ çš„æ„æ€æ˜¯ï¼Œæ¯æ¬¡è°ƒåº¦ç®—å­å¤„ç†æ•°ç»„ï¼Œéƒ½ä¼šåœ¨æœ€å¼€å§‹åœ¨ç‰©ç†å±‚é¢é€‰æ‹©ç©ºé—²çš„gridï¼Œå®é™…çš„gridå¯èƒ½å¹¶ä¸ä¸€å®šæ˜¯ä»0å¼€å§‹ï¼Œä½†æ˜¯è¿™æ˜¯èƒŒåçš„è°ƒåº¦æœºåˆ¶ï¼Œç®—å­ä»£ç å±‚é¢æ— éœ€å…³å¿ƒï¼Ÿå°±ç±»ä¼¼è™šæ‹Ÿæœºå’Œå®é™…çš„æ“ä½œç³»ç»Ÿä¸€æ ·ï¼Œä¼ å…¥ç®—å­å‡½æ•°çš„idxå…¶å®å°±æ˜¯è™šæ‹Ÿæœºï¼Œé»˜è®¤å°±æ˜¯ä»0å¼€å§‹ï¼Œä½†å…¶å®èƒŒåç‰©ç†çš„gridå¯èƒ½å¹¶ä¸æ˜¯ä»0å¼€å§‹ï¼Ÿ

ä½ ç†è§£çš„å·²ç»éå¸¸æ¥è¿‘æ­£ç¡®ç­”æ¡ˆäº†ï¼Œæˆ‘ç°åœ¨ç”¨**â€œè™šæ‹Ÿç¼–å·ç©ºé—´ vs ç‰©ç†æ‰§è¡Œå•å…ƒâ€**çš„è§’åº¦ï¼ŒæŠŠè¿™ä»¶äº‹è®²å¾—å®Œå…¨é€å½»ã€‚ä½ ä¼šçœ‹åˆ° CUDA çš„è®¾è®¡å’Œæ“ä½œç³»ç»Ÿçš„â€œè™šæ‹Ÿåœ°å€ vs ç‰©ç†åœ°å€â€å‡ ä¹ä¸€æ¨¡ä¸€æ ·ã€‚

ä¸‹é¢æ˜¯ä½ æå‡ºçš„é—®é¢˜çš„æ ¸å¿ƒï¼š

> **ä¸ºä»€ä¹ˆä¸€æ¬¡ kernel å¯åŠ¨å¯¹åº”ä¸€ä¸ªä» 0 å¼€å§‹çš„è¿ç»­çº¿ç¨‹ç¼–å·ç©ºé—´ï¼Ÿ
>  çœŸå®ç‰©ç† SM ä¸Š block å¹¶ä¸æ˜¯ä» 0 å¼€å§‹æ‰§è¡Œï¼Œé‚£ä¸ºä»€ä¹ˆä»£ç é‡Œ blockIdx ä»ä» 0 å¼€å§‹ï¼Ÿ**

ç­”æ¡ˆï¼š

ğŸš€ CUDA ä¸­çš„ blockIdx/threadIdx æ˜¯ **è™šæ‹Ÿç¼–å·ç©ºé—´**

å®ƒå®Œå…¨ç”± **CUDA runtime / driver** åœ¨ kernel å¯åŠ¨æ—¶å»ºç«‹ï¼Œä¸ç‰©ç†ç¡¬ä»¶è°ƒåº¦æ— å…³ã€‚

å°±åƒï¼š

- ç¨‹åºçœ‹åˆ°çš„è™šæ‹Ÿåœ°å€æ˜¯è¿ç»­çš„ï¼Œä½†å®é™…ç‰©ç†å†…å­˜å¯èƒ½æ•£è½å„å¤„
- CUDA kernel çœ‹åˆ°çš„ blockIdx/threadIdx æ˜¯è¿ç»­çš„ï¼Œä½†ç‰©ç† SM æ‰§è¡Œé¡ºåºå¯ä»¥ä¹±åºã€è½®æ¢ã€è·³è·ƒ

ä½ è¯´çš„æ¯”å–»å®Œå…¨æ­£ç¡®ï¼š

> **ä¼ å…¥ç®—å­å‡½æ•°çš„ idx æ˜¯â€œè™šæ‹Ÿæœºï¼ˆé€»è¾‘ç©ºé—´ï¼‰â€ï¼Œ
>  å®é™… GPU ä¸Šæ‰§è¡Œçš„æ˜¯â€œç‰©ç†æœºï¼ˆçœŸå® SMï¼‰â€ã€‚
>  CUDA ä¼šè‡ªåŠ¨åšæ˜ å°„ï¼Œä½ æ— éœ€å…³å¿ƒç‰©ç†å¸ƒå±€ã€‚**

ğŸ”¥ æœ€å…³é”®çš„äº‹å®ï¼ˆé¡¶çº§ç†è§£ï¼‰

**1. CUDA kernel çš„ grid/block/thread éƒ½æ˜¯é€»è¾‘æ¦‚å¿µï¼ˆè™šæ‹Ÿç©ºé—´ï¼‰**

å½“ä½ å†™ï¼š

```
relu_kernel<<<gridDim, blockDim>>>(...)
```

CUDA ä¼šåˆ›å»ºä¸€ä¸ª**é€»è¾‘ grid**ï¼š

- gridDim.x ä¸ª block
- æ¯ä¸ª block æœ‰ blockDim.x ä¸ª thread
- block çš„é€»è¾‘ç¼–å·å›ºå®šæ˜¯ 0,1,2,...gridDim.x-1
- thread çš„é€»è¾‘ç¼–å·å›ºå®šæ˜¯ 0..blockDim.x-1
- å…¨å±€ idx é€»è¾‘ä¸Šä» 0 é€’å¢ï¼Œè¿ç»­

è¿™äº›ç¼–å·æ˜¯ CUDA runtime **åˆ›å»ºå‡ºæ¥çš„è™šæ‹Ÿç»Ÿä¸€ç¼–å·ç©ºé—´**ã€‚

**2. ç‰©ç† SM æ˜¯æ‰§è¡Œå•ä½ï¼Œblock ä¼šè¢«è°ƒåº¦åˆ°ä»»æ„ SMï¼Œé¡ºåºä¸ä¿è¯**

æ¯”å¦‚ä½ å¯åŠ¨äº† 1000 ä¸ª blockï¼Œä½† GPU æœ‰ 80 ä¸ª SMï¼Œé‚£ä¹ˆï¼š

- SM#3 å¯èƒ½å…ˆæ‰§è¡Œ block#17
- SM#10 å¯èƒ½åŒæ—¶åœ¨æ‰§è¡Œ block#400
- SM#1 å¯èƒ½åœ¨ block#2 æ‰§è¡Œä¸€åŠè¢«æŠ¢å å»æ‰§è¡Œ block#999

æ‰§è¡Œé¡ºåºæ˜¯**å®Œå…¨ä¸å¯é¢„æµ‹çš„**
 ä½†ï¼ï¼ï¼

ğŸš¨ è¿™äº›ç‰©ç†è°ƒåº¦ä¸ä¼šæ”¹å˜ blockIdxï¼

å› ä¸º blockIdx æ˜¯è™šæ‹Ÿç¼–å·ï¼Œæ˜¯é€»è¾‘å±‚çš„å±æ€§ã€‚

å°±åƒï¼š

è™šæ‹Ÿåœ°å€ 0x1000 æŒ‡å‘é¡µå· 123ï¼Œä½† OS ä¼šæŠŠå®ƒæ˜ å°„åˆ°ç‰©ç†åœ°å€ 0x75302000ï¼›

ä½†ä½ çš„ç¨‹åºæ°¸è¿œåªçœ‹åˆ° 0x1000ï¼Œä¸ä¼šå˜ã€‚

åŒç†ï¼š

```
blockIdx.x = 15
```

æ— è®ºç‰©ç†ä¸Šè¿è¡Œåœ¨å“ªä¸ª SMã€ä»¥ä»€ä¹ˆé¡ºåºæ‰§è¡Œï¼Œéƒ½ä¸ä¼šå˜ã€‚

**3. æ‰€ä»¥ idx å½“ç„¶ä» 0 å¼€å§‹ä¸”è¿ç»­**

ä½ çš„ kernel é€»è¾‘ç©ºé—´é•¿è¿™æ ·ï¼š

```
Grid:
  Block 0: thread 0~127   â†’ idx 0~127
  Block 1: thread 0~127   â†’ idx 128~255
  Block 2: thread 0~127   â†’ idx 256~383
  ...
```

ç‰©ç†æ‰§è¡Œé¡ºåºå¯èƒ½å®Œå…¨ä¹±ï¼Œä½†å¯¹ä½ å†™ kernel çš„ä»£ç æ¥è¯´ï¼š

```
idx æ°¸è¿œä» 0 å¼€å§‹ä¸”è¿ç»­
```

å¹¶ä¸”ï¼š

**CUDA ä¿è¯ blockIdx/threadIdx çš„å€¼æ°¸è¿œä¸å—ç¡¬ä»¶è°ƒåº¦å½±å“ã€‚**

**è¿™æ˜¯ API å¥‘çº¦ã€‚**

ğŸ¯ å›åˆ°ä½ çš„ç›´æ¥é—®é¢˜

> ä½ çš„æ„æ€æ˜¯ï¼Œç®—å­ä»£ç å±‚é¢çœ‹åˆ°çš„ idx æ˜¯è™šæ‹Ÿç¼–å·ï¼Œç‰©ç† grid å¯èƒ½ä¸æ˜¯ä» 0 å¼€å§‹ï¼Ÿ

**æ˜¯çš„ï¼Œè¿™è¯ä½ èƒ½è¿™æ ·ç†è§£ï¼š**

- **ä»£ç å±‚çœ‹åˆ°çš„æ˜¯é€»è¾‘ grid â†’ å¿…ç„¶ä» 0 å¼€å§‹**
- **ç‰©ç† GPU æ‰§è¡Œçš„ block æ˜¯æŒ‰ SM è°ƒåº¦ â†’ é¡ºåºä¸ä¿è¯**
- CUDA ä¼šè‡ªåŠ¨æŠŠé€»è¾‘ grid æ˜ å°„åˆ°ç‰©ç† SM

ä½ è¯´çš„ç±»æ¯”éå¸¸å‡†ç¡®ï¼š

> åƒè™šæ‹Ÿæœºä¸­çš„è™šæ‹Ÿåœ°å€ä¸€æ ·ï¼Œæˆ‘ä»¬è®¿é—®çš„ idx æ˜¯è™šæ‹Ÿé€»è¾‘ç¼–å·ï¼Œä» 0 å¼€å§‹ï¼›
>  ç‰©ç†ç¡¬ä»¶å®é™…æ‰§è¡Œçš„ block åˆ†å¸ƒæ˜¯éšæœºçš„ï¼Œä¸æ˜¯ä» 0 å¼€å§‹ã€‚

CUDA runtime æ˜¯â€œæ“ä½œç³»ç»Ÿâ€ï¼Œè´Ÿè´£æ˜ å°„è¿™ä¸¤ä¸ªä¸–ç•Œã€‚

â­ ä¸ºä»€ä¹ˆ CUDA è¦è¿™æ ·è®¾è®¡ï¼Ÿï¼ˆæ·±å±‚åŸå› ï¼‰

å¦åˆ™ä½ å¿…é¡»è‡ªå·±å†™è¿™ç§ä»£ç ï¼š

- å½“å‰ block æ˜¯ç¬¬å‡ ä¸ªï¼Ÿ
- æ˜¯å¦è¿è¡Œåœ¨å‰é¢å…¶ä»– block ä¹‹åï¼Ÿ
- å¦‚ä½•ä¿è¯æ‰€æœ‰çº¿ç¨‹åœ¨æ­£ç¡®çš„æ•°æ®ç´¢å¼•ä¸Šï¼Ÿ
- block æ˜¯å¦è¢«é‡å¤è°ƒåº¦ï¼Ÿ

è¿™ä¼šè®©ç¨‹åºå‘˜ç–¯æ‰ã€‚

æ‰€ä»¥ CUDA éšè—äº† SM çš„ç‰©ç†æ‰§è¡Œç»†èŠ‚ï¼ŒæŠŠæ‰€æœ‰çº¿ç¨‹æ”¾åˆ°ç»Ÿä¸€ç¼–å·ç©ºé—´é‡Œï¼Œä¿è¯ï¼š

- ç¨‹åºçœ‹åˆ°çš„ blockIdx/threadIdx æ˜¯ç¡®å®šçš„
- ç¡¬ä»¶å¦‚ä½•è°ƒåº¦ä½ ä¸å¿…å…³å¿ƒ
- æ•°æ®æ˜ å°„ï¼ˆidxâ†â†’æ•°ç»„ä¸‹æ ‡ï¼‰éå¸¸ç®€å•

è¿™å°±æ˜¯â€œæ¼‚äº®è®¾è®¡â€ã€‚

ğŸ”¥ æœ€ç»ˆä¸€å¥æ€»ç»“

> **ä¸€æ¬¡ kernel = ä¸€ä¸ªé€»è¾‘ gridï¼ˆè™šæ‹Ÿç©ºé—´ï¼‰
>  blockIdx/threadIdx æ˜¯è™šæ‹Ÿç¼–å·ï¼Œä» 0 å¼€å§‹è¿ç»­
>  ç‰©ç† SM å¦‚ä½•æ‰§è¡Œä¸å½±å“é€»è¾‘ç¼–å·
>  CUDA runtime è‡ªåŠ¨åšæ˜ å°„**

æ‰€ä»¥ä½ å¯ä»¥å¤§èƒ†åœ°è®©ï¼š

```
idx = blockIdx.x * blockDim.x + threadIdx.x
if (idx < n) y[idx] = fmaxf(...);
```

æ°¸è¿œæ­£ç¡®ã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘èƒ½ç»§ç»­ç”¨ç”»å›¾çš„æ–¹å¼ï¼ˆéå¸¸ç›´è§‚ï¼‰è¯´æ˜ï¼š

ğŸ–¼ â€œè™šæ‹Ÿ gridï¼ˆblockIdxï¼‰â€

vs

âš™ â€œç‰©ç† SM æ‰§è¡Œé¡ºåºâ€

ä¸‹é¢æˆ‘ä¼šç»™ä½ ç”»å‡ºï¼š

1. **é€»è¾‘ gridï¼ˆè™šæ‹Ÿç©ºé—´ï¼‰æ˜¯å¦‚ä½•ç¼–å·çš„**
2. **ç‰©ç† SM å¦‚ä½•éšæœºæ‰§è¡Œ block**
3. **CUDA runtime å¦‚ä½•æŠŠé€»è¾‘ç¼–å·æ˜ å°„åˆ°ç‰©ç†æ‰§è¡Œ**
4. **æœ€ç»ˆä¸ºä»€ä¹ˆ idx å¿…ç„¶ä» 0 å¼€å§‹**

è¯·ä½ è€å¿ƒçœ‹å®Œè¿™ä¸€èŠ‚ï¼Œä½ ä¼šå½»åº•ç†è§£ CUDA æœ€æ ¸å¿ƒçš„æ‰§è¡Œæ¨¡å‹ã€‚

1. é€»è¾‘ä¸–ç•Œï¼ˆè™šæ‹Ÿç©ºé—´ï¼‰ï¼šblockIdx å¿…ç„¶ä» 0 è¿ç»­å¢é•¿

å½“ä½ å†™ï¼š

```
relu_kernel<<<gridDim=4, blockDim=128>>>(...)
```

CUDA ä¼šåˆ›å»ºâ€œé€»è¾‘ gridâ€ï¼š

```
é€»è¾‘ Gridï¼ˆè™šæ‹Ÿï¼‰ï¼š
  Block 0
  Block 1
  Block 2
  Block 3
```

æ¯ä¸ª block é‡Œé¢çš„ threadIdx.x ä¹Ÿæ˜¯é€»è¾‘åœ°å€ï¼š

```
Block 0:
  thread 0
  thread 1
  ...
  thread 127

Block 1:
  thread 0
  thread 1
  ...
  thread 127
```

**è¿™é‡Œ blockIdx å’Œ threadIdx æ˜¯ runtime ç»™ä½ çš„ \*è™šæ‹Ÿç¼–å·ç©ºé—´\***

å®ƒä»¬çš„ç¼–å·å¹¶ä¸è¡¨ç¤º GPU ç‰©ç†ç¡¬ä»¶ä¸Šå‘ç”Ÿçš„äº‹æƒ…ã€‚

2. ç‰©ç†ä¸–ç•Œï¼ˆçœŸå® GPUï¼‰ï¼šSM ä»¥ä»»æ„é¡ºåºæ‰§è¡Œ block

å‡è®¾ GPU åªæœ‰ 2 ä¸ª SMï¼ˆå®é™…ä¸Šå‡ åä¸ªï¼‰ã€‚

ç‰©ç†è°ƒåº¦å¯èƒ½çœ‹èµ·æ¥åƒè¿™æ ·ï¼š

```
ç‰©ç† SM æ‰§è¡Œé¡ºåºï¼ˆçœŸå®ï¼‰ï¼š

SM0 å…ˆè¿è¡Œé€»è¾‘ Block 2
SM1 å…ˆè¿è¡Œé€»è¾‘ Block 0
SM0 æ‰§è¡Œå®Œ Block 2ï¼Œåˆè¿è¡Œ Block 3
SM1 æ‰§è¡Œå®Œ Block 0ï¼Œåˆè¿è¡Œ Block 1
```

æ³¨æ„ï¼š

- ç‰©ç†æ‰§è¡Œé¡ºåºå®Œå…¨ä¹±åº
- blockIdx *ä¸ä¼šå› ä¸ºæ‰§è¡Œé¡ºåºæ”¹å˜*

3. CUDA çš„æ ¸å¿ƒæœºåˆ¶ï¼šé€»è¾‘ç¼–å· â†’ ç‰©ç†æ‰§è¡Œ çš„æ˜ å°„

ä½ å¯ä»¥æŠŠç†è§£æˆå¦‚ä¸‹æŠ½è±¡ï¼š

```
é€»è¾‘ BlockIdx    â†’ï¼ˆç”± CUDA runtime æ˜ å°„ï¼‰â†’   ç‰©ç† SM æ‰§è¡Œé¡ºåº
```

ä¸¾ä¸ª ASCII æµç¨‹ï¼š

```
é€»è¾‘ç¼–å·ç©ºé—´ï¼š
  Block 0
  Block 1
  Block 2
  Block 3

           |
           | CUDA runtime åšæ˜ å°„ï¼ˆä½ çœ‹ä¸åˆ°ï¼‰
           v

ç‰©ç†æ‰§è¡Œé¡ºåºï¼ˆä¸å¯é¢„æµ‹ï¼‰ï¼š
  SM0 â† Block 2
  SM1 â† Block 0
  SM1 â† Block 1
  SM0 â† Block 3
```

å…³é”®ç‚¹ï¼š

> è™½ç„¶ç‰©ç†æ‰§è¡Œæ˜¯ä¹±çš„ï¼Œä½† **blockIdx.x åœ¨ kernel å†…æ°¸è¿œå°±æ˜¯é€»è¾‘ç¼–å·ï¼Œä¸ä¼šå˜**ã€‚

å“ªæ€• Block 2 æ˜¯ç¬¬ä¸€ä¸ªè¢«ç‰©ç† SM æ‰§è¡Œçš„ï¼Œåœ¨ kernel å†…å®ƒä»ç„¶èƒ½è¯»åˆ°ï¼š

```
blockIdx.x == 2
```

è¿™å°±æ˜¯â€œè™šæ‹Ÿç©ºé—´â€çš„æ¦‚å¿µï¼Œä¸çœŸå®æ‰§è¡Œé¡ºåºæ— å…³ã€‚

4. æœ€å…³é”®ï¼šå…¨å±€ thread indexï¼ˆidxï¼‰ = é€»è¾‘ç¼–å·æ¨å‡ºæ¥çš„

idx çš„å…¬å¼ï¼š

```
idx = blockIdx.x * blockDim.x + threadIdx.x;
```

å› ä¸º blockIdx ç”± **é€»è¾‘ç¼–å·ç©ºé—´** å†³å®š
 ï¼ˆä¸æ˜¯ç‰©ç†è°ƒåº¦å†³å®šï¼‰

æ‰€ä»¥ idx ä¸€å®šæ˜¯ 0,1,2,3... è¿ç»­å¢é•¿ã€‚

å“ªæ€• Block 2 æœ€å…ˆè¢«æ‰§è¡Œï¼Œå®ƒå†…éƒ¨çš„çº¿ç¨‹ä»ç„¶è¯´ï¼š

```
idx = 2 * 128 + threadIdx.x        // 256 åˆ° 383
```

å®ƒä¸ä¼šå˜æˆâ€œä» 0 å¼€å§‹â€ï¼Œä¹Ÿä¸ä¼šå› ä¸ºç‰©ç†é¡ºåºè€Œæ”¹å˜ã€‚

5. è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ idx å¿…é¡»ä» 0 å¼€å§‹ï¼Œä¸”è¿ç»­

æ ¸å¿ƒåŸå› æ€»ç»“ï¼š

1. **blockIdx/threadIdx æ˜¯ CUDA runtime ç»™å‡ºçš„é€»è¾‘è™šæ‹Ÿç¼–å·**
2. ç‰©ç† SM æ‰§è¡Œé¡ºåºå†ä¹±ï¼Œä¹Ÿä¸ä¼šå½±å“é€»è¾‘ç¼–å·
3. é€»è¾‘ grid ä¸­ blockIdx ä» 0~gridDim.x-1 è¿ç»­
4. å› æ­¤ idx ä» 0 å¼€å§‹è¿ç»­
5. æ‰€ä»¥å¯ä»¥å®‰å…¨ä½¿ç”¨ `idx < n`

6. å›ç­”ä½ æå‡ºçš„å…³é”®ç±»æ¯”

ä½ è¯´ï¼š

> æ˜¯ä¸æ˜¯å°±åƒè™šæ‹Ÿæœºä¸€æ ·ï¼Œä¼ å…¥ç®—å­å‡½æ•°çš„ idx æ˜¯è™šæ‹Ÿç¼–å·ï¼Œä» 0 å¼€å§‹ï¼Ÿ
>  å®é™…ç‰©ç† grid å¯èƒ½ä¸æ˜¯ä» 0 å¼€å§‹ï¼Ÿ

æ­£ç¡®ï¼Œè€Œä¸”éå¸¸å‡†ç¡®ï¼

ä½ å¯ä»¥æŠŠ CUDA çš„æ‰§è¡Œè¿‡ç¨‹ç†è§£ä¸ºï¼š

```
è™šæ‹Ÿ Gridï¼ˆblockIdx ä» 0 å¼€å§‹ï¼‰
      â†“ æ˜ å°„
ç‰©ç† SMï¼ˆæ‰§è¡Œé¡ºåºéšæœºï¼‰
```

å°±åƒï¼š

```
è™šæ‹Ÿå†…å­˜åœ°å€ï¼ˆ0x0000...ï¼‰ 
      â†“ æ˜ å°„
ç‰©ç†å†…å­˜åœ°å€ï¼ˆéšæœºåˆ†å¸ƒï¼‰
```

7. æœ€ç»ˆä¸€å¥æ€»ç»“ï¼ˆå†™ç®—å­çš„äººå¿…é¡»ç‰¢è®°ï¼‰

> **blockIdx/threadIdx/idx æ˜¯è™šæ‹Ÿç¼–å·ï¼Œç”± CUDA runtime ä¿è¯ä» 0 å¼€å§‹è¿ç»­ã€‚
>  ç‰©ç†æ‰§è¡Œé¡ºåºå®Œå…¨ç‹¬ç«‹ï¼Œä¸ä¼šå½±å“é€»è¾‘ç¼–å·ã€‚
>  æ‰€ä»¥ä¸€ä¸ª kernel çš„ idx æ€»æ˜¯ä» 0 å¼€å§‹ï¼Œä¸æ•°ç»„ç´¢å¼•å¤©ç„¶å¯¹é½ã€‚**

é—®é¢˜ï¼š

> æ—¢ç„¶å®é™…æ˜¯ä¹±åºæ‰§è¡Œçš„ï¼Œé‚£æ˜¯ä¸æ˜¯ç®—å­å¿…é¡»ä¿è¯æ˜¯å¯å¹¶è¡Œçš„ï¼Œå³é¡ºåºæ— å…³çš„ï¼Ÿ

æ˜¯çš„ï¼Œä½ æŠ“åˆ°æ ¸å¿ƒç‚¹äº†ï¼š

**ç”±äº CUDA kernel å†…çš„ thread å’Œ block æ‰§è¡Œé¡ºåºæ˜¯å®Œå…¨ä¸å¯æ§ï¼ˆä¹±åºã€å¹¶è¡Œã€é‡æ’ã€äº¤é”™æ‰§è¡Œï¼‰â€”â€”å› æ­¤æ‰€æœ‰ GPU kernel éƒ½å¿…é¡»ä¿è¯ï¼š**

âœ… **kernel é€»è¾‘å¿…é¡»æ˜¯â€œé¡ºåºæ— å…³â€çš„ï¼ˆorder-independentï¼‰**

æ¢å¥è¯è¯´ï¼š

> **ä»»ä½• thread åœ¨ä»»ä½•æ—¶åˆ»æ‰§è¡Œï¼Œéƒ½ä¸èƒ½å½±å“å…¶ä»– thread æ­£ç¡®æ€§ã€‚**

GPU æ˜¯é«˜åº¦å¹¶è¡Œæ¶æ„ï¼Œä¸ä¿è¯ï¼š

- å“ªä¸ª block å…ˆæ‰§è¡Œ
- å“ªä¸ª thread å…ˆæ‰§è¡Œ
- å“ªä¸ª thread ä¼šè¢«æš‚åœã€æ¢å¤
- å“ªä¸ªä¼šè¢«è°ƒåº¦åˆ°å“ªä¸ª SM
- å“ªäº› warp ä¼šè¢«åˆå¹¶æ‰§è¡Œ

ä½ ä¸èƒ½ä¾èµ–ä»»ä½• thread/warp çš„é¡ºåºå…³ç³»ã€‚

è¿™å°±æ˜¯ CUDA kernel çš„æ ¸å¿ƒçº¦æŸä¹‹ä¸€ã€‚

â— é‚£å“ªäº›ç®—å­æ˜¯â€œé¡ºåºæ— å…³â€çš„ï¼Ÿ

å…¸å‹ç¬¦åˆ GPU å‹å¥½çš„ç®—å­ï¼š

âœ” æ¯ä¸ªå…ƒç´ ç‹¬ç«‹è®¡ç®—ï¼ˆelementwiseï¼‰

å¦‚ä½ çœ‹åˆ°çš„ reluã€addã€mulã€sigmoidï¼š

```
y[idx] = f(x[idx]);
```

æ¯ä¸ª thread åªå¤„ç†è‡ªå·±çš„ idxï¼Œä¸ä¾èµ–åˆ«äººã€‚

âœ” Reduce æ“ä½œï¼ˆsum/max/minï¼‰

å¦‚ï¼š

- sum
- max reduce
- softmax çš„å‰åŠéƒ¨åˆ† reduce

å‡å°‘æ“ä½œéœ€è¦ç²¾å¿ƒå†™ï¼Œä½†ä¹Ÿèƒ½ä¿è¯æ— é¡ºåºä¾èµ–ï¼ˆä¾‹å¦‚ warp shuffle æˆ– block reduceï¼‰ã€‚

âœ” å·ç§¯ï¼ˆconvï¼‰

è¯»é‚»åŸŸï¼Œä½†å¯ä»¥ä¿è¯æ¯ä¸ªè¾“å‡ºåƒç´ ç‹¬ç«‹è®¡ç®—ã€‚

âŒ é‚£å“ªäº›ç®—å­ä¸èƒ½ä¹±åºï¼Ÿ

å¦‚æœä½ å†™å‡ºè¿™æ ·ï¼š

```
y[idx] = y[idx-1] + x[idx];
```

ä½ å°±æ­»å®šäº†â€¦â€¦

å› ä¸º idx=10 çš„çº¿ç¨‹å¯èƒ½å…ˆæ‰§è¡Œï¼Œè€Œ idx=9 çš„çº¿ç¨‹è¿˜æ²¡å†™å®Œ y[9]

-> **ç»“æœéšæœº**ï¼Œ**é”™è¯¯**ã€‚

è¿™ç±»å°±å« **é¡ºåºç›¸å…³ï¼ˆsequential-dependentï¼‰**ã€‚

GPU æ— æ³•ä¿è¯é¡ºåºï¼Œåªèƒ½ä¿è¯ï¼š

> æ¯ä¸ª thread çš„ä»£ç æ˜¯ä¸²è¡Œçš„
>  ä½† thread ä¹‹é—´æ²¡æœ‰ä»»ä½•é¡ºåºä¿è¯

â“ é‚£å†…å­˜å†™å†²çªæ€ä¹ˆåŠï¼Ÿ

å†æ¯”å¦‚è¿™ä¸ªï¼š

```
y[0] += x[idx];   // æ‰€æœ‰ thread éƒ½å†™ y[0]
```

è¿™ä¹Ÿä¸è¡Œâ€”â€”ä¼šå‡ºç°å¹¶å‘å†²çªï¼ˆrace conditionï¼‰
 ä½ éœ€è¦ç”¨ï¼š

- `atomicAdd`
- æˆ–è€…ç”¨ block reduction å† atomic ä¸€æ¬¡

æ‰èƒ½ä¿è¯æ­£ç¡®æ€§ã€‚

âœ¨ ä½ ç°åœ¨å®Œå…¨ç†è§£ CUDA ç¼–ç¨‹æ¨¡å‹äº†å—ï¼Ÿ

æ€»ç»“ä¸ºä¸€å¥è¯ï¼š

ğŸ¯ **CUDA kernel = å¤§é‡çº¿ç¨‹å¹¶è¡Œæ‰§è¡Œï¼Œçº¿ç¨‹ä¹‹é—´é¡ºåºå®Œå…¨ä¸å¯æ§ï¼Œå› æ­¤ kernel å¿…é¡»æ˜¯â€œå¹¶è¡Œå®‰å…¨ã€é¡ºåºæ— å…³â€çš„ã€‚**

é—®é¢˜ï¼š

> é‚£å…¶å®å°±æ˜¯ä¸ªå¹¶å‘ç¼–ç¨‹ï¼Œåªä¸è¿‡æ˜¯è¢«ç”¨åœ¨gpuä¸Šæ‰§è¡Œï¼Œå…·ä½“çš„å¹¶å‘ä½ ä¸ç”¨ç®¡ï¼Œä½ åªéœ€è¦å†™å¥½å¹¶å‘çš„æ¯ä¸ªçº¿ç¨‹çš„å‡½æ•°å¤„ç†éƒ¨åˆ†å°±å¥½äº†ã€‚èƒ½è¿™æ ·ç†è§£å—ï¼Ÿ

å¯ä»¥ï¼Œè€Œä¸” **è¿™æ˜¯æœ€æ­£ç¡®ã€æœ€æ¥åœ°æ°”çš„ç†è§£æ–¹å¼ä¹‹ä¸€**ã€‚

ä½ å¯ä»¥ç›´æ¥æŠŠ CUDA kernel ç†è§£ä¸ºï¼š

ğŸ¯ **â€œå†™ä¸€ä¸ªèƒ½åœ¨æˆåƒä¸Šä¸‡ä¸ªçº¿ç¨‹ä¸Šå¹¶å‘è¿è¡Œçš„å‡½æ•°ï¼ˆthread functionï¼‰â€**

ç„¶åï¼š

- **æ¯ä¸ªçº¿ç¨‹æœ‰è‡ªå·±çš„ threadIdx / blockIdx ç”¨æ¥åŒºåˆ†è‡ªå·±**
- **æ¯ä¸ªçº¿ç¨‹åšè‡ªå·±çš„å°ä»»åŠ¡**
- **çº¿ç¨‹ä¹‹é—´å®Œå…¨ä¸ä¿è¯æ‰§è¡Œé¡ºåº**
- **ä½ å†™çš„ä»£ç ä¸èƒ½ä¾èµ–å…¶ä»–çº¿ç¨‹çš„æ‰§è¡Œé¡ºåº**

è¿™è·Ÿ CPU ä¸Šçš„å¹¶å‘ç¼–ç¨‹æ˜¯åŒä¸€æ€æƒ³ï¼Œåªä¸è¿‡ CUDAï¼š

- ä¸æ˜¯ 8 ä¸ªçº¿ç¨‹ï¼Œè€Œæ˜¯ 10,000+ ä¸ªçº¿ç¨‹
- è¿™äº›çº¿ç¨‹æ˜¯ SIMTï¼ˆæ‰¹å¤„ç†å¼æ‰§è¡Œï¼‰
- è°ƒåº¦ç”± GPU ç»Ÿä¸€ç®¡ç†
- ä¸èƒ½é”ï¼ˆmutexï¼‰ï¼ŒåŒæ­¥æ‰‹æ®µéå¸¸æœ‰é™ï¼ˆåªèƒ½ block å†…åŒæ­¥ï¼‰

æ‰€ä»¥ä½ çš„ç†è§£å¯ä»¥æ€»ç»“ä¸ºä¸€å¥è¯ï¼š

âœ… **CUDA = massively parallel concurrency programmingï¼ˆå¤§è§„æ¨¡å¹¶å‘ç¼–ç¨‹ï¼‰**

ä½ åªéœ€è¦ï¼š

- å†™ä¸€ä¸ª **æ¯ä¸ª thread è‡ªå·±èƒ½ç‹¬ç«‹å®Œæˆä»»åŠ¡** çš„å‡½æ•°
- ä¿è¯ **ä¸éœ€è¦ thread é¡ºåº**
- ä¿è¯ **æ²¡æœ‰è·¨ thread çš„ä¾èµ–/å†²çª**

CUDA runtime ä¼šè‡ªåŠ¨ï¼š

- æŠŠè¿™äº› thread æ˜ å°„åˆ°å®é™…ç¡¬ä»¶ä¸Š
- è°ƒåº¦åˆ° SM é‡Œæ‰§è¡Œ
- ç®¡ç† warp
- ç®¡ç†å¯„å­˜å™¨ã€å…±äº«å†…å­˜ç­‰èµ„æº

ä½ å®Œå…¨ä¸ç”¨å…³å¿ƒå…·ä½“å“ªä¸ª thread åœ¨å®é™…ç¡¬ä»¶ä¸Šè·‘åœ¨å“ªä¸ªæ ¸å¿ƒã€‚

å¯¹ä½ å½“å‰ä»»åŠ¡ï¼ˆå†™ç®—å­ï¼‰æ¥è¯´

å°±æ˜¯ï¼š

1. **æ‰¾åˆ°å…ƒç´ å¹¶è¡Œæ–¹å¼**
    ä¸€ä¸ª thread â†’ ä¸€ä»½æ•°æ®ã€‚
2. **ç®— row/col å¯¹åº”å…³ç³»ï¼ˆæ¯”å¦‚ matmulã€convï¼‰**
    thread è´Ÿè´£ (i,j)
3. **é¿å…å…±äº«å†™å†²çª**
    é™¤éä½ ç”¨ atomic æˆ–è€… reduceã€‚
4. **ä¸ä¾èµ–é¡ºåº**

åšåˆ°è¿™å‡ ç‚¹ï¼š

> ä½ å·²ç»èƒ½å†™ 80% çš„æ·±åº¦å­¦ä¹ ç®—å­äº†ï¼ˆreluã€addã€mulã€broadcastã€matmul blockã€layernormã€softmaxâ€¦â€¦ï¼‰

é—®é¢˜ï¼š

> ä¸èƒ½é”ï¼ˆmutexï¼‰ï¼ŒåŒæ­¥æ‰‹æ®µéå¸¸æœ‰é™ï¼ˆåªèƒ½ block å†…åŒæ­¥ï¼‰ åŒæ­¥æ‰‹æ®µéå¸¸æœ‰é™ï¼ˆåªèƒ½ block å†…åŒæ­¥ï¼‰ è¿™å¥è¯æ˜¯å•¥æ„æ€ï¼Ÿ

æˆ‘æ¥ä»”ç»†è§£é‡Šè¿™å¥è¯ï¼Œå®ƒæ˜¯ CUDA å¹¶è¡Œç¼–ç¨‹é‡Œéå¸¸æ ¸å¿ƒçš„æ¦‚å¿µï¼Œä½ å®Œå…¨éœ€è¦ç†è§£å®ƒæ‰èƒ½å†™å¯é çš„ç®—å­ã€‚

1ï¸âƒ£ GPU çš„çº¿ç¨‹å±‚çº§

CUDA çš„çº¿ç¨‹ç»„ç»‡åˆ†ä¸ºä¸‰å±‚ï¼š

```
Gridï¼ˆç½‘æ ¼ï¼‰           â†’ ä¸€æ¬¡ kernel å¯åŠ¨åˆ›å»ºçš„æ‰€æœ‰ block
 â””â”€ Blockï¼ˆçº¿ç¨‹å—ï¼‰    â†’ æ¯ä¸ª block æœ‰ blockDim.x ä¸ªçº¿ç¨‹
     â””â”€ Threadï¼ˆçº¿ç¨‹ï¼‰ â†’ æ¯ä¸ª thread æœ‰ threadIdx.x
```

- **çº¿ç¨‹å†…ï¼ˆthreadï¼‰**ï¼šæœ€å°æ‰§è¡Œå•ä½ï¼Œé¡ºåºæ˜¯ç¡®å®šçš„
- **block å†…ï¼ˆthreads in a blockï¼‰**ï¼šå…±äº«å¯„å­˜å™¨/å…±äº«å†…å­˜ï¼Œå¯ä»¥åŒæ­¥
- **block ä¹‹é—´**ï¼šå®Œå…¨ç‹¬ç«‹ï¼Œæ²¡æœ‰åŒæ­¥æœºåˆ¶ï¼Œä¹Ÿä¸ä¿è¯é¡ºåº

2ï¸âƒ£ ä¸ºä»€ä¹ˆè¯´â€œåªèƒ½ block å†…åŒæ­¥â€ï¼Ÿ

CUDA æä¾›çš„åŒæ­¥å‡½æ•°åªæœ‰ï¼š

```
__syncthreads();
```

- ä½œç”¨ï¼š**åœ¨åŒä¸€ä¸ª block å†…ï¼Œè®©æ‰€æœ‰çº¿ç¨‹åœ¨è¿™ä¸€ç‚¹ç­‰å¾…ï¼Œç›´åˆ° block å†…æ¯ä¸ªçº¿ç¨‹éƒ½åˆ°è¾¾æ­¤åŒæ­¥ç‚¹**
- **é™åˆ¶**ï¼šåªèƒ½åŒæ­¥åŒä¸€ä¸ª block å†…çš„çº¿ç¨‹
- **åŸå› **ï¼šä¸åŒ block å¯èƒ½è¢«è°ƒåº¦åˆ°ä¸åŒ SM ä¸Šï¼Œç”šè‡³å¯èƒ½å»¶è¿Ÿæ‰§è¡Œæˆ–æš‚åœ
  - SM ä¹‹é—´æ²¡æœ‰å…±äº«è°ƒåº¦çŠ¶æ€
  - GPU ç¡¬ä»¶ä¸æä¾›å…¨å±€ barrier
- æ‰€ä»¥å¦‚æœä½ åœ¨ kernel é‡Œå°è¯•è·¨ block åŒæ­¥ï¼Œæ˜¯ä¸å¯èƒ½çš„

3ï¸âƒ£ ä»£ç ä¸¾ä¾‹

å‡è®¾ block å†… 4 ä¸ªçº¿ç¨‹ï¼š

```
__shared__ float temp[4];

int tid = threadIdx.x;
temp[tid] = tid * 2;

// åŒæ­¥ block å†…çº¿ç¨‹
__syncthreads();

// ç°åœ¨å¯ä»¥å®‰å…¨è¯»å– temp
if (tid == 0) {
    float sum = temp[0] + temp[1] + temp[2] + temp[3];
}
```

âœ… å¯ä»¥ä¿è¯ temp ä¸­çš„æ•°æ®éƒ½æ˜¯ block å†…çº¿ç¨‹å†™å¥½çš„ã€‚

ä½†æ˜¯ **è·¨ block**ï¼š

```
if (blockIdx.x == 0) {
    y[0] = ...;
}
if (blockIdx.x == 1) {
    y[0] += ...;   // è¯•å›¾å’Œ block 0 åŒæ­¥
}
```

âŒ ä¸å®‰å…¨ï¼Œå› ä¸º block 1 æ‰§è¡Œæ—¶ block 0 å¯èƒ½è¿˜æ²¡å†™å®Œ y[0]ï¼ŒGPU ä¸ä¼šç­‰å¾…ã€‚

4ï¸âƒ£ å…³é”®ç†è§£

> â€œåŒæ­¥æ‰‹æ®µéå¸¸æœ‰é™ï¼ˆåªèƒ½ block å†…åŒæ­¥ï¼‰â€ =
>  ä½ åªèƒ½åœ¨åŒä¸€ä¸ª block å†…ä½¿ç”¨ `__syncthreads()` åš barrierï¼›
>  è·¨ block çš„åŒæ­¥å¿…é¡»é€šè¿‡ **åˆ†ä¸¤æ¬¡ kernel** æˆ–è€… **atomic æ“ä½œ** æ¥å®ç°ã€‚

ä¹Ÿå°±æ˜¯ï¼š

- **block å†…** â†’ å…±äº«å†…å­˜ + barrier å¯ç”¨
- **block é—´** â†’ ä¸èƒ½ barrierï¼ŒåŒæ­¥é  kernel launch æˆ–åŸå­æ“ä½œ

5ï¸âƒ£ å¯¹ç®—å­è®¾è®¡çš„å½±å“

- Elementwise æ“ä½œï¼ˆrelu/add/mulï¼‰ â†’ æ²¡é—®é¢˜
- Reductionï¼ˆsum/max/meanï¼‰ â†’
  - block å†… reduction â†’ __syncthreads() å¯ä»¥
  - block é—´ reduction â†’ éœ€è¦ atomic æˆ–åˆ†ä¸¤æ¬¡ kernel

#### ä¸åŒä»£ç çš„å†™æ³•çš„åŒºåˆ«

é—®é¢˜ï¼š

> ä¸ºä»€ä¹ˆæœ‰çš„èµ„æ–™ä»‹ç»æ—¶ï¼Œæ˜¯ï¼š
>
> ```c++
> __global__ void relu_kernel(const float* x, float* y, int n) {
>     int idx = blockIdx.x * blockDim.x + threadIdx.x;
>     if (idx < n) y[idx] = fmaxf(x[idx], 0.0f);
> }
> ```
>
> ä½†æ˜¯å®é™…å†™ä»£ç æ—¶ï¼Œåˆå˜æˆäº†ï¼š
>
> ```c++
> __global__ void relu_kernel(const float* __restrict__ x,
>                             float* __restrict__ y,
>                             int n) {
>     int idx = blockIdx.x * blockDim.x + threadIdx.x;
>     if (idx < n) {
>         float val = x[idx];
>         y[idx] = val > 0 ? val : 0.f;
>     }
> }
> ```

å®Œå…¨å¯ä»¥ç†è§£ä½ çš„å›°æƒ‘ï¼Œè¿™é‡Œæ¶‰åŠä¸¤ä¸ªç‚¹ï¼š**`__restrict__` çš„ä½¿ç”¨** å’Œ **kernel å†…å†™æ³•ä¼˜åŒ–**ã€‚æˆ‘æŠŠå®ƒæ‹†å¼€è¯¦ç»†è®²æ¸…æ¥šï¼Œè®©ä½ å®Œå…¨ç†è§£ä¸ºä»€ä¹ˆçœ‹èµ·æ¥æ˜¯åŒä¸€ä¸ªç®—å­ï¼Œä½†å†™æ³•å·®åˆ«å¾ˆå¤§ã€‚

1ï¸âƒ£ `__restrict__` çš„ä½œç”¨

```
const float* __restrict__ x
float* __restrict__ y
```

- `__restrict__` æ˜¯ C/C++/CUDA çš„å…³é”®å­—ï¼Œç”¨æ¥å‘Šè¯‰ç¼–è¯‘å™¨ï¼š

> â€œæˆ‘ä¿è¯åœ¨è¿™ä¸ª kernel ä¸­ï¼ŒæŒ‡é’ˆ xã€y æ‰€æŒ‡çš„å†…å­˜åŒºåŸŸä¸ä¼šé‡å ï¼ˆalias-freeï¼‰ã€‚â€

- ä¸ºä»€ä¹ˆé‡è¦ï¼ŸGPU ç¼–è¯‘å™¨ï¼ˆnvccï¼‰ä¼šæ ¹æ®è¿™ä¸ªä¿¡æ¯åš**æ›´æ¿€è¿›çš„ä¼˜åŒ–**ï¼š
  - å¯ä»¥å®‰å…¨åœ°å°† x[idx] æå‰åŠ è½½åˆ°å¯„å­˜å™¨
  - å¯ä»¥é‡æ’åºå†…å­˜è®¿é—®ï¼Œä¸ç”¨æ‹…å¿ƒè¯»å†™å†²çª
  - æé«˜å†…å­˜å¸¦å®½åˆ©ç”¨ç‡å’ŒæŒ‡ä»¤å¹¶è¡Œåº¦

å¦‚æœä¸åŠ  `__restrict__`ï¼Œç¼–è¯‘å™¨ä¸ºäº†å®‰å…¨ï¼Œä¼šå‡è®¾ x å’Œ y å¯èƒ½é‡å ï¼Œä»è€Œé™ä½ä¼˜åŒ–ï¼ˆä¾‹å¦‚é¿å…æŠŠ x[idx] æå‰åŠ è½½ï¼Œå› ä¸ºå†™ y[idx] å¯èƒ½å½±å“ x[idx]ï¼‰ã€‚

âœ… æ‰€ä»¥ `__restrict__` æ˜¯**æ€§èƒ½ä¼˜åŒ–**ï¼Œä¸æ”¹å˜é€»è¾‘ï¼Œåªå‘Šè¯‰ç¼–è¯‘å™¨â€œæ”¾å¿ƒå¤§èƒ†ä¼˜åŒ–â€ã€‚

2ï¸âƒ£ ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ `fmaxf`ï¼Ÿ

åŸå§‹å†™æ³•ï¼š

```
y[idx] = fmaxf(x[idx], 0.0f);
```

- `fmaxf` æ˜¯ CUDA å†…ç½®çš„æµ®ç‚¹ max å‡½æ•°
- å†…éƒ¨å¯èƒ½æœ‰åˆ†æ”¯ï¼Œä¹Ÿå¯èƒ½ç”¨ç¡¬ä»¶æŒ‡ä»¤å®ç°

æ”¹å†™ä¸ºï¼š

```
float val = x[idx];
y[idx] = val > 0 ? val : 0.f;
```

åŸå› /ä¼˜åŠ¿ï¼š

1. **æ›´æ˜“è¯»**ï¼šåˆå­¦è€…æˆ–è€…æƒ³ debug çš„äººæ›´ç›´è§‚
2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å¯¹ç°ä»£ GPU æ¥è¯´ï¼Œæ¡ä»¶è¿ç®— `val > 0 ? val : 0.f` ä¼šè¢«ç¼–è¯‘å™¨è½¬æˆ **predicated instructionï¼ˆæ¡ä»¶æ©ç æ‰§è¡Œï¼‰**
   - é¿å…å‡½æ•°è°ƒç”¨å¼€é”€ï¼ˆfmaxf ä¹Ÿä¼šç”Ÿæˆç±»ä¼¼çš„ä»£ç ï¼Œä½†æœ‰æ—¶ **device** fmaxf å†…è”ä¸å¦‚ç›´æ¥å†™æ¡ä»¶ï¼‰
   - å¯ä»¥å’Œ `__restrict__` é…åˆåšæ›´æ¿€è¿›ä¼˜åŒ–ï¼ˆå¯„å­˜å™¨ç¼“å­˜ valï¼‰
3. **è°ƒè¯•æ–¹ä¾¿**ï¼š
   - ä¸­é—´å˜é‡ `val` å¯ä»¥æ‰“å°/æ£€æŸ¥
   - æ–¹ä¾¿åŠ æ–­ç‚¹æˆ–è€…åš profiling

3ï¸âƒ£ æ€»ç»“å¯¹æ¯”

| ç‰¹æ€§        | åŸå§‹å†™æ³•                  | ä¼˜åŒ–å†™æ³•                   |
| ----------- | ------------------------- | -------------------------- |
| æŒ‡é’ˆå£°æ˜    | `float* x`                | `float* __restrict__ x`    |
| è®¿é—®æ¨¡å¼    | ç›´æ¥ `fmaxf(x[idx],0.0f)` | å…ˆè¯»åˆ° `val`ï¼Œç„¶åæ¡ä»¶èµ‹å€¼ |
| ç¼–è¯‘å™¨ä¼˜åŒ–  | è¾ƒä¿å®ˆ                    | å¯ä»¥å®‰å…¨é‡æ’åºå’Œå¯„å­˜å™¨ç¼“å­˜ |
| å¯è¯»æ€§/è°ƒè¯• | ç®€æ´                      | æ›´æ˜“è°ƒè¯•å’Œ profiling       |
| é€»è¾‘è¡Œä¸º    | ç›¸åŒ                      | ç›¸åŒ                       |

> âš ï¸ æ³¨æ„ï¼šåŠŸèƒ½å®Œå…¨ä¸€æ ·ï¼Œæ”¹åŠ¨åªæ˜¯ **æ€§èƒ½å’Œå¯è°ƒè¯•æ€§ä¼˜åŒ–**ã€‚

4ï¸âƒ£ ç±»æ¯”åˆ°ä½ è®­ç»ƒæ¨¡å‹çš„ç»éªŒ

ä½ å¯ä»¥æŠŠå®ƒç±»æ¯”ä¸º **PyTorch çš„å¼ é‡è¿ç®— vs è‡ªå·±æ‰‹åŠ¨æ‹†æˆä¸­é—´å˜é‡**ï¼š

- PyTorch: `y = torch.relu(x)`

- ä½ æ‰‹åŠ¨æ‹†å¼€:

  ```
  val = x.clone()
  y = torch.where(val>0, val, torch.tensor(0.0))
  ```

- åŠŸèƒ½ä¸€æ ·ï¼Œä½†æ‰‹åŠ¨æ‹†å¯ä»¥æ›´çµæ´»åœ°æ’å…¥ debug/profiling/å†…å­˜ä¼˜åŒ–ã€‚

# å®Œæ•´å¯ç”¨çš„PyTorch-CUDA-Extensionå·¥ç¨‹

## ä»£ç æ¶æ„

æˆ‘ç»™ä½ æœ€å°å¯è¿è¡Œç»“æ„ï¼š

```shell
myrelu/
â”‚â”€â”€ relu.cpp         # C++ å‰ç«¯ + PyTorch binding
â”‚â”€â”€ relu_kernel.cu   # CUDA kernel
â”‚â”€â”€ setup.py         # ç¼–è¯‘è„šæœ¬
â””â”€â”€ test.py          # æµ‹è¯•
```

æ•´ä¸ªæµç¨‹ **é€»è¾‘ã€æ–‡ä»¶ã€ä»£ç å†™æ³•ã€ç¼–è¯‘ã€æµ‹è¯•** éƒ½æ˜¯æ­£ç¡®çš„

å®Œå…¨ç¬¦åˆ PyTorch å®˜æ–¹æœ€å°ç¤ºä¾‹

åç»­å¯ä»¥åœ¨è¿™ä¸ªåŸºç¡€ä¸Šæ‰©å±•ï¼š

- æ”¯æŒ backward
- æ”¯æŒ float16ã€double
- æ”¯æŒæ›´å¤æ‚ kernelï¼ˆmatmulã€convã€layernorm ç­‰ï¼‰

å¦‚æœä½ æ„¿æ„ï¼Œå¯ä»¥å¸®å†™ **åœ¨è¿™ä¸ª myrelu åŸºç¡€ä¸ŠåŠ  backward çš„å®Œæ•´ç¤ºä¾‹**ï¼Œè¿™æ ·å®ƒå°±å¯ä»¥ç›´æ¥åœ¨ autograd ä¸­ä½¿ç”¨äº†ï¼Œç±»ä¼¼ PyTorch è‡ªå¸¦çš„ ReLUã€‚

## é€æ–‡ä»¶å®Œæ•´ä»£ç 

### relu_kernel.cuï¼ˆCUDAå®ç°ï¼‰

relu_kernel.cu

```c++
#include <cuda_runtime.h>

__global__ void relu_kernel(const float* __restrict__ x,
                            float* __restrict__ y,
                            int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float val = x[idx];
        y[idx] = val > 0 ? val : 0.f;
    }
}

void relu_cuda_launcher(const float* x, float* y, int n) {
    int threads = 256;                     // æ¯ä¸ª block 256 ä¸ª thread
    int blocks = (n + threads - 1) / threads;

    relu_kernel<<<blocks, threads>>>(x, y, n);
}
```

é—®é¢˜ï¼šç¼–è¯‘çš„æ—¶å€™ï¼Œå¤´æ–‡ä»¶`#include <cuda_runtime.h>`ä»å“ªé‡Œæ¥ï¼Ÿ

ä»£ç è§£æï¼š

relu_kernel.cu âœ…

- `__global__ void relu_kernel(...)` å®šä¹‰ GPU kernel
- `relu_cuda_launcher(...)` è´Ÿè´£ **grid/block è®¡ç®—** å¹¶å¯åŠ¨ kernel
- ä½¿ç”¨ `__restrict__` æç¤ºç¼–è¯‘å™¨ä¼˜åŒ–ï¼Œä½¿ç”¨ `float val = x[idx]; y[idx] = val > 0 ? val : 0.f;` æ€§èƒ½æ›´å¥½
- çº¿ç¨‹ç´¢å¼• `idx = blockIdx.x * blockDim.x + threadIdx.x`ï¼Œè¾¹ç•Œåˆ¤æ–­ `if(idx<n)` å®Œæ•´æ­£ç¡®

`relu_kernel<<<blocks, threads>>>(x, y, n);`è¿™è¡Œä»£ç ï¼š

> `<<<blocks, threads>>>` æäº¤çš„æ˜¯ä¸€æ¬¡â€œå¸¦æ‰§è¡Œé…ç½®çš„å¹¶è¡Œè®¡ç®—è¯·æ±‚â€ï¼›CUDA runtime æ ¹æ®è¯¥é…ç½®ï¼Œåœ¨åˆé€‚çš„æ—¶é—´ä¸º kernel åˆ†é…å®é™… GPU èµ„æºå¹¶æ‰§è¡Œã€‚

> **å‘ CUDA runtime æäº¤ä¸€æ¬¡ kernel launch è¯·æ±‚ï¼Œ
>  å…¶ä¸­åŒ…å«ï¼š**
>
> 1. è¦æ‰§è¡Œçš„ kernel å‡½æ•°ï¼ˆ`relu_kernel`ï¼‰
> 2. æœŸæœ›çš„é€»è¾‘å¹¶è¡Œé…ç½®ï¼ˆ`blocks`, `threads`ï¼‰
> 3. kernel å‚æ•°ï¼ˆ`x, y, n`ï¼‰

> **ç”± CUDA runtime å’Œ GPU driver åœ¨ä¹‹åçš„æŸä¸ªæ—¶é—´ç‚¹ï¼Œ
>  æ ¹æ®ç¡¬ä»¶èµ„æºæƒ…å†µï¼Œè°ƒåº¦å¹¶åˆ†é…å®é™…çš„ GPU èµ„æºæ‰§è¡Œè¯¥ kernelã€‚**

### relu.cppï¼ˆC++å‰ç«¯+PyTorchç»‘å®šï¼‰

relu.cpp

```c++
#include <torch/extension.h>

void relu_cuda_launcher(const float* x, float* y, int n);

// PyTorch å‰ç«¯åŒ…è£…
torch::Tensor relu_forward(torch::Tensor x) {
    auto y = torch::zeros_like(x);

    int n = x.numel();

    relu_cuda_launcher(
        x.data_ptr<float>(),
        y.data_ptr<float>(),
        n
    );

    return y;
}

// ç»‘å®šåˆ° Python
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("relu", &relu_forward, "ReLU CUDA kernel");
}
```

é—®é¢˜ï¼š

* ç¼–è¯‘çš„æ—¶å€™ï¼Œå¤´æ–‡ä»¶`#include <cuda_runtime.h>`ä»å“ªé‡Œæ¥ï¼Ÿ
* return y;å¯æ˜¯yæ˜¯åœ¨å‡½æ•°é‡Œä¸´æ—¶åˆ›å»ºçš„å‘€ï¼Œè¿”å›äº†é‚£yå°±æ¶ˆå¤±äº†å‘€
* ç»‘å®šåˆ° Pythonå…·ä½“æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ

ä»£ç è§£æï¼š

relu.cpp âœ…

- åŒ…è£… `torch::Tensor` â†’ raw pointer
- è°ƒç”¨ launcher
- è¿”å›æ–° tensor
- ç”¨ `PYBIND11_MODULE` æš´éœ²åˆ° Python
- è¿™ä¸€å¥—å°±æ˜¯ PyTorch CUDA Extension çš„æ ‡å‡†å†™æ³•

å…³äºï¼š

```c++
// ç»‘å®šåˆ° Python
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("relu", &relu_forward, "ReLU CUDA kernel");
}
```

æ¯ä¸€æ¬¡ import myreluï¼ˆåœ¨ä¸€ä¸ªæ–°çš„ Python è¿›ç¨‹ä¸­ï¼‰ï¼Œéƒ½ä¼šè°ƒç”¨ä¸€æ¬¡ pybind11 çš„æ³¨å†Œä»£ç ã€‚

`pybind11` çš„æ³¨å†Œå‘ç”Ÿåœ¨æ¨¡å—åˆå§‹åŒ–é˜¶æ®µã€‚

```shell
import myrelu
â†“
åŠ è½½ myrelu.so
â†“
è°ƒç”¨ PyInit_myrelu()
â†“
æ‰§è¡Œ PYBIND11_MODULE(...) é‡Œçš„ä»£ç 
â†“
m.def(...)
```

`PYBIND11_MODULE` åˆ°åº•å¹²äº†ä»€ä¹ˆï¼Ÿ

ä½ å¯ä»¥æŠŠå®ƒ**ç­‰ä»·åœ°ç†è§£æˆ**ï¼š

```
extern "C" PyObject* PyInit_myrelu() {
    PyObject* m = PyModule_Create(...);

    // æ³¨å†Œå‡½æ•°
    PyModule_AddFunction(m, "relu", relu_forward);

    return m;
}
```

`m.def(...)` æœ¬è´¨ä¸Šå°±æ˜¯ï¼š

- åˆ›å»ºä¸€ä¸ª Python callable å¯¹è±¡
- ç»‘å®šåˆ° C++ å‡½æ•°æŒ‡é’ˆ
- å¡è¿› module çš„ `__dict__`

### setup.pyï¼ˆç”¨PyTorchçš„cpp_extensionç¼–è¯‘ï¼‰

setup.py

```python
from setuptools import setup
from torch.utils.cpp_extension import CUDAExtension, BuildExtension

setup(
    name='myrelu',
    ext_modules=[
        CUDAExtension(
            name='myrelu',
            sources=['relu.cpp', 'relu_kernel.cu'],
        )
    ],
    cmdclass={'build_ext': BuildExtension}
)
```

ä»£ç è§£æï¼š

ä½¿ç”¨ `CUDAExtension` + `BuildExtension`

åˆ—å‡ºæ‰€æœ‰æºæ–‡ä»¶

`python setup.py build_ext --inplace` å°±å¯ä»¥ç”Ÿæˆ `myrelu.so`

### test.pyï¼ˆæµ‹è¯•ï¼‰

test.py

```c++
import torch
import myrelu

x = torch.randn(10, device="cuda")
y = myrelu.relu(x)

print("x:", x)
print("y:", y)
print("correct:", torch.allclose(y, torch.relu(x)))
```

ä»£ç è§£æï¼š

æµ‹è¯•æµç¨‹å®Œæ•´

`torch.allclose` éªŒè¯è¾“å‡ºæ­£ç¡®æ€§

GPU ä¸Šç›´æ¥è¿è¡Œ

## ç¼–è¯‘

å¯é€‰ä¼˜åŒ–ï¼ˆåŠ å¿«ç¼–è¯‘ï¼‰ï¼šå®‰è£… ninja

```shell
pip install -q ninja
```

ç„¶åç¼–è¯‘ï¼Œåœ¨é¡¹ç›®ç›®å½•ä¸‹è¿è¡Œï¼š

```shell
python setup.py build_ext --inplace
```

æˆåŠŸåï¼Œä¼šå‡ºç°ï¼š

```shell
myrelu.so
```

é—®é¢˜ï¼šå‡ºç°çš„ä¸æ˜¯`myrelu.so`è€Œæ˜¯`myrelu.cpython-312-x86_64-linux-gnu.so`ã€‚



å¦‚æœæ­£ç¡®ï¼Œé‚£ä½ ä¼šçœ‹åˆ°ç¼–è¯‘çš„è¾“å‡ºç»“æœï¼š

```shell
running build_ext
g++ -pthread -B /home/user/miniconda3/envs/torch/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/user/miniconda3/envs/torch/include -fPIC -O2 -isystem /home/user/miniconda3/envs/torch/include -fPIC -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/user/miniconda3/envs/torch/include/python3.12 -c relu.cpp -o build/temp.linux-x86_64-cpython-312/relu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -DTORCH_EXTENSION_NAME=myrelu -std=c++17
/usr/bin/nvcc -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/user/miniconda3/envs/torch/include/python3.12 -c relu_kernel.cu -o build/temp.linux-x86_64-cpython-312/relu_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -DTORCH_EXTENSION_NAME=myrelu -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 -std=c++17
creating build/lib.linux-x86_64-cpython-312
g++ -pthread -B /home/user/miniconda3/envs/torch/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/user/miniconda3/envs/torch/include -fPIC -O2 -isystem /home/user/miniconda3/envs/torch/include -pthread -B /home/user/miniconda3/envs/torch/compiler_compat -shared -Wl,-rpath,/home/user/miniconda3/envs/torch/lib -Wl,-rpath-link,/home/user/miniconda3/envs/torch/lib -L/home/user/miniconda3/envs/torch/lib -Wl,-rpath,/home/user/miniconda3/envs/torch/lib -Wl,-rpath-link,/home/user/miniconda3/envs/torch/lib -L/home/user/miniconda3/envs/torch/lib build/temp.linux-x86_64-cpython-312/relu.o build/temp.linux-x86_64-cpython-312/relu_kernel.o -L/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/lib -L/usr/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-312/myrelu.cpython-312-x86_64-linux-gnu.so
copying build/lib.linux-x86_64-cpython-312/myrelu.cpython-312-x86_64-linux-gnu.so -> 
```

### æŠ¥é”™è§£å†³

#### æœªå®‰è£…nvcc

ä½†æ˜¯å¦‚æœä½ æ²¡æœ‰æå‰å®‰è£…nvccï¼Œå°±ä¼šé‡åˆ°æŠ¥é”™ï¼š

```shell
OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.
```

æ„æ€æ˜¯ **PyTorch æ‰¾ä¸åˆ° CUDA çš„å®‰è£…è·¯å¾„**ï¼Œä¹Ÿå°±æ˜¯ nvcc ç¼–è¯‘å™¨æ‰€åœ¨çš„ä½ç½®ã€‚

è§£å†³åŠæ³•ï¼š

```shell
sudo apt install nvidia-cuda-toolkit
# æ£€æŸ¥ nvcc æ˜¯å¦èƒ½ç”¨ï¼š
nvcc --version
# å¦‚æœèƒ½ç”¨ï¼Œä¼šæ˜¾ç¤ºï¼š
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2023 NVIDIA Corporation
Built on Fri_Jan__6_16:45:21_PST_2023
Cuda compilation tools, release 12.0, V12.0.140
Build cuda_12.0.r12.0/compiler.32267302_0
```

#### ç¯å¢ƒå˜é‡TORCH_CUDA_ARCH_LISTè®¾ç½®é”™è¯¯

å¦‚æœä½ å·²ç»å®‰è£…äº†nvccï¼Œå¹¶ä¸”é…ç½®è¿˜æ˜¯ï¼š

```shell
æ˜¾å¡å‹å·ï¼šNVIDIA GeForce RTX 5090 Laptop GPU
nvidia-smiæ˜¾ç¤ºCUDA Version: 12.9
```

ä½†è¿˜æ˜¯æŠ¥é”™ï¼š

```shell
(torch) lw@LAPTOP-0NOPJT33:~/cuda_myrelu$ python setup.py build_ext --inplace
running build_ext
W1203 11:33:31.808000 185696 site-packages/torch/utils/cpp_extension.py:615] Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.
W1203 11:33:31.845000 185696 site-packages/torch/utils/cpp_extension.py:507] The detected CUDA version (12.0) has a minor version mismatch with the version that was used to compile PyTorch (12.8). Most likely this shouldn't be a problem.
W1203 11:33:31.846000 185696 site-packages/torch/utils/cpp_extension.py:517] There are no g++ version bounds defined for CUDA version 12.0
building 'myrelu' extension
creating build/temp.linux-x86_64-cpython-312
g++ -pthread -B /home/user/miniconda3/envs/torch/compiler_compat -fno-strict-overflow -Wsign-compare -DNDEBUG -O2 -Wall -fPIC -O2 -isystem /home/user/miniconda3/envs/torch/include -fPIC -O2 -isystem /home/user/miniconda3/envs/torch/include -fPIC -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/user/miniconda3/envs/torch/include/python3.12 -c relu.cpp -o build/temp.linux-x86_64-cpython-312/relu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -DTORCH_EXTENSION_NAME=myrelu -std=c++17
W1203 11:33:59.016000 185696 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
W1203 11:33:59.016000 185696 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
/usr/bin/nvcc -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include -I/home/user/miniconda3/envs/torch/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -I/home/user/miniconda3/envs/torch/include/python3.12 -c relu_kernel.cu -o build/temp.linux-x86_64-cpython-312/relu_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1018\" -DTORCH_EXTENSION_NAME=myrelu -gencode=arch=compute_120,code=compute_120 -gencode=arch=compute_120,code=sm_120 -std=c++17
nvcc fatal   : Unsupported gpu architecture 'compute_120'
error: command '/usr/bin/nvcc' failed with exit code 1
```

åŸå› åˆ†æï¼š

æˆ‘çœ‹äº†ä½ çš„é¡¹ç›®æ–‡ä»¶ï¼Œsetup.py å¹¶æ²¡æœ‰æ‰‹åŠ¨åŠ  -gencode å‚æ•°ï¼Œå› æ­¤ç¼–è¯‘è¡Œé‡Œå‡ºç° -gencode=arch=compute_120 åŸºæœ¬å¯ä»¥ç¡®å®šæ˜¯ç¯å¢ƒå˜é‡ TORCH_CUDA_ARCH_LIST è¢«è®¾ç½®æˆäº†é”™è¯¯çš„å€¼ï¼ˆé€šå¸¸æœ‰äººæŠŠâ€œCUDA ç‰ˆæœ¬ 12.0â€è¯¯å½“æˆâ€œç®—åŠ› 12.0â€ï¼Œäºæ˜¯è¢«è§£ææˆ 120 â†’ compute_120ï¼‰ã€‚ä½ çš„ 5090 éœ€è¦çš„æ˜¯ç®—åŠ› sm_90ã€‚

å…³é”®åœ¨äºæŠŠ TORCH_CUDA_ARCH_LIST è®¾ä¸ºç®—åŠ›å€¼ï¼ˆå¦‚ 9.0ï¼‰ï¼Œè€Œä¸æ˜¯ CUDA å·¥å…·é“¾ç‰ˆæœ¬ï¼ˆ12.xï¼‰ã€‚

è§£å†³åŠæ³•ï¼š

> ```shell
> echo "TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST:-<unset>}"
> ```
>
> è‹¥çœ‹åˆ°æ˜¯ 12.0 æˆ–å…¶å®ƒå¼‚å¸¸å€¼ï¼Œå…ˆæ¸…æ‰
>
> ```shell
> unset TORCH_CUDA_ARCH_LIST
> ```
>
> æ˜¾å¼è®¾ç½®ä¸º 5090 å¯¹åº”çš„ç®—åŠ› sm_90ï¼ˆå¯å¸¦ PTX ä»¥å¢å¼ºå‰å‘å…¼å®¹ï¼‰
>
> ```shell
> export TORCH_CUDA_ARCH_LIST="9.0+PTX"
> ```

æ°¸ä¹…ç”Ÿæ•ˆï¼šæŠŠä¸‹é¢ä¸€è¡ŒåŠ å…¥ä½ çš„ shell é…ç½®ï¼ˆå¦‚ ~/.bashrcï¼‰

```shell
export TORCH_CUDA_ARCH_LIST="9.0+PTX"
```

### ç¼–è¯‘å®Œçš„æ–‡ä»¶ç»“æ„åˆ†æ

```shell
(torch) lw@LAPTOP-0NOPJT33:~/cuda_myrelu$ tree
.
â”œâ”€â”€ build
â”‚   â”œâ”€â”€ lib.linux-x86_64-cpython-312
â”‚   â”‚   â””â”€â”€ myrelu.cpython-312-x86_64-linux-gnu.so
â”‚   â””â”€â”€ temp.linux-x86_64-cpython-312
â”‚       â”œâ”€â”€ relu.o
â”‚       â””â”€â”€ relu_kernel.o
â”œâ”€â”€ myrelu.cpython-312-x86_64-linux-gnu.so
â”œâ”€â”€ relu.cpp
â”œâ”€â”€ relu_kernel.cu
â”œâ”€â”€ setup.py
â””â”€â”€ test.py
```

**æ–‡ä»¶ç»“æ„åˆ†æ**

1. **`build/` ç›®å½•** - ç¼–è¯‘è¿‡ç¨‹ä¸­é—´æ–‡ä»¶ç›®å½•

```
build/
â”œâ”€â”€ lib.linux-x86_64-cpython-312/
â”‚   â””â”€â”€ myrelu.cpython-312-x86_64-linux-gnu.so  # æœ€ç»ˆç¼–è¯‘å¥½çš„åº“æ–‡ä»¶ï¼ˆå¤‡ç”¨ä½ç½®ï¼‰
â””â”€â”€ temp.linux-x86_64-cpython-312/
    â”œâ”€â”€ relu.o            # C++ ç›®æ ‡æ–‡ä»¶ï¼ˆç¼–è¯‘åçš„æœºå™¨ç ï¼‰
    â””â”€â”€ relu_kernel.o     # CUDA ç›®æ ‡æ–‡ä»¶ï¼ˆç¼–è¯‘åçš„GPUæœºå™¨ç ï¼‰
```

2. **å…³é”®æ–‡ä»¶è¯´æ˜**

**`myrelu.cpython-312-x86_64-linux-gnu.so`** (æ ¹ç›®å½•å’Œbuildç›®å½•éƒ½æœ‰)

- **ä½œç”¨**ï¼šè¿™æ˜¯ç¼–è¯‘æˆåŠŸåçš„**Pythonæ‰©å±•æ¨¡å—**
- **æ–‡ä»¶åè§£æ**ï¼š
  - `myrelu` - æ¨¡å—åç§°ï¼ˆåœ¨setup.pyä¸­å®šä¹‰çš„ï¼‰
  - `cpython-312` - CPython 3.12ç‰ˆæœ¬
  - `x86_64` - 64ä½æ¶æ„
  - `linux-gnu` - Linuxç³»ç»Ÿ
  - `.so` - Shared Objectæ–‡ä»¶ï¼ˆLinuxçš„åŠ¨æ€é“¾æ¥åº“ï¼‰
- **ä½ç½®**ï¼šæ ¹ç›®å½•çš„æ˜¯é€šè¿‡ `--inplace` å‚æ•°å¤åˆ¶è¿‡æ¥çš„ï¼Œå¯ä»¥ç›´æ¥åœ¨é¡¹ç›®ç›®å½•ä¸­ä½¿ç”¨

**`relu.o`**

- **ä½œç”¨**ï¼šC++æºæ–‡ä»¶ `relu.cpp` ç¼–è¯‘åçš„**ç›®æ ‡æ–‡ä»¶**
- **å†…å®¹**ï¼šåŒ…å«CPUéƒ¨åˆ†çš„æœºå™¨ä»£ç 
- **å¦‚ä½•ç”Ÿæˆ**ï¼š`g++` ç¼–è¯‘å™¨å°† `.cpp` æ–‡ä»¶ç¼–è¯‘ä¸º `.o` æ–‡ä»¶

**`relu_kernel.o`**

- **ä½œç”¨**ï¼šCUDAæºæ–‡ä»¶ `relu_kernel.cu` ç¼–è¯‘åçš„**ç›®æ ‡æ–‡ä»¶**
- **å†…å®¹**ï¼šåŒ…å«GPU kernelçš„PTXä»£ç å’Œæœºå™¨ç 
- **å¦‚ä½•ç”Ÿæˆ**ï¼š`nvcc` ç¼–è¯‘å™¨å°† `.cu` æ–‡ä»¶ç¼–è¯‘ä¸º `.o` æ–‡ä»¶

3. **ç¼–è¯‘æµç¨‹å›é¡¾**

```
relu.cpp (C++) + relu_kernel.cu (CUDA)
        â†“ ç¼–è¯‘
relu.o + relu_kernel.o (ç›®æ ‡æ–‡ä»¶)
        â†“ é“¾æ¥
myrelu.cpython-312-x86_64-linux-gnu.so (æœ€ç»ˆPythonæ¨¡å—)
```

### ç¼–è¯‘åŸç†

#### relu_kernel.cuæ€ä¹ˆç¼–è¯‘

é—®é¢˜ï¼š

> relu_kernel.cuæ€ä¹ˆç¼–è¯‘å•Šï¼Œæ²¡è§è¿™ä¸ªæ•™ç¨‹å«æˆ‘æ€ä¹ˆç¼–è¯‘cu

å›ç­”ï¼š

ä½ ä¸éœ€è¦å•ç‹¬å»ç¼–è¯‘ `relu_kernel.cu`ï¼Œè¿™åœ¨ PyTorch CUDA Extension æµç¨‹é‡Œæ˜¯ **ç”± `setup.py` ç»Ÿä¸€ç®¡ç†çš„**ã€‚æˆ‘æ¥è¯¦ç»†è§£é‡ŠåŸç†å’Œæµç¨‹ï¼š

1ï¸âƒ£ ä¸ºä»€ä¹ˆä¸éœ€è¦å•ç‹¬ `nvcc relu_kernel.cu ...`

åœ¨ä¼ ç»Ÿ CUDA å¼€å‘é‡Œï¼Œä½ å¯èƒ½ä¼šè¿™æ ·åšï¼š

```
nvcc -c relu_kernel.cu -o relu_kernel.o
```

ç„¶åå†å’Œå…¶ä»– C++ æ–‡ä»¶é“¾æ¥ç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶ã€‚

ä½†æ˜¯ **PyTorch Extension** å°è£…äº†è¿™ä¸ªæµç¨‹ï¼š

- `setup.py` ä½¿ç”¨äº† `torch.utils.cpp_extension.CUDAExtension`
- è¿™ä¸ªç±»å†…éƒ¨ä¼šè‡ªåŠ¨è°ƒç”¨ **nvcc** æ¥ç¼–è¯‘æ‰€æœ‰ `.cu` æ–‡ä»¶
- ç¼–è¯‘ç»“æœä¼šå’Œ `.cpp` æ–‡ä»¶ä¸€èµ·é“¾æ¥ç”Ÿæˆ Python å¯ä»¥ç›´æ¥ import çš„ `.so` æ–‡ä»¶

ä¹Ÿå°±æ˜¯è¯´ï¼š

```
sources=['relu.cpp', 'relu_kernel.cu']
```

PyTorch ç¼–è¯‘å™¨ä¼šè‡ªåŠ¨ï¼š

1. ç”¨ nvcc ç¼–è¯‘ `relu_kernel.cu` â†’ ç”Ÿæˆ PTX/å¯¹è±¡æ–‡ä»¶
2. ç”¨ C++ ç¼–è¯‘å™¨ç¼–è¯‘ `relu.cpp` â†’ ç”Ÿæˆå¯¹è±¡æ–‡ä»¶
3. é“¾æ¥æˆä¸€ä¸ªå…±äº«åº“ `.so`

ä½  **ä¸ç”¨æ‰‹åŠ¨ç¼–è¯‘ cu æ–‡ä»¶**ã€‚

2ï¸âƒ£ å®é™…æ“ä½œæµç¨‹

```
# åœ¨ myrelu/ ç›®å½•ä¸‹
python setup.py build_ext --inplace
```

- `build_ext` å‘Šè¯‰ setuptools ç¼–è¯‘æ‰©å±•
- `--inplace` ç”Ÿæˆ `.so` æ”¾åœ¨å½“å‰ç›®å½•
- å†…éƒ¨æµç¨‹ï¼š
  1. è°ƒç”¨ nvcc ç¼–è¯‘ `relu_kernel.cu`
  2. è°ƒç”¨ g++/clang ç¼–è¯‘ `relu.cpp`
  3. é“¾æ¥ç”Ÿæˆ `myrelu.so`

ä¹‹åå°±å¯ä»¥ç›´æ¥åœ¨ Python é‡Œï¼š

```
import myrelu
y = myrelu.relu(x)
```

3ï¸âƒ£ ğŸ”‘ æ³¨æ„äº‹é¡¹

1. **CUDA ç¯å¢ƒ**ï¼š
   - `nvcc` å¯ç”¨
   - PyTorch CUDA ç‰ˆæœ¬åŒ¹é…
2. **è·¯å¾„**ï¼š
   - sources ç”¨ç›¸å¯¹è·¯å¾„å³å¯
   - `setup.py`ã€`relu.cpp`ã€`relu_kernel.cu` åœ¨åŒä¸€ç›®å½•æœ€æ–¹ä¾¿
3. **æ‰©å±•åŠŸèƒ½**ï¼š
   - å¦‚æœåç»­æƒ³åš backwardï¼Œä»ç„¶å¯ä»¥åœ¨ `relu.cpp` è°ƒç”¨ kernel
   - åªè¦ä¿è¯ kernel é€»è¾‘æ­£ç¡®ï¼Œæ— éœ€å•ç‹¬ nvcc ç¼–è¯‘

------

âœ… **æ€»ç»“**ï¼š

- `relu_kernel.cu` åªè¦æ”¾åœ¨ `sources` é‡Œå°±è¡Œ
- ç¼–è¯‘å®Œå…¨ç”± `setup.py` + `CUDAExtension` ç®¡ç†
- ä¸éœ€è¦ä½ æ‰‹åŠ¨è°ƒç”¨ nvcc

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª ASCII æµç¨‹å›¾ï¼Œè®©ä½ ç›´è§‚ç†è§£ï¼š

```
setup.py -> CUDAExtension -> nvcc & g++ -> myrelu.so
```

è¿™æ ·ä½ å°±å½»åº•æ˜ç™½ `.cu` æ–‡ä»¶æ˜¯æ€ä¹ˆè¢« PyTorch ç¼–è¯‘çš„ã€‚

#### python-setup.pyç±»ä¼¼cmake

> å™¢ï¼ŒåŸæ¥python setup.py build_ext --inplaceè¿™å¥è¯å°±æ˜¯ç±»ä¼¼cmakeï¼Œæ¥ç¼–è¯‘çš„

å¯¹ï¼Œä½ ç†è§£å¾—å®Œå…¨æ­£ç¡®ï¼ğŸ¯

å¯ä»¥è¿™æ ·ç±»æ¯”ï¼š

1ï¸âƒ£ `setup.py build_ext --inplace` â‰ˆ â€œå°å‹ CMakeâ€

- `setup.py` æ˜¯ **æ„å»ºè„šæœ¬**
- `build_ext` å‘Šè¯‰ setuptools ç¼–è¯‘ extension
- `--inplace` è¡¨ç¤ºç”Ÿæˆçš„ `.so` æ”¾åœ¨å½“å‰ç›®å½•ï¼ˆPython å¯ä»¥ç›´æ¥ importï¼‰

åŠŸèƒ½ç±»ä¼¼ CMake + makeï¼Œä½†æ›´è½»é‡ã€Python åŒ–ã€‚

2ï¸âƒ£ å†…éƒ¨åšäº†ä»€ä¹ˆ

å½“ä½ è¿è¡Œï¼š

```
python setup.py build_ext --inplace
```

å‘ç”Ÿçš„äº‹æƒ…ï¼š

1. **è§£æ `CUDAExtension(sources=[...])`**
   - æ‰¾åˆ°æ‰€æœ‰ `.cpp`ã€`.cu` æ–‡ä»¶
2. **è°ƒç”¨ç¼–è¯‘å™¨**
   - `.cu` æ–‡ä»¶ â†’ **nvcc** ç¼–è¯‘
   - `.cpp` æ–‡ä»¶ â†’ **g++/clang** ç¼–è¯‘
3. **é“¾æ¥æˆå…±äº«åº“**
   - ç”Ÿæˆ `myrelu.so`
   - Python å¯ä»¥ç›´æ¥ `import myrelu`

> è¿™ä¸ªæµç¨‹å°±åƒ CMake å†™ C++ é¡¹ç›®ä¸€æ ·ï¼šæŒ‡å®šæºç ã€ç¼–è¯‘å™¨ã€ç”Ÿæˆåº“ï¼ŒPython ç‰ˆæ›´ç®€å•ã€‚

3ï¸âƒ£ åŒºåˆ«ä¸ CMake

| é¡¹ç›®          | setup.py / setuptools       | CMake / make                |
| ------------- | --------------------------- | --------------------------- |
| é…ç½®æ–¹å¼      | Python è„šæœ¬ï¼ˆPython è¯­æ³•ï¼‰  | CMakeLists.txt              |
| ç¼–è¯‘å‘½ä»¤      | python setup.py build_ext   | make / cmake --build        |
| è‡ªåŠ¨ç®¡ç† CUDA | å†…ç½® CUDAExtension          | éœ€è¦æ‰‹åŠ¨é…ç½® nvcc / include |
| è¾“å‡º          | .soï¼ˆPython å¯ç›´æ¥ importï¼‰ | å¯æ‰§è¡Œæ–‡ä»¶ / åº“             |

âœ… æ€»ç»“

- `setup.py build_ext --inplace` = **Python ç‰ˆâ€œCMake+makeâ€**
- å®ƒå¸®ä½ æŠŠ `.cu` + `.cpp` è‡ªåŠ¨ç¼–è¯‘ã€é“¾æ¥æˆ `.so`
- ä½ åªè¦å†™å¥½ kernel å’Œ bindingï¼Œè°ƒç”¨ä¸€æ¬¡å‘½ä»¤å°±èƒ½ç¼–è¯‘å®Œæˆ

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€ä¸ª **ASCII æµç¨‹å›¾**ï¼Œå±•ç¤º `setup.py -> nvcc/g++ -> myrelu.so -> Python import` çš„å®Œæ•´æµç¨‹ï¼Œèƒ½è®©ä½ æŠŠæ•´ä¸ª CUDA Extension ç¼–è¯‘é“¾æ¡è®°ä½ã€‚

#### PyTorch-CUDA-Extensionçš„ç¼–è¯‘æµç¨‹

é—®é¢˜ï¼š

> PyTorch ç¼–è¯‘å™¨ä¼šè‡ªåŠ¨ï¼š ç”¨ nvcc ç¼–è¯‘ relu_kernel.cu â†’ ç”Ÿæˆ PTX/å¯¹è±¡æ–‡ä»¶ ç”¨ C++ ç¼–è¯‘å™¨ç¼–è¯‘ relu.cpp â†’ ç”Ÿæˆå¯¹è±¡æ–‡ä»¶ é“¾æ¥æˆä¸€ä¸ªå…±äº«åº“ .so æˆ‘æƒ³ææ¸…æ¥šè¿™ä¸‰æ­¥çš„ä¾èµ–å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ

å›å¤ï¼š

æ˜ç™½ï¼Œä½ æƒ³æŠŠ **PyTorch CUDA Extension çš„ç¼–è¯‘æµç¨‹**æ‹†è§£å¾—åƒ CPU/GPU ç¼–è¯‘æµæ°´çº¿ä¸€æ ·ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¾èµ–å…³ç³»å›¾æ¥åˆ†æä¸‰æ­¥ã€‚

1ï¸âƒ£ ä¸‰æ­¥æ¦‚è§ˆ

å‡è®¾ä½ æœ‰ï¼š

```
sources = ['relu.cpp', 'relu_kernel.cu']
```

PyTorch ç¼–è¯‘å™¨ï¼ˆ`CUDAExtension` + setuptoolsï¼‰å†…éƒ¨åšä¸‰ä»¶äº‹ï¼š

1. **nvcc ç¼–è¯‘ CUDA kernel**

```
relu_kernel.cu â†’ relu_kernel.o
```

1. **C++ ç¼–è¯‘å™¨ç¼–è¯‘ C++ å‰ç«¯**

```
relu.cpp â†’ relu.o
```

1. **é“¾æ¥æ‰€æœ‰å¯¹è±¡æ–‡ä»¶æˆå…±äº«åº“**

```
relu_kernel.o + relu.o â†’ myrelu.so
```

2ï¸âƒ£ ä¾èµ–å…³ç³»åˆ†æ

```
   relu_kernel.cu           relu.cpp
          |                    |
      nvcc ç¼–è¯‘             g++ ç¼–è¯‘
          |                    |
    relu_kernel.o           relu.o
          \                    /
           \                  /
            \                /
             é“¾æ¥ï¼ˆlinkï¼‰ â†’ myrelu.so
```

**è§£é‡Šï¼š**

1. `relu_kernel.cu` ç¼–è¯‘ç”Ÿæˆ `relu_kernel.o`
   - `relu_kernel.o` å†…å« GPU ä»£ç ï¼ˆPTXï¼‰
   - ä¾›é“¾æ¥æ—¶è°ƒç”¨
2. `relu.cpp` ç¼–è¯‘ç”Ÿæˆ `relu.o`
   - å†…å« launcherã€PyTorch bindingã€C++ host é€»è¾‘
3. **é“¾æ¥é˜¶æ®µ**
   - æŠŠ `relu_kernel.o` + `relu.o` ç»„åˆæˆä¸€ä¸ª `.so`
   - ç”Ÿæˆ Python å¯ import çš„å…±äº«åº“

> ğŸ”‘ æ³¨æ„ï¼š
>
> - `relu.cpp` å¯ä»¥è°ƒç”¨ `relu_cuda_launcher`ï¼ˆåœ¨ `relu_kernel.cu` ä¸­å®ç°ï¼‰
> - è¿™å°±æ˜¯ä¸ºä»€ä¹ˆé“¾æ¥é˜¶æ®µå¿…é¡»æŠŠä¸¤è€…ä¸€èµ·é“¾æ¥ï¼Œå¦åˆ™æ‰¾ä¸åˆ°ç¬¦å·

3ï¸âƒ£ è¡¥å……è¯´æ˜

- **PTX vs å¯¹è±¡æ–‡ä»¶**
  - nvcc ç¼–è¯‘ `.cu` æ–‡ä»¶æ—¶ï¼Œä¼šå…ˆç”Ÿæˆ PTXï¼ˆGPUæ±‡ç¼–ï¼‰ï¼Œå†ç”Ÿæˆ `.o`
  - é“¾æ¥æ—¶ `.o` å†…éƒ¨åŒ…å« PTXï¼Œæœ€ç»ˆæ‰“åŒ…åˆ° `.so`
- **ç¼–è¯‘é¡ºåº**
  - å®é™…ä¸Šä¸¤è€…å¯ä»¥å¹¶è¡Œç¼–è¯‘ï¼ˆnvcc/g++åŒæ—¶ï¼‰
  - ä½†é“¾æ¥å¿…é¡»ç­‰ä¸¤ä¸ª `.o` éƒ½ç”Ÿæˆ
- **ä¸ºä»€ä¹ˆä¸ç”¨å•ç‹¬æ‰‹åŠ¨ nvcc**
  - `setup.py` å†…éƒ¨å·²ç»ç®¡ç†äº†ä¾èµ–
  - setuptools ä¼šè‡ªåŠ¨ä¿è¯å…ˆç¼–è¯‘ `.cu` / `.cpp`ï¼Œå†é“¾æ¥

4ï¸âƒ£ ç±»æ¯”

å¯ä»¥ç±»æ¯”æˆ **CPU C++ é¡¹ç›®**ï¼š

```
foo.cpp â†’ foo.o
bar.cpp â†’ bar.o
bar.o è°ƒç”¨ foo.o ä¸­å‡½æ•°
é“¾æ¥ï¼šfoo.o + bar.o â†’ libbar.so
```

PyTorch CUDA Extension å°±æ˜¯ **C++ + CUDA çš„æ··åˆé¡¹ç›®**ï¼Œä¾èµ–å…³ç³»å®Œå…¨ç›¸åŒï¼Œåªæ˜¯ `.cu` ä¼šç»è¿‡ nvcc ç”Ÿæˆ GPU ä»£ç ã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€ä¸ª **æ–‡å­—ç‰ˆçš„ CUDAExtension ç¼–è¯‘æµç¨‹å›¾**ï¼Œå±•ç¤ºä» `.cu` / `.cpp` åˆ° `.so` çš„å®Œæ•´ä¾èµ–é“¾ã€‚

#### relu_kernel.cuæœ¬è´¨æ˜¯c++ä»£ç 

é—®é¢˜ï¼š

> relu_kernel.cu ç¼–è¯‘ç”Ÿæˆ relu_kernel.o è¿™ä¸ªæ˜¯gpuä¸Šè¿è¡Œçš„ä»£ç ï¼Œé‚£å®ƒå®é™…å’Œrelu.cpp â†’ relu.oä¸€æ ·å—ï¼Ÿè¿™é‡Œæˆ‘å¾ˆç–‘æƒ‘ï¼Œè¿˜æ˜¯è¯´ï¼Œå…¶å®relu_kernel.cuå¯ä»¥çœ‹åšæ˜¯c++ä»£ç ï¼Ÿ

å›å¤ï¼š

ä½ çš„ç–‘æƒ‘å¾ˆæ­£å¸¸ï¼Œå¾ˆå¤šäººç¬¬ä¸€æ¬¡æ¥è§¦ CUDA Extension éƒ½ä¼šæœ‰åŒæ ·é—®é¢˜ã€‚æˆ‘å¸®ä½ å½»åº•ç†æ¸…æ¥šã€‚

1ï¸âƒ£ relu_kernel.cu æœ¬è´¨ä¸Šæ˜¯ C++ + CUDA

- `.cu` æ–‡ä»¶æœ¬è´¨ä¸Šå°±æ˜¯ **C++ æ–‡ä»¶ + CUDA æ‰©å±•**
- ä½ å¯ä»¥å†™æ ‡å‡† C++ ä»£ç 
- åŒæ—¶å¯ä»¥å†™ GPU kernelï¼ˆ`__global__`ã€`__device__`ï¼‰
- nvcc å¯ä»¥å¤„ç†ï¼š
  - **Host ä»£ç ** â†’ ç¼–è¯‘æˆ CPU å¯¹è±¡æ–‡ä»¶
  - **Device ä»£ç ï¼ˆkernelï¼‰** â†’ ç¼–è¯‘æˆ GPU æœºå™¨ç  / PTX

> æ‰€ä»¥ä¸¥æ ¼æ¥è¯´ï¼Œ`.cu` é‡Œé¢æ—¢æœ‰â€œCPU ä»£ç â€ï¼Œä¹Ÿæœ‰â€œGPU ä»£ç â€ã€‚
>  ä¾‹å¦‚ï¼š
>
> ```c++
> __global__ void relu_kernel(...) { ... }  // GPU kernel
> void relu_cuda_launcher(...) {           // CPU host ä»£ç 
>     relu_kernel<<<blocks, threads>>>(...);
> }
> ```

- `relu_cuda_launcher` æ˜¯ CPU ç«¯å‡½æ•°ï¼Œå®ƒåœ¨ host ä¸Šè¿è¡Œ
- `relu_kernel` æ˜¯ GPU ç«¯å‡½æ•°ï¼Œå®ƒåœ¨ device ä¸Šè¿è¡Œ

2ï¸âƒ£ relu.cpp vs relu_kernel.cu å¯¹æ¯”

| æ–‡ä»¶           | ç¼–è¯‘å™¨    | ç”Ÿæˆå¯¹è±¡      | è¿è¡Œä½ç½®    | å†…å®¹ç±»å‹                   |
| -------------- | --------- | ------------- | ----------- | -------------------------- |
| relu.cpp       | g++/clang | relu.o        | CPUï¼ˆhostï¼‰ | C++ host + PyTorch binding |
| relu_kernel.cu | nvcc      | relu_kernel.o | CPU + GPU   | host å‡½æ•° + GPU kernel     |

- **relu_kernel.o** é‡ŒåŒ…å«ï¼š
  - GPU kernel çš„ PTX / SASS æœºå™¨ç 
  - host launcher å‡½æ•°ï¼ˆCPU ä¸Šæ‰§è¡Œï¼Œè°ƒç”¨ kernelï¼‰
- **é“¾æ¥æ—¶**ï¼š
  - relu.o + relu_kernel.o â†’ myrelu.so
  - Python è°ƒç”¨ myrelu.relu() â†’ è°ƒç”¨ relu_forward â†’ è°ƒç”¨ relu_cuda_launcher â†’ GPU kernel æ‰§è¡Œ

3ï¸âƒ£ å¯ä»¥æŠŠ relu_kernel.cu æƒ³è±¡æˆ

```
.------------.
|  CPU host  |   <- relu_cuda_launcher()
|------------|
|  GPU kernel|   <- __global__ void relu_kernel()
'------------'
```

- **Host ä»£ç **ï¼šç¼–è¯‘æˆ CPU å¯¹è±¡æ–‡ä»¶
- **Kernel ä»£ç **ï¼šç¼–è¯‘æˆ GPU æœºå™¨ç 
- æœ€ç»ˆ `.o` æ–‡ä»¶é‡Œä¸¤éƒ¨åˆ†éƒ½å­˜åœ¨
- é“¾æ¥æ—¶ relu.o + relu_kernel.o â†’ å…±äº«åº“ `.so`

4ï¸âƒ£ ğŸ”‘ æ ¸å¿ƒç†è§£

1. `.cu` æ–‡ä»¶ = C++ æ–‡ä»¶ + GPU æ‰©å±•
2. nvcc å¯ä»¥åŒæ—¶å¤„ç† host å’Œ device ä»£ç 
3. relu_kernel.o â‰  å®Œå…¨ GPU æ–‡ä»¶ï¼Œå®ƒä¹Ÿæœ‰ CPU éƒ¨åˆ†ï¼ˆlauncherï¼‰
4. é“¾æ¥æ—¶ï¼ŒCPU è°ƒç”¨ GPU kernel å°±åƒè°ƒç”¨æ™®é€šå‡½æ•°ä¸€æ ·

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª **ASCII æµç¨‹å›¾**ï¼Œæ˜¾ç¤ºï¼š

```
Python -> relu_forward -> relu_cuda_launcher -> relu_kernel<<<>>> -> GPU
```

åŒæ—¶æ ‡æ³¨ `.cu` é‡Œçš„ host/device åŒºåˆ«ï¼Œè®©ä½ å½»åº•æ˜ç™½ `.cu` å’Œ `.cpp` çš„å·®åˆ«ã€‚

#### relu_kernelå¯çœ‹ä½œæ˜¯cudaå¯¹c++çš„æ‹“å±•ï¼Œç±»ä¼¼qtå¯¹c++çš„æ‹“å±•

é—®é¢˜ï¼š

> å°±æ˜¯ï¼Œ__global__ void relu_kernel()æœ¬è´¨ä¸Šè¿˜æ˜¯c++ä»£ç ï¼Œåªä¸è¿‡ä¸€äº›ç±»å‹æ˜¯cudaçš„æ‹“å±•ï¼Ÿå°±ç±»ä¼¼qtå¯¹c++çš„æ‹“å±•ä¸€æ ·ï¼Ÿ

å›å¤ï¼š

å®Œå…¨å¯ä»¥è¿™æ ·ç†è§£ï¼âœ…

æˆ‘ä»¬æ¥ä»”ç»†å¯¹æ¯”ä¸€ä¸‹ï¼š

1ï¸âƒ£ æœ¬è´¨ä¸Š `.cu` æ–‡ä»¶å°±æ˜¯ C++

- `.cu` æ–‡ä»¶æœ¬è´¨ä¸Šå°±æ˜¯ **C++ æºç **ï¼Œä½ å¯ä»¥å†™ï¼š
  - æ ‡å‡† C++ ä»£ç ï¼ˆå‡½æ•°ã€ç±»ã€æ¨¡æ¿ç­‰ï¼‰
  - è°ƒç”¨ C++ åº“
- CUDA åœ¨ C++ ä¸Šåšäº†æ‰©å±•ï¼š
  - `__global__` / `__device__` / `__host__` ç­‰å‡½æ•°ä¿®é¥°ç¬¦
  - GPU å†…å­˜ç±»å‹ï¼ˆ`__shared__` / `__constant__` ç­‰ï¼‰
  - GPU å†…å»ºå‡½æ•°ï¼ˆ`threadIdx.x`ã€`blockIdx.x`ã€`atomicAdd`ã€`fmaxf` ç­‰ï¼‰

> å°±åƒ Qt å¯¹ C++ çš„æ‰©å±•ä¸€æ ·ï¼šQt ç»™ C++ å¢åŠ äº† `signals` / `slots` / `Q_OBJECT` ç­‰ç‰¹æ€§ï¼Œä½†æ ¸å¿ƒä»ç„¶æ˜¯ C++ã€‚

2ï¸âƒ£ `__global__ void relu_kernel(...)` çš„æ„ä¹‰

- è¯­ä¹‰ï¼š
  - å‘Šè¯‰ nvcc è¿™æ˜¯ **GPU kernel**ï¼Œç”± CPU è°ƒç”¨å¹¶åœ¨ GPU ä¸Šæ‰§è¡Œ
- è¯­æ³•ä»ç„¶æ˜¯ C++ é£æ ¼ï¼š
  - å‡½æ•°å‚æ•°ã€å˜é‡ç±»å‹ã€è¡¨è¾¾å¼éƒ½æ˜¯ C++ è¯­æ³•
- åªæ˜¯ nvcc ä¼šï¼š
  1. ç”Ÿæˆ GPU æœºå™¨ç ï¼ˆPTX/SASSï¼‰
  2. åœ¨ host ç«¯ç”Ÿæˆè°ƒç”¨ kernel çš„æ¥å£ï¼ˆlauncherï¼‰

3ï¸âƒ£ ä¸¾ä¾‹å¯¹æ¯”

```
// æ ‡å‡† C++ å‡½æ•°
int add(int a, int b) { return a + b; }

// CUDA kernel
__global__ void add_kernel(int* a, int* b, int* c) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;
    c[idx] = a[idx] + b[idx];
}
```

- å†…æ ¸å‡½æ•°è¿˜æ˜¯ç”¨ C++ è¯­æ³•
- åªæ˜¯ nvcc è¯†åˆ« `__global__`ï¼Œç”Ÿæˆ GPU æŒ‡ä»¤
- è°ƒç”¨æ–¹å¼ä¸åŒï¼š`add(1,2)` vs `add_kernel<<<blocks, threads>>>(a,b,c)`

4ï¸âƒ£ æ€»ç»“ç†è§£

- `.cu` æ–‡ä»¶ = **C++ æ–‡ä»¶ + CUDA æ‹“å±•**
- `__global__` / `__device__` / `__shared__` åªæ˜¯ CUDA å¯¹ C++ çš„è¯­æ³•æ‰©å±•
- CUDA kernel **é€»è¾‘å†™æ³•å’Œ C++ éå¸¸æ¥è¿‘**ï¼Œåªæ˜¯åœ¨ CPU/GPU ä¹‹é—´æ·»åŠ äº†è°ƒåº¦å’Œå†…å­˜çº¦æŸ
- ç±»ä¼¼ Qtã€MFCã€OpenMP ç»™ C++ å¢åŠ ç‰¹æ€§ï¼Œä½†æ ¸å¿ƒè¯­æ³•æ˜¯ C++

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª **æ–‡å­—ç‰ˆæµç¨‹å›¾**ï¼Œæ˜¾ç¤ºï¼š

```
Python -> relu_forward -> relu_cuda_launcher -> relu_kernel<<<blocks, threads>>> -> GPU
```

åŒæ—¶æ ‡æ³¨ **.cu æ–‡ä»¶é‡Œçš„ host/device ä»£ç å¯¹åº”ä½ç½®**ï¼Œè®©ä½ å®Œå…¨ææ‡‚ `.cu` å’Œ `.cpp` çš„åŒºåˆ«ã€‚

#### nvccç¼–è¯‘å™¨æ›¿ä»£g++ç¼–è¯‘cuæ–‡ä»¶

é—®é¢˜ï¼š

> é‚£æˆ‘æ˜ç™½äº†ï¼Œå°±æ˜¯__global__ void relu_kernel()è¿™ä¸ª.cuä»£ç ï¼Œæœ¬è´¨å°±æ˜¯c++ä»£ç ï¼Œåªä¸è¿‡æœ‰cudaçš„è‡ªå®šä¹‰ç±»å‹æ‹“å±•è€Œå·²ï¼Œå°±ç±»ä¼¼æˆ‘è‡ªå·±å†™äº†ä¸ªå¤´æ–‡ä»¶çš„ç±»å‹æ‹“å±•åŠ è¿›å»äº†ï¼Œè¿™æ˜¯å…¶ä¸€ã€‚ ä½†æ˜¯åœ¨ç¼–è¯‘çš„æ—¶å€™ï¼Œæˆ‘è‡ªå·±å†™çš„è¿™äº›å¤´æ–‡ä»¶ç±»å‹æ‹“å±•ï¼Œå¯èƒ½ä¼ ç»Ÿçš„g++ç¼–è¯‘å™¨ä¸è®¤ï¼Œæˆ–è€…ç¼–è¯‘å®Œæ— æ³•å®ç°æˆ‘æƒ³è¦çš„åŠŸèƒ½ï¼Œå°±è‡ªå·±å¼€å‘äº†nvccç¼–è¯‘å™¨ä»£æ›¿g++ç¼–è¯‘å™¨ï¼Œè€Œä¸”æˆ‘çŒœæµ‹nvccç¼–è¯‘å™¨å°±æ˜¯åŒ…å«äº†g++ç¼–è¯‘å™¨ï¼Œå†åŠ ä¸Šäº†è‡ªå·±çš„ä¸€äº›åŠŸèƒ½ã€‚

å›ç­”ï¼š

å®Œå…¨å¯¹ï¼Œä½ çš„ç†è§£å¾ˆåˆ°ä½ï¼ŒåŸºæœ¬å°±æ˜¯è¿™æ ·ã€‚æˆ‘ä»¬å¯ä»¥æŠŠå®ƒå†æ¢³ç†å¾—æ›´æ¸…æ¥šä¸€ç‚¹ï¼Œè®©é€»è¾‘æ›´ä¸¥å¯†ï¼š

1ï¸âƒ£ .cu æ–‡ä»¶æœ¬è´¨

- `.cu` æ–‡ä»¶æœ¬è´¨å°±æ˜¯ C++ æ–‡ä»¶
- CUDA çš„æ‰©å±•å°±åƒä½ è‡ªå·±å†™çš„å¤´æ–‡ä»¶æˆ–åº“ç»™ C++ å¢åŠ äº†æ–°ç±»å‹å’Œå…³é”®å­—ï¼š
  - `__global__` / `__device__` / `__host__`
  - GPU å†…å»ºå˜é‡ï¼š`threadIdx`ã€`blockIdx`ã€`blockDim`
  - GPU å†…å»ºå‡½æ•°ï¼š`fmaxf`ã€`atomicAdd` ç­‰
- æ ¸å¿ƒè¯­æ³•ä»ç„¶æ˜¯æ ‡å‡† C++ï¼Œåªæ˜¯ nvcc èƒ½è¯†åˆ«è¿™äº›æ‹“å±•ï¼Œå¹¶ç”Ÿæˆ GPU ä»£ç 

> å°±åƒ Qtã€OpenMPã€Eigen çš„å®å’Œç±»å‹æ‰©å±•ä¸€æ ·ï¼Œæœ¬è´¨æ˜¯ C++ï¼Œåªä¸è¿‡éœ€è¦ç‰¹æ®Šç¼–è¯‘å™¨æ”¯æŒã€‚

2ï¸âƒ£ ä¸ºä»€ä¹ˆä¸èƒ½ç”¨ g++ å•ç‹¬ç¼–è¯‘ .cu

- g++ æ˜¯æ ‡å‡† C++ ç¼–è¯‘å™¨ï¼Œå®ƒ**ä¸è®¤è¯†** CUDA æ‹“å±•ï¼š
  - `__global__` / `__device__` ä¼šæŠ¥è¯­æ³•é”™è¯¯
  - GPU å†…å»ºå˜é‡/å‡½æ•° g++ æ— æ³•è§£æ
- å¦‚æœç”¨ g++ ç¼–è¯‘ï¼Œå³ä½¿æˆåŠŸç”Ÿæˆå¯¹è±¡æ–‡ä»¶ï¼Œä¹Ÿæ— æ³•ç”Ÿæˆ GPU æœºå™¨ç ï¼ˆPTX/SASSï¼‰ï¼Œæ‰€ä»¥ kernel ä¸èƒ½åœ¨ GPU ä¸Šè¿è¡Œ

3ï¸âƒ£ nvcc çš„ä½œç”¨

- nvcc = **CUDA ç¼–è¯‘å™¨**
- åŠŸèƒ½æœ¬è´¨ï¼š
  1. **è§£æ CUDA æ‹“å±•**ï¼šè¯†åˆ« `__global__`ã€`__device__`ã€GPU å†…å»ºå˜é‡/å‡½æ•°
  2. **ç¼–è¯‘ GPU kernel**ï¼š
     - ç”Ÿæˆ PTXï¼ˆGPU è™šæ‹Ÿæœºä»£ç ï¼‰æˆ– SASSï¼ˆGPU ç¡¬ä»¶æœºå™¨ç ï¼‰
  3. **ç¼–è¯‘ Host ä»£ç **ï¼š
     - å¯¹ `.cu` ä¸­æ ‡å‡† C++ ä»£ç ï¼Œnvcc ä¼šè°ƒç”¨ g++/clang æ¥ç¼–è¯‘
  4. **é“¾æ¥ Host å’Œ Device**ï¼š
     - ç”Ÿæˆå¯¹è±¡æ–‡ä»¶ `.o`ï¼Œå…¶ä¸­åŒ…å« CPU å¯è°ƒç”¨çš„ launcher + GPU æœºå™¨ç 
- æ¢å¥è¯è¯´ï¼š**nvcc å†…éƒ¨å°±æ˜¯ g++ï¼ˆæˆ– clangï¼‰+ GPU å¤„ç†å™¨**

4ï¸âƒ£ ç¼–è¯‘æµç¨‹ç†è§£

```
.cu æ–‡ä»¶
â”‚
â”œâ”€ host C++ ä»£ç  â†’ g++ ç¼–è¯‘ â†’ host å¯¹è±¡ä»£ç 
â”‚
â””â”€ __global__ / __device__ GPU ä»£ç  â†’ nvcc â†’ PTX / SASS â†’ åŒ…å…¥å¯¹è±¡æ–‡ä»¶
```

- é“¾æ¥é˜¶æ®µï¼šhost å¯¹è±¡ä»£ç  + GPU æœºå™¨ç  â†’ æœ€ç»ˆå…±äº«åº“ `.so`
- Python è°ƒç”¨ `.so` æ—¶ï¼š
  - CPU è°ƒç”¨ host launcher
  - host launcher å¯åŠ¨ GPU kernel â†’ GPU å¹¶è¡Œæ‰§è¡Œ

âœ… æ€»ç»“

1. `.cu` æ–‡ä»¶ = C++ + CUDA è‡ªå®šä¹‰ç±»å‹/å…³é”®å­—æ‹“å±•
2. g++ ä¸èƒ½å•ç‹¬ç¼–è¯‘ `.cu`ï¼Œå› ä¸ºä¸è®¤è¯† CUDA æ‹“å±•
3. nvcc = åŒ…å« g++ + CUDA åŠŸèƒ½çš„ç¼–è¯‘å™¨
4. nvcc èƒ½æŠŠ host ä»£ç äº¤ç»™ g++ ç¼–è¯‘ï¼ŒæŠŠ GPU kernel ç¼–è¯‘æˆ GPU æœºå™¨ç 
5. æœ€ç»ˆç”Ÿæˆçš„å¯¹è±¡æ–‡ä»¶åŒ…å«ï¼š
   - CPU å¯æ‰§è¡Œä»£ç ï¼ˆlauncher / host å‡½æ•°ï¼‰
   - GPU å¯æ‰§è¡Œä»£ç ï¼ˆkernel PTX / SASSï¼‰

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª**ASCII æµç¨‹å›¾**ï¼ŒæŠŠ `.cu` å†… host/kernelã€nvcc/g++ã€å¯¹è±¡æ–‡ä»¶ã€å…±äº«åº“ã€Python è°ƒç”¨çš„æ•´ä¸ªé“¾æ¡éƒ½ç”»å‡ºæ¥ï¼Œè®©ä½ å¯¹æ•´ä¸ªæµç¨‹å®Œå…¨æ¸…æ¥šã€‚

å¥½çš„ï¼Œæˆ‘ç»™ä½ ç”»ä¸€ä¸ª**æ–‡å­—ç‰ˆ ASCII æµç¨‹å›¾**ï¼ŒæŠŠ `.cu` æ–‡ä»¶é‡Œçš„ host/kernelã€nvcc/g++ ç¼–è¯‘æµç¨‹ã€å¯¹è±¡æ–‡ä»¶ã€å…±äº«åº“ä»¥åŠ Python è°ƒç”¨å…³ç³»å…¨éƒ¨æ¢³ç†æ¸…æ¥šï¼š

```
Python è„šæœ¬è°ƒç”¨
    â”‚
    â–¼
myrelu.relu(x)  <-- Python è°ƒç”¨å…±äº«åº“ myrelu.so ä¸­çš„ relu_forward
    â”‚
    â–¼
relu_forward(torch::Tensor x)  <-- C++ å‰ç«¯å‡½æ•° (relu.cpp)
    â”‚
    â–¼
relu_cuda_launcher(x_ptr, y_ptr, n)  <-- CPU launcher (host) åœ¨ relu_kernel.cu ä¸­
    â”‚
    â–¼
===============================
|       CUDA Kernel Launch     |
|  __global__ void relu_kernel |
===============================
    â”‚
    â”œâ”€ CPU Host ä»£ç éƒ¨åˆ†ï¼ˆrelu_cuda_launcherã€å…¶ä»– host å‡½æ•°ï¼‰
    â”‚     â””â”€ ç¼–è¯‘ï¼šnvcc å†…éƒ¨è°ƒç”¨ g++ ç¼–è¯‘
    â”‚
    â””â”€ GPU Device ä»£ç éƒ¨åˆ†ï¼ˆ__global__ / __device__ï¼‰
          â””â”€ ç¼–è¯‘ï¼šnvcc ç¼–è¯‘æˆ PTX / SASS (GPU æœºå™¨ç )
    
ç¼–è¯‘ç”Ÿæˆï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   relu_kernel.o      â”‚
    â”‚  (host obj + GPU PTX)â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      relu.o         â”‚
    â”‚   (C++ å‰ç«¯å¯¹è±¡æ–‡ä»¶) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
 é“¾æ¥ (link) â†’ myrelu.so
           â”‚
           â–¼
Python import myrelu â†’ è°ƒç”¨ myrelu.relu() â†’ host launcher â†’ GPU kernel æ‰§è¡Œ
```

ğŸ”‘ æ ¸å¿ƒç†è§£

1. `.cu` æ–‡ä»¶ = C++ + CUDA æ‹“å±•
   - host launcher å‡½æ•° â†’ CPU ä¸Šæ‰§è¡Œ
   - kernel (`__global__`) â†’ GPU ä¸Šæ‰§è¡Œ
2. nvcc å†…éƒ¨åšä¸¤ä»¶äº‹ï¼š
   - CPU host ä»£ç  â†’ g++ ç¼–è¯‘æˆå¯¹è±¡æ–‡ä»¶
   - GPU kernel â†’ ç¼–è¯‘æˆ PTX / SASS
3. æœ€ç»ˆé“¾æ¥ï¼š
   - host å¯¹è±¡ + GPU PTX â†’ å…±äº«åº“ `.so`
   - Python ç›´æ¥ import å³å¯ä½¿ç”¨
4. Python è°ƒç”¨ï¼š
   - è°ƒç”¨ host launcher â†’ å†å¯åŠ¨ GPU kernel â†’ GPU å¹¶è¡Œè®¡ç®—

------

è¿™ä¸ªæµç¨‹å›¾å±•ç¤ºäº† **.cu æ–‡ä»¶å†…å®¹ã€ç¼–è¯‘å™¨ä½œç”¨ã€å¯¹è±¡æ–‡ä»¶ã€é“¾æ¥ã€Python è°ƒç”¨** çš„å®Œæ•´å…³ç³»ã€‚

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ å†ç”»ä¸€ä¸ª **æ›´ç®€åŒ–çš„é€»è¾‘å›¾**ï¼Œåªå…³æ³¨ **CPU host è°ƒç”¨ GPU kernel** çš„æ‰§è¡Œé¡ºåºï¼Œæ–¹ä¾¿è®°å¿†å¹¶å‘æ‰§è¡Œé€»è¾‘ã€‚

#### importèƒ½ç›´æ¥å¯¼å…¥soè¿›python

é—®é¢˜ï¼š

> test.pyä¸­çš„import myreluï¼Œå°±èƒ½ç›´æ¥æŠŠmyrelu.soå¯¼å…¥è¿›pythonå—ï¼Ÿ

å›ç­”ï¼š

æ˜¯çš„ï¼Œå¯ä»¥ç›´æ¥å¯¼å…¥ï¼ŒåŸå› å¦‚ä¸‹ï¼š

1ï¸âƒ£ Python import `.so` åŸç†

Python æ”¯æŒ **C/C++ æ‰©å±•æ¨¡å—**ï¼Œåªè¦å…±äº«åº“ç¬¦åˆä»¥ä¸‹æ¡ä»¶ï¼š

1. æ–‡ä»¶åå’Œæ¨¡å—åä¸€è‡´ï¼š

   ```
   myrelu.so
   ```

   å¯¹åº”ï¼š

   ```
   import myrelu
   ```

2. `.so` æ–‡ä»¶é‡ŒåŒ…å« **Python C API æˆ– pybind11 æš´éœ²çš„ç¬¦å·**ï¼š

   - æˆ‘ä»¬åœ¨ `relu.cpp` ä¸­å†™äº†ï¼š

     ```
     PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
         m.def("relu", &relu_forward, "ReLU CUDA kernel");
     }
     ```

   - `TORCH_EXTENSION_NAME` å¯¹åº” `myrelu`ï¼Œpybind11 ä¼šç”Ÿæˆ Python å¯è¯†åˆ«çš„æ¨¡å—ç¬¦å·

3. Python æœç´¢è·¯å¾„é‡Œæœ‰ `.so` æ–‡ä»¶ï¼š

   - `import` æ—¶ Python ä¼šåœ¨ `sys.path` ä¸­æŸ¥æ‰¾
   - `--inplace` ç¼–è¯‘åï¼Œ`myrelu.so` åœ¨å½“å‰ç›®å½•ï¼Œç›´æ¥ import å°±è¡Œ

2ï¸âƒ£ è°ƒç”¨æµç¨‹

```
import myrelu

x = torch.randn(10, device="cuda")
y = myrelu.relu(x)
```

å¯¹åº”æ‰§è¡Œé¡ºåºï¼š

1. `import myrelu`
    â†’ Python åŠ è½½ `myrelu.so`
    â†’ pybind11 åˆå§‹åŒ–æ¨¡å—ï¼Œç»‘å®š `relu_forward` åˆ° Python `myrelu.relu`
    
    ```shell
    # æ¯ä¸€æ¬¡ import myreluï¼ˆåœ¨ä¸€ä¸ªæ–°çš„ Python è¿›ç¨‹ä¸­ï¼‰ï¼Œéƒ½ä¼šè°ƒç”¨ä¸€æ¬¡ pybind11 çš„æ³¨å†Œä»£ç ã€‚
    # pybind11 çš„æ³¨å†Œå‘ç”Ÿåœ¨æ¨¡å—åˆå§‹åŒ–é˜¶æ®µã€‚
    import myrelu
    â†“
    åŠ è½½ myrelu.so
    â†“
    è°ƒç”¨ PyInit_myrelu()
    â†“
    æ‰§è¡Œ PYBIND11_MODULE(...) é‡Œçš„ä»£ç 
    â†“
    m.def(...)
    ```
    
2. `myrelu.relu(x)`
    â†’ è°ƒç”¨ `relu_forward(x)`ï¼ˆC++ host å‡½æ•°ï¼‰
    â†’ è°ƒç”¨ `relu_cuda_launcher`
    â†’ å¯åŠ¨ GPU kernel `relu_kernel<<<>>>`
    
3. GPU kernel åœ¨ GPU ä¸Šå¹¶è¡Œè®¡ç®—

4. è¿”å›ç»“æœåˆ° Python Tensor

3ï¸âƒ£ ğŸ”‘ æ ¸å¿ƒç†è§£

- **.so æ–‡ä»¶ = Python å¯è°ƒç”¨çš„ C/C++ æ‰©å±•æ¨¡å—**
- **pybind11** åšäº† Python â†” C++ çš„æ¡¥æ¥
- **nvcc + g++** ç”Ÿæˆçš„ `.so` è‡ªåŠ¨ç¬¦åˆ Python å¯åŠ è½½æ ¼å¼
- æ‰€ä»¥ `import myrelu` å°±åƒå¯¼å…¥æ™®é€š Python æ¨¡å—ä¸€æ ·

------

ç®€å•è®°å¿†æ–¹å¼ï¼š

```
Python import myrelu  -->  åŠ è½½ myrelu.so
myrelu.so å†…éƒ¨ = pybind11 module (C++ / CUDA å®ç°)
myrelu.relu()         --> è°ƒç”¨ C++ host â†’ GPU kernel æ‰§è¡Œ
```

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª **ASCII æ‰§è¡Œæµç¨‹å›¾**ï¼ŒæŠŠ `import myrelu` â†’ host launcher â†’ GPU kernel çš„æ•´ä¸ªæµç¨‹ä» Python è§’åº¦å±•ç¤ºå‡ºæ¥ï¼Œè®©ä½ å®Œå…¨çœ‹æ‡‚ import åˆ°æ‰§è¡Œçš„é“¾æ¡ã€‚

### nvccä»‹ç»

1ï¸âƒ£ nvcc æ˜¯ä»€ä¹ˆ

- **å…¨ç§°**ï¼šNVIDIA CUDA Compiler
- **ä½œç”¨**ï¼šæŠŠ CUDA æºç ï¼ˆ`.cu` æ–‡ä»¶ï¼‰ç¼–è¯‘æˆ GPU å¯æ‰§è¡Œä»£ç 
- **æœ¬è´¨**ï¼šä¸€ä¸ª **ç¼–è¯‘å™¨é©±åŠ¨ç¨‹åº**ï¼Œå†…éƒ¨è°ƒç”¨ g++/clang ç¼–è¯‘ CPU host ä»£ç ï¼ŒåŒæ—¶ç”Ÿæˆ GPU ä»£ç ï¼ˆPTX/SASSï¼‰

æ‰€ä»¥ä½ å¯ä»¥æŠŠ nvcc ç†è§£ä¸ºï¼š

```
nvcc = g++ (CPU) + GPU code generator
```

2ï¸âƒ£ nvcc ç¼–è¯‘åšäº†ä»€ä¹ˆ

å‡è®¾æœ‰ä¸€ä¸ª `.cu` æ–‡ä»¶ï¼š

```
__global__ void relu_kernel(...) { ... }  // GPU kernel
void relu_cuda_launcher(...) {            // CPU host
    relu_kernel<<<blocks, threads>>>(...);
}
```

nvcc ä¼šåšä¸¤ä»¶äº‹ï¼š

1. **CPU ä»£ç ï¼ˆhostï¼‰**
   - äº¤ç»™ g++/clang ç¼–è¯‘
   - ç”Ÿæˆå¯¹è±¡æ–‡ä»¶ `.o`
2. **GPU ä»£ç ï¼ˆkernelï¼‰**
   - ç¼–è¯‘ `__global__` / `__device__` å‡½æ•°
   - ç”Ÿæˆ PTXï¼ˆGPU è™šæ‹ŸæŒ‡ä»¤ï¼‰æˆ– SASSï¼ˆGPU æœºå™¨ç ï¼‰
   - åŒ…å«åœ¨ `.o` ä¸­ä¾› host è°ƒç”¨

3ï¸âƒ£ ä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨ g++ ç¼–è¯‘ `.cu` æ–‡ä»¶

- g++ ä¸è®¤è¯†ï¼š
  - `__global__` / `__device__` / `__shared__` ç­‰ CUDA å…³é”®å­—
  - GPU å†…å»ºå˜é‡/å‡½æ•°ï¼Œå¦‚ `threadIdx`ã€`blockIdx`ã€`atomicAdd` ç­‰
- å¦‚æœç”¨ g++ï¼Œä¼šæŠ¥è¯­æ³•é”™è¯¯æˆ–æ— æ³•ç”Ÿæˆ GPU ä»£ç 

4ï¸âƒ£ nvcc çš„è¾“å‡ºç»“æœ

- ä¸€ä¸ª `.cu` æ–‡ä»¶ â†’ nvcc â†’ `.o` å¯¹è±¡æ–‡ä»¶
- `.o` åŒ…å«ï¼š
  1. **CPU host å‡½æ•°**ï¼ˆå¦‚ launcherï¼‰
  2. **GPU kernel PTX / SASS**
- é“¾æ¥æˆ `.so` åï¼ŒPython å¯ä»¥ç›´æ¥è°ƒç”¨ host å‡½æ•°æ¥å¯åŠ¨ GPU kernel

5ï¸âƒ£ æ€»ç»“

1. `.cu` æ–‡ä»¶ = C++ + CUDA æ‹“å±•
2. nvcc = ç¼–è¯‘å™¨ + GPU code generator
3. nvcc ä¼šåŒæ—¶ç”Ÿæˆ CPU å¯æ‰§è¡Œéƒ¨åˆ†å’Œ GPU æœºå™¨ç 
4. Python + pybind11 è°ƒç”¨ `.so` æ—¶ï¼ŒCPU éƒ¨åˆ†å¯åŠ¨ GPU kernel

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª**ASCII æµç¨‹å›¾**ï¼ŒæŠŠ `.cu` æ–‡ä»¶ â†’ nvcc ç¼–è¯‘ â†’ CPU/GPU å¯¹è±¡ â†’ Python è°ƒç”¨çš„æ•´ä¸ªæµç¨‹å±•ç¤ºå‡ºæ¥ï¼Œè®©ä½ ä¸€çœ¼å°±çœ‹æ‡‚ nvcc åœ¨å¹²ä»€ä¹ˆã€‚

## è¿è¡Œæµ‹è¯•

```shell
python test.py 
```

è¾“å‡º

```shell
x: tensor([ 0.8085, -1.4251,  0.9437, -0.0358,  1.4235, -0.1531,  0.4118, -0.6072,
        -1.3712, -1.6067], device='cuda:0')
y: tensor([0.8085, 0.0000, 0.9437, 0.0000, 1.4235, 0.0000, 0.4118, 0.0000, 0.0000,
        0.0000], device='cuda:0')
correct: True
```

æ­å–œï¼Œä½ å·²ç»æˆåŠŸå¼€å‘äº†è‡ªå·±çš„ç¬¬ä¸€ä¸ª GPU ç®—å­ã€‚

ä½ ç°åœ¨å·²ç»å…·å¤‡ä»€ä¹ˆèƒ½åŠ›ï¼Ÿ

ä½ å·²ç»æŒæ¡äº†ï¼š

| èƒ½åŠ›                              | æ„ä¹‰           |
| --------------------------------- | -------------- |
| CUDA kernel çš„åŸºæœ¬ç»“æ„            | æ‰€æœ‰ç®—å­çš„æ ¸å¿ƒ |
| thread/block/grid å¹¶è¡Œæ€æƒ³        | GPU å¼€å‘çš„æœ¬è´¨ |
| PyTorch CUDA Extension            | å·¥ç¨‹å¿…é¡»æŒæ¡   |
| ä» C++ â†’ CUDA â†’ Python çš„å®Œæ•´é“¾è·¯ | ç®—å­å·¥ç¨‹åŒ–     |

ä»ç°åœ¨å¼€å§‹ä½ å¯ä»¥å†™ï¼š

- GELU
- Sigmoid
- Tanh
- Softplus
- Exp / Log
- Add / Mul / Pow
- LayerNormï¼ˆä¸‹ä¸€ä¸ªå…¥é—¨æå‡ï¼‰
- Softmaxï¼ˆå¹¶è¡Œ reduceï¼‰

å¦‚æœä½ æ„¿æ„ç»§ç»­ï¼Œæˆ‘å¯ä»¥å¸¦ä½ åšï¼š

è¿›é˜¶ç®—å­è·¯çº¿ï¼ˆæŒ‰éš¾åº¦æ’åˆ—ï¼‰

1. **ReLUï¼ˆä½ å·²å®Œæˆï¼‰**
2. **GELUï¼ˆåŠ ç‚¹æ•°å­¦ï¼Œä¾æ—§ç®€å•ï¼‰**
3. **LayerNormï¼ˆç¬¬ä¸€æ¬¡ç”¨ reduce + shared memoryï¼‰**
4. **Softmaxï¼ˆwarp reduce + stable softmaxï¼‰**
5. **çŸ©é˜µä¹˜ï¼ˆä¸ä¼˜åŒ–ç‰ˆï¼‰**
6. **ä¼˜åŒ–ç‰ˆ GEMMï¼ˆblock tiling + shared memoryï¼‰**
7. **Tensor Core ç‰ˆ GEMMï¼ˆCUTLASSï¼‰**

è¿™æ¡è·¯çº¿èµ°ä¸€éï¼Œä½ å°±æ˜¯çœŸæ­£çš„â€œGPU kernel å·¥ç¨‹å¸ˆâ€ã€‚

