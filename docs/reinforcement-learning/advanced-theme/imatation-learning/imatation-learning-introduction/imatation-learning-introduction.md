# 模仿学习介绍

* [返回上层目录](../imatation-learning.md)



模仿学习是强化学习的好伙伴，使用模仿学习可以让智能体在比强化学习短得多的时间内得到与人类操作相近的结果，但是这种做法并不能超越人类，而强化学习能够得到远超人类的智能体，但训练时间往往非常漫长。因此我们希望让智能体并不是从零开始学，我们给予智能体人类的演示，在学习人类演示的基础上，再进行强化学习。这样往往能大大减少强化学习的训练时间。在金字塔环境中，**只需要四轮人类的游戏数据，就能使训练步数减少四分之三以上**。因此，模仿学习和强化学习往往是一起使用的。好处是既能大大加快训练速度，又能得到超越人类的超高水准。

模仿学习是一种Supervised Learning（监督学习）的方法，也就是根据我们给定人类演示中的状态和对应的动作，就能训练智能体的策略网络去逼近我们的这个演示。光用模仿学习的缺点是，人类没有办法给出环境中所有的状态对应的做法，往往人类的演示中只包含了所有状态中的一小部分的应对方式，因此只进行模仿学习后，智能体没有办法应对人类演示数据中没有遇到过的情况，因此才需要用强化学习进行弥补。



# 参考资料

===

[许天，李子牛，俞扬，模仿学习简洁教程，2021](http://www.lamda.nju.edu.cn/xut/Imitation_Learning.pdf)

[【RLChina 2021】第10课 强化学习前沿（二）俞扬](https://www.bilibili.com/video/BV1qM4y1L7w9)

