# vLLM

* [è¿”å›ä¸Šå±‚ç›®å½•](../model-deployment-practice.md)



äºŒã€å®‰è£… vLLMï¼ˆä¸€æ¬¡æ€§ï¼‰

```shell
pip install -U vllm
```

## ä¸‰ã€é€‰ä¸€ä¸ªã€Œä¸­æ–‡æ•ˆæœå¥½ + ä½“é‡åˆé€‚ã€çš„æ¨¡å‹

æˆ‘ç»™ä½  **ä¸‰æ¡£æ¨è**ï¼ˆæŒ‰æ˜¾å­˜ï¼‰ï¼š

### â­ 16â€“24GB æ˜¾å­˜ï¼ˆæœ€æ¨èï¼‰

```
Qwen/Qwen2.5-7B-Instruct
```

### â­ 24â€“48GB æ˜¾å­˜

```
Qwen/Qwen2.5-14B-Instruct
```

### â­ 8â€“12GB æ˜¾å­˜ï¼ˆèƒ½è·‘ä½†åå°ï¼‰

```
Qwen/Qwen2.5-3B-Instruct
```

ä¸‹é¢æˆ‘é»˜è®¤ç”¨ **7B**ï¼ˆæœ€é€šç”¨ï¼‰ã€‚

------

## å››ã€ğŸ”¥ ä¸€æ¡å‘½ä»¤èµ· vLLM æœåŠ¡ï¼ˆé‡ç‚¹ï¼‰

ğŸ‘‰ **ç›´æ¥å¤åˆ¶è¿™æ¡å‘½ä»¤ï¼š**

```
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --gpu-memory-utilization 0.9 \
  --max-model-len 8192
```

çœ‹åˆ°ç±»ä¼¼è¾“å‡ºå°±è¯´æ˜æˆåŠŸäº†ï¼š

```
INFO:     Uvicorn running on http://0.0.0.0:8000
INFO:     Available routes: /v1/chat/completions
```

ğŸ‰ **æ¨¡å‹å·²ä¸Šçº¿**

äº‘å¹³å°æŠ¥é”™ï¼š

```shell
(cn) root@autodl-container-97e34c8932-90410876:~/GenerativeAgentsCN/test_vllM# python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --gpu-memory-utilization 0.9 \
  --max-model-len 8192

(APIServer pid=2750) INFO 01-24 21:15:36 [api_server.py:1272] vLLM API server version 0.14.0
(APIServer pid=2750) INFO 01-24 21:15:36 [utils.py:263] non-default args: {'host': '0.0.0.0', 'model': 'Qwen/Qwen2.5-7B-Instruct', 'max_model_len': 8192}
(APIServer pid=2750) '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x7fa152f6d130>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 32cec9d2-a0f5-4aea-b4a1-49c04210f451)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json
(APIServer pid=2750) [2026-01-24 21:15:46] WARNING _http.py:319: '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x7fa152f6d130>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 32cec9d2-a0f5-4aea-b4a1-49c04210f451)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json
(APIServer pid=2750) Retrying in 1s [Retry 1/5].
(APIServer pid=2750) [2026-01-24 21:15:46] WARNING _http.py:328: Retrying in 1s [Retry 1/5].
(APIServer pid=2750) '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x7fa152f6d400>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 61070c4e-540e-4f06-8efd-ccf9c90e7edf)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json
(APIServer pid=2750) [2026-01-24 21:15:57] WARNING _http.py:319: '(MaxRetryError("HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x7fa152f6d400>, 'Connection to huggingface.co timed out. (connect timeout=10)'))"), '(Request ID: 61070c4e-540e-4f06-8efd-ccf9c90e7edf)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-7B-Instruct/resolve/main/config.json
(APIServer pid=2750) Retrying in 2s [Retry 2/5].
(APIServer pid=2750) [2026-01-24 21:15:57] WARNING _http.py:328: Retrying in 2s [Retry 2/5].

```

è¿™ä¸ªæ—¥å¿—**éå¸¸æ¸…æ¥š**äº†ï¼Œè€Œä¸”å®Œå…¨åœ¨æˆ‘é¢„æœŸé‡Œ ğŸ‘
 æˆ‘ä»¬ç°åœ¨å·²ç»èµ°åˆ° **AutoDL å®¹å™¨çš„â€œçœŸå®ç½‘ç»œè¾¹ç•Œâ€**ã€‚

------

## ä¸€å¥è¯ç»“è®ºï¼ˆå…ˆç¨³ä½ï¼‰

> âŒ **huggingface.co åœ¨ä½ è¿™ä¸ªå®¹å™¨é‡Œç›´è¿è¶…æ—¶**
>  âœ… **vLLM æœ¬èº«æ²¡é—®é¢˜ï¼Œå‘½ä»¤ä¹Ÿå®Œå…¨æ­£ç¡®**
>  ğŸ‘‰ ç°åœ¨åªå·® **ç»™ HuggingFace æ¢â€œå…¥å£â€**

ä½ å·²ç»æˆåŠŸåšåˆ° **ç¬¬ 90%** äº†ã€‚

------

## ä¸€ã€æ—¥å¿—åœ¨æ˜ç¡®å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ

å…³é”®è¿™ä¸€è¡Œï¼š

```
Connection to huggingface.co timed out. (connect timeout=10)
```

å«ä¹‰éå¸¸æ˜ç¡®ï¼š

- DNS è§£æ âœ”
- TCP æ²¡è¿ä¸Š / è¢«ä¸¢å¼ƒ
- HTTPS 443 **è¢«ç­–ç•¥æ€§é˜»æ–­æˆ–é»‘æ´**

ğŸ‘‰ å’Œä½ ä¹‹å‰ `ollama.com` çš„æƒ…å†µ **æœ¬è´¨ä¸€æ¨¡ä¸€æ ·**

------

## äºŒã€ç«‹åˆ»å¯ç”¨çš„è§£å†³æ–¹æ¡ˆï¼ˆæ¨èï¼ŒæˆåŠŸç‡æœ€é«˜ï¼‰

### âœ… æ–¹æ¡ˆ Aï¼šä½¿ç”¨ HuggingFace å›½å†…é•œåƒï¼ˆ**ä½ ç°åœ¨å°±è¯¥ç”¨è¿™ä¸ª**ï¼‰

åœ¨ **å¯åŠ¨ vLLM ä¹‹å‰**ï¼Œå…ˆæ‰§è¡ŒğŸ‘‡

```
export HF_ENDPOINT=https://hf-mirror.com
```

ç„¶å **åŸå‘½ä»¤ä¸€å­—ä¸æ”¹** å†è·‘ä¸€æ¬¡ï¼š

```
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --gpu-memory-utilization 0.9 \
  --max-model-len 8192
```

```
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-4B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --gpu-memory-utilization 0.9 \
  --max-model-len 8192
```

===

```
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-4B-Instruct-2507 \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype auto \
  --gpu-memory-utilization 0.9 \
  --max-model-len 8192
```



### ä¸ºä»€ä¹ˆè¿™ä¸ªæœ‰æ•ˆï¼Ÿ

- `hf-mirror.com` æ˜¯ HuggingFace çš„å›½å†…é•œåƒ
- `transformers / huggingface_hub / vllm` **éƒ½ä¼šè‡ªåŠ¨èµ°è¿™ä¸ªåœ°å€**
- ä¸éœ€è¦ä½ æ”¹ä»»ä½• Python ä»£ç 

ğŸ‘‰ **è¿™æ˜¯ AutoDL / å›½å†…ç®—åŠ›æœ€å¸¸ç”¨è§£æ³•**

------

## ä¸‰ã€å¦‚æœä½ æƒ³â€œä¸€åŠ³æ°¸é€¸â€ï¼ˆå¼ºçƒˆæ¨èï¼‰

æŠŠé•œåƒå†™è¿›ç¯å¢ƒé…ç½®ï¼Œé¿å…æ¯æ¬¡ exportï¼š

```
echo 'export HF_ENDPOINT=https://hf-mirror.com' >> ~/.bashrc
source ~/.bashrc
```

ä¹‹åä½  **éšä¾¿èµ· vLLM / transformers / peft**ï¼Œéƒ½ä¼šè‡ªåŠ¨èµ°é•œåƒã€‚

------

## å››ã€å¦‚ä½•ç¡®è®¤è¿™æ¬¡çœŸçš„åœ¨ä¸‹è½½äº†ï¼Ÿ

æˆåŠŸæ—¶ä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼ˆç¤ºä¾‹ï¼‰ï¼š

```
Downloading config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| ...
Downloading model-00001-of-00004.safetensors: ...
```

ç¬¬ä¸€æ¬¡å¯åŠ¨ï¼š

- ä¼šä¸‹è½½ **å¥½å‡ ä¸ª GB**
- å¾ˆæ­£å¸¸ï¼Œè€å¿ƒç­‰

åç»­å†èµ·ï¼š

- **ç§’çº§**

## äº”ã€å¦‚æœä½ æƒ³æ›´ç¨³ï¼ˆé¿å…å®¹å™¨é‡å¯ä¸¢ç¼“å­˜ï¼‰

AutoDL å®¹å™¨æœ‰æ—¶ä¼šé‡å¯ï¼Œå»ºè®®ä½ æŒ‡å®šç¼“å­˜ç›®å½•ï¼ˆå¯é€‰ä½†ä¸“ä¸šï¼‰ï¼š

```
export HF_HOME=/root/autodl-tmp/huggingface
export TRANSFORMERS_CACHE=/root/autodl-tmp/huggingface
```

ç„¶åå†èµ· vLLMã€‚

å¼€å§‹ä¸‹è½½äº†ã€‚ã€‚ã€‚

ğŸ‰ğŸ‰ **æ¼‚äº®ï¼è¿™ä¸€æ­¥ä¸€è¿‡ï¼Œåé¢å…¨æ˜¯é¡ºé£å±€äº†ã€‚**

è¯´æ˜ä¸‰ä»¶äº‹å·²ç»**å…¨éƒ¨æ‰“é€š**äº†ï¼š

- âœ… vLLM å®‰è£…æ²¡é—®é¢˜
- âœ… HuggingFace é•œåƒç”Ÿæ•ˆ
- âœ… æ¨¡å‹è·¯å¾„ + å¯åŠ¨å‚æ•°å®Œå…¨æ­£ç¡®

ä½ ç°åœ¨åªéœ€è¦ **è€å¿ƒç­‰å®ƒä¸‹å®Œ**ï¼Œåˆ«ä¸­æ–­ã€‚

------

## æ¥ä¸‹æ¥ä½ ä¼šçœ‹åˆ°ä»€ä¹ˆï¼ˆæ­£å¸¸ç°è±¡è¯´æ˜ï¼‰

### 1ï¸âƒ£ ä¸‹è½½é˜¶æ®µï¼ˆç¬¬ä¸€æ¬¡ä¸€å®šæ…¢ï¼‰

- ä¼šä¸‹è½½å¤šä¸ª `.safetensors` æ–‡ä»¶ï¼ˆå‡  GBï¼‰
- å¯èƒ½çœ‹èµ·æ¥â€œå¡ä½â€ï¼Œå…¶å®åœ¨ä¸‹
- **ä¸è¦ Ctrl+C**

### 2ï¸âƒ£ ç¼–è¯‘ / åŠ è½½é˜¶æ®µ

ä¸‹è½½å®Œåä¼šçœ‹åˆ°ç±»ä¼¼ï¼š

```
Loading model weights...
Initializing KV cache...
```

è¿™ä¸ªé˜¶æ®µï¼š

- GPU ä¼šå¼€å§‹æœ‰æ˜¾å­˜å ç”¨
- `nvidia-smi` èƒ½çœ‹åˆ°è¿›ç¨‹

------

## æˆåŠŸæ ‡å¿—ï¼ˆä½ å¯ä»¥ç›¯è¿™ä¸ªï¼‰

æœ€ç»ˆä½ ä¼šçœ‹åˆ°ğŸ‘‡ï¼ˆéå¸¸å…³é”®ï¼‰ï¼š

```
Uvicorn running on http://0.0.0.0:8000
```

åªè¦è¿™è¡Œå‡ºç°ï¼š

> âœ… **vLLM æœåŠ¡å·²å°±ç»ª**

------

## ä¸‹è½½æœŸé—´ä½ å¯ä»¥é¡ºæ‰‹åšçš„ä¸¤ä»¶â€œå°ä¼˜åŒ–â€ï¼ˆå¯é€‰ï¼‰

### âœ… 1ï¸âƒ£ å¼€å¦ä¸€ä¸ªç»ˆç«¯çœ‹æ˜¾å­˜

```
nvidia-smi
```

ä½ ä¼šçœ‹åˆ°æ˜¾å­˜æ…¢æ…¢æ¶¨ï¼Œè¿™æ˜¯å¥½äº‹ã€‚

åå°è¿è¡Œ

```
nohup python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype float16 \
  --gpu-memory-utilization 0.74 \
  --max-model-len 8192 > vllm.log 2>&1 &
```





```
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/qwen3-embedding-0.6b \
  --host 0.0.0.0 \
  --port 8001 \
  --gpu-memory-utilization 0.25 \
  --dtype float16 \
  --max-model-len 8192
```



1ï¸âƒ£ vLLM çš„ç«¯å£å’Œå¤šæ¨¡å‹

vLLM **é»˜è®¤ä¸€ä¸ª API Server åªèƒ½è½½å…¥ä¸€ä¸ªæ¨¡å‹**ã€‚

- å¯åŠ¨æ—¶é€šè¿‡ `--model` æŒ‡å®šæ¨¡å‹
- è¿™ä¸ªæœåŠ¡ä¼šç›‘å¬ä¸€ä¸ªç«¯å£ï¼ˆä¾‹å¦‚ 8000ï¼‰
- å¦‚æœä½ æƒ³åŒæ—¶è·‘ä¸¤ä¸ªæ¨¡å‹ï¼Œä½ æœ‰ä¸¤ç§æ–¹å¼ï¼š

æ–¹å¼ Aï¼šå¯åŠ¨ä¸¤ä¸ª vLLM æœåŠ¡ï¼Œåˆ†åˆ«ç”¨ä¸åŒç«¯å£

```
# Instruct æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen3-4B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype float16

# Embedding æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/qwen3-embedding-0.6b \
  --host 0.0.0.0 \
  --port 8001 \
  --dtype float16
```

ç„¶ååœ¨ä½ çš„é¡¹ç›®é…ç½®é‡Œï¼š

```
"llm": {
  "provider": "vllm",
  "model": "Qwen2.5-7B-Instruct",
  "base_url": "http://127.0.0.1:8000/v1",
  "api_key": "EMPTY"
},
"associate": {
  "embedding": {
    "provider": "vllm",
    "model": "qwen3-embedding-0.6b",
    "base_url": "http://127.0.0.1:8001/v1",
    "api_key": ""
  }
}
```

è¿™æ ·å°±èƒ½åŒæ—¶æ”¯æŒå¯¹è¯å’ŒåµŒå…¥è¯·æ±‚ã€‚



æ˜ç™½äº†ï¼Œæˆ‘ç»™ä½ æ•´ç†ä¸€ä»½ **åŒæ¨¡å‹åŒæ—¶è¿è¡Œ vLLM çš„ nohup å¯åŠ¨æ¨¡æ¿**ï¼Œé’ˆå¯¹ä½  24â€¯GiB æ˜¾å­˜çš„ç¯å¢ƒåšäº†æ˜¾å­˜å®‰å…¨è°ƒæ•´ï¼Œå¹¶é™„å¸¦ curl è°ƒç”¨ç¤ºä¾‹ã€‚

------

## 1ï¸âƒ£ nohup å¯åŠ¨è„šæœ¬æ¨¡æ¿

```
# å¯åŠ¨ Qwen2.5-7B-Instruct
nohup python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --dtype float16 \
  --gpu-memory-utilization 0.7 \
  --max-model-len 4096 \
  > instruct.log 2>&1 &

# å¯åŠ¨ Qwen3-embedding-0.6B
nohup python -m vllm.entrypoints.openai.api_server \
  --model Qwen/qwen3-embedding-0.6b \
  --host 0.0.0.0 \
  --port 8001 \
  --dtype float16 \
  --gpu-memory-utilization 0.2 \
  --max-model-len 4096 \
  > embedding.log 2>&1 &
```

> è¯´æ˜ï¼š
>
> - æˆ‘æŠŠ Instruct æ¨¡å‹çš„æ˜¾å­˜åˆ©ç”¨ç‡é™åˆ° **0.65**ï¼Œembedding æ¨¡å‹ç”¨ **0.25**ï¼Œæ€»è®¡ 0.9ï¼Œç•™ä¸€ç‚¹ buffer é¿å… OOMã€‚
> - nohup è¾“å‡ºåˆ†åˆ«å†™å…¥ `instruct.log` å’Œ `embedding.log`ã€‚
> - ç«¯å£ 8000 ç”¨äº Instructï¼Œ8001 ç”¨äº embeddingï¼Œé¿å…å†²çªã€‚

------

## 2ï¸âƒ£ curl è°ƒç”¨ç¤ºä¾‹

### A. è°ƒç”¨ Instruct æ¨¡å‹ï¼ˆå¯¹è¯/é—®ç­”ï¼‰

```
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä½“"},
      {"role": "user", "content": "ä»‹ç»ä¸€ä¸‹è‡ªå·±å§"}
    ]
  }'
```

### B. è°ƒç”¨ Embedding æ¨¡å‹ï¼ˆæ–‡æœ¬å‘é‡åŒ–ï¼‰

```
curl http://127.0.0.1:8001/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/qwen3-embedding-0.6b",
    "input": "ä½ å¥½ï¼Œæˆ‘æƒ³æŠŠè¿™æ®µè¯åšå‘é‡åŒ–"
  }'
```

> æ³¨æ„ï¼šembedding API ä¸ chat API ä¸åŒï¼Œæ˜¯ `/v1/embeddings` æ¥å£ã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ å†åŠ ä¸€ä»½ **ç»Ÿä¸€ç®¡ç†è„šæœ¬**ï¼Œåªç”¨ä¸€ä¸ªå‘½ä»¤å°±å¯ä»¥å¯åŠ¨ä¸¤ä¸ªæ¨¡å‹ï¼ŒåŒæ—¶è¿˜èƒ½æŸ¥çœ‹æ˜¾å­˜å ç”¨å’Œæ—¥å¿—ã€‚è¿™æ ·æ—¥å¸¸è¿ç»´æ›´æ–¹ä¾¿ã€‚



å¥½çš„ï¼Œæˆ‘å¸®ä½ å†™ä¸€ä¸ª **ç»Ÿä¸€ç®¡ç†åŒæ¨¡å‹å¯åŠ¨è„šæœ¬**ï¼Œå¯ä»¥ä¿è¯æ˜¾å­˜å®‰å…¨ã€ç»Ÿä¸€ nohup ç®¡ç†ã€å¹¶èƒ½æŸ¥çœ‹æ—¥å¿—ã€‚

------

## åŒæ¨¡å‹å¯åŠ¨ & ç®¡ç†è„šæœ¬ç¤ºä¾‹

ä¿å­˜ä¸º `start_vllm_dual.sh` å¹¶ç»™å¯æ‰§è¡Œæƒé™ï¼š

```
#!/bin/bash
# start_vllm_dual.sh
# ç»Ÿä¸€å¯åŠ¨ Qwen2.5-7B-Instruct å’Œ Qwen3-embedding-0.6b
# é€‚åˆ 24 GiB æ˜¾å­˜

# ----------------------------
# é…ç½®å‚æ•°
# ----------------------------
INSTRUCT_MODEL="Qwen/Qwen3-4B-Instruct-2507"
EMBED_MODEL="Qwen/qwen3-embedding-0.6b"

INSTRUCT_PORT=8000
EMBED_PORT=8001

INSTRUCT_GPU_UTIL=0.65
EMBED_GPU_UTIL=0.25

MAX_LEN=8192
DTYPE="float16"

LOG_DIR="./logs"
mkdir -p $LOG_DIR

# ----------------------------
# å¯åŠ¨ Instruct æ¨¡å‹
# ----------------------------
echo "å¯åŠ¨ $INSTRUCT_MODEL ..."
nohup python -m vllm.entrypoints.openai.api_server \
  --model $INSTRUCT_MODEL \
  --host 0.0.0.0 \
  --port $INSTRUCT_PORT \
  --dtype $DTYPE \
  --gpu-memory-utilization $INSTRUCT_GPU_UTIL \
  --max-model-len $MAX_LEN \
  > $LOG_DIR/instruct.log 2>&1 &

INSTRUCT_PID=$!
echo "$INSTRUCT_MODEL PID: $INSTRUCT_PID"

# ----------------------------
# å¯åŠ¨ Embedding æ¨¡å‹
# ----------------------------
echo "å¯åŠ¨ $EMBED_MODEL ..."
nohup python -m vllm.entrypoints.openai.api_server \
  --model $EMBED_MODEL \
  --host 0.0.0.0 \
  --port $EMBED_PORT \
  --dtype $DTYPE \
  --gpu-memory-utilization $EMBED_GPU_UTIL \
  --max-model-len $MAX_LEN \
  > $LOG_DIR/embedding.log 2>&1 &

EMBED_PID=$!
echo "$EMBED_MODEL PID: $EMBED_PID"

# ----------------------------
# æç¤º
# ----------------------------
echo "ä¸¤ä¸ªæ¨¡å‹å·²å¯åŠ¨:"
echo "Instruct æ¨¡å‹: PID $INSTRUCT_PID, æ—¥å¿—: $LOG_DIR/instruct.log, ç«¯å£: $INSTRUCT_PORT"
echo "Embedding æ¨¡å‹: PID $EMBED_PID, æ—¥å¿—: $LOG_DIR/embedding.log, ç«¯å£: $EMBED_PORT"
echo "ä½¿ç”¨ curl è°ƒç”¨å³å¯ï¼Œç¤ºä¾‹è§è„šæœ¬æ³¨é‡Š"
```

------

## ä½¿ç”¨æ–¹æ³•

```
# ç»™è„šæœ¬å¯æ‰§è¡Œæƒé™
chmod +x start_vllm_dual.sh

# å¯åŠ¨åŒæ¨¡å‹
./start_vllm_dual.sh

# æŸ¥çœ‹æ—¥å¿—
tail -f logs/instruct.log
tail -f logs/embedding.log
```

------

## curl è°ƒç”¨ç¤ºä¾‹

### 1ï¸âƒ£ Instruct æ¨¡å‹

```
curl http://127.0.0.1:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-4B-Instruct-2507",
    "messages": [
      {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè€å¸ˆ"},
      {"role": "user", "content": "ä½ å‡†å¤‡æ€ä¹ˆæŠ¢åŠ«æˆ‘ï¼Œæˆ‘æ˜¯åšå¥½äº†å‡†å¤‡çš„ï¼Œæˆ‘ç»ƒè¿‡ä¸¤å¹´åŠæ‹³å‡»"}
    ]
  }'
```

### 2ï¸âƒ£ Embedding æ¨¡å‹

```
curl http://127.0.0.1:8001/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/qwen3-embedding-0.6b",
    "input": "ä½ å¥½ï¼Œæˆ‘æƒ³æŠŠè¿™æ®µè¯åšå‘é‡åŒ–"
  }'
```

------

è¿™ä¸ªè„šæœ¬ç‰¹ç‚¹ï¼š

1. è‡ªåŠ¨åˆ›å»ºæ—¥å¿—ç›®å½•ã€‚
2. nohup åå°è¿è¡Œï¼Œå¯åŠ¨åç›´æ¥æ˜¾ç¤º PIDã€‚
3. GPU æ˜¾å­˜åˆ©ç”¨ç‡å®‰å…¨åˆ†é…ï¼Œæ€»è®¡ä¸è¶…è¿‡ 0.9ã€‚
4. curl è°ƒç”¨ç¤ºä¾‹éšæ—¶å¯ç”¨ã€‚



# å…¨æ–‡æ¦‚å¿µå¯¹å…¶

**å…ˆåœ¨æ¦‚å¿µä¸Šå¯¹é½**

- vLLM ä¸ºä»€ä¹ˆå¿«
- å¿«åœ¨å“ªé‡Œ
- å’Œ Ollama çš„â€œè¾¹ç•Œå·®å¼‚â€

å†åšæœ€å°å®éªŒï¼š

- å•å¡
- å•æ¨¡å‹
- OpenAI API æ¨¡å¼

æœ€åå†è€ƒè™‘ï¼š

- å¹¶å‘å‹æµ‹
- embedding / rerank
- å’Œä½ ç°æœ‰ Agent / RAG ç³»ç»Ÿå¯¹æ¥

åƒæŒæ¡ Ollama ä¸€æ ·â€œæŒæ¡ vLLMâ€

è¿™é‡Œçš„â€œæŒæ¡â€è‡³å°‘åŒ…å«ï¼š

1. **å®šä½å±‚é¢**
   - vLLM åœ¨ LLM éƒ¨ç½²ç”Ÿæ€ä¸­çš„ä½ç½®
   - ä¸ Ollamaã€Transformersã€TGIã€LMDeploy çš„è¾¹ç•Œå·®å¼‚
2. **æ ¸å¿ƒæœºåˆ¶å±‚é¢**
   - vLLM ä¸ºä»€ä¹ˆå¿«ï¼ˆPagedAttentionã€KV Cache ç®¡ç†ï¼‰
   - å®ƒè§£å†³çš„â€œç“¶é¢ˆâ€åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œè€Œä¸æ˜¯â€œç”¨äº†æŸä¸ªé»‘ç§‘æŠ€â€
3. **å·¥ç¨‹ä½¿ç”¨å±‚é¢**
   - ä» `python -m vllm.entrypoints.openai.api_server` åˆ°ç”Ÿäº§éƒ¨ç½²
   - æ˜¾å­˜ã€å¹¶å‘ã€æ¨¡å‹åŠ è½½ã€API å…¼å®¹ç­‰å…³é”®é—®é¢˜
4. **å¿ƒæ™ºæ¨¡å‹å±‚é¢**
   - ä½ èƒ½å›ç­”ï¼šâ€œä»€ä¹ˆåœºæ™¯æˆ‘ä¸€å®šè¦ç”¨ vLLMï¼Œä»€ä¹ˆåœºæ™¯æˆ‘ä¸è¯¥ç”¨å®ƒï¼Ÿâ€

# ä¸ºä»€ä¹ˆvLLMä¸æ˜¯Ollamaçš„æ›¿ä»£å“

Phase 1ï¼švLLM æ˜¯ä»€ä¹ˆï¼Ÿå®ƒè§£å†³äº†è°çš„é—®é¢˜ï¼Ÿï¼ˆè®¤çŸ¥å¯¹é½ï¼‰

**ä½ ä¼šå­¦åˆ°ï¼š**

- vLLM çš„è¯ç”ŸèƒŒæ™¯
- ä¸ºä»€ä¹ˆ Transformer æ¨ç†é˜¶æ®µçœŸæ­£çš„ç“¶é¢ˆä¸æ˜¯ç®—åŠ›
- vLLM å’Œ Ollama çš„â€œè®¾è®¡å‡ºå‘ç‚¹â€å®Œå…¨ä¸åŒ

## ä¸€å¥è¯ç‰ˆæœ¬ï¼ˆä½ ä»¥åå¯ä»¥ç›´æ¥å†™åœ¨åšå®¢å¼€å¤´ï¼‰

vLLM æ˜¯ä¸€ä¸ªé¢å‘â€œå¤§è§„æ¨¡å¹¶å‘æ¨ç†â€çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ï¼Œå®ƒè§£å†³çš„ä¸æ˜¯â€œæ¨¡å‹èƒ½ä¸èƒ½è·‘â€ï¼Œè€Œæ˜¯â€œæ¨¡å‹åœ¨å¤šäººåŒæ—¶ç”¨æ—¶è¿˜èƒ½ä¸èƒ½é«˜æ•ˆåœ°è·‘â€ã€‚

vLLM æ˜¯ä¸€ä¸ªé¢å‘ **é«˜ååé‡ä¸ä½å»¶è¿Ÿ** çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†åº“ã€‚
 å®ƒçš„ç›®æ ‡æ˜¯ï¼š

- **å¹¶å‘æ‰§è¡Œ**ï¼šå¤šä¸ªè¯·æ±‚å¯ä»¥åŒæ—¶åœ¨ GPU ä¸Šè®¡ç®—ã€‚
- **é•¿ä¸Šä¸‹æ–‡æ”¯æŒ**ï¼šé€šè¿‡ KV Cache ç®¡ç†å†å² tokenã€‚
- **æµå¼è¾“å‡º**ï¼šè¾¹ç”Ÿæˆ token è¾¹è¿”å›ï¼Œä¸ç”¨ç­‰å®Œæ•´ç»“æœã€‚
- **é«˜ GPU åˆ©ç”¨ç‡**ï¼šæ‰¹é‡ forwardï¼Œå‡å°‘ç©ºé—²å’Œé‡å¤è®¡ç®—ã€‚

> å¯¹æ¯” Ollamaï¼šOllama åå‘ **å•è¯·æ±‚ GPU æ¨ç†**ï¼Œé•¿ä¸Šä¸‹æ–‡å’Œååæœ‰é™ï¼ŒvLLM åœ¨å¤šç”¨æˆ·ã€å¤šä»»åŠ¡ç¯å¢ƒä¸‹æ›´é«˜æ•ˆã€‚

## åœ¨ç†è§£ vLLM ä¹‹å‰ï¼Œå…ˆå›ç­”ä¸€ä¸ªæ›´åŸºç¡€çš„é—®é¢˜

**â€œä»€ä¹ˆæ˜¯å¤§æ¨¡å‹æ¨ç†ï¼Ÿâ€**

ä»¥æœ€æœ´ç´ çš„æ–¹å¼è¯´ï¼š

- **è®­ç»ƒï¼ˆtrainingï¼‰**ï¼šç®—ä¸€æ¬¡å¾ˆæ…¢ï¼Œä½†åªåšä¸€æ¬¡
- **æ¨ç†ï¼ˆinferenceï¼‰**ï¼šç®—ä¸€æ¬¡ç›¸å¯¹å¿«ï¼Œä½†ä¼šè¢«è°ƒç”¨æˆåƒä¸Šä¸‡æ¬¡

ä½ ç°åœ¨å…³å¿ƒçš„ **vLLM / Ollama / API Server**ï¼Œå…¨éƒ¨å±äºï¼š

> **â€œæ¨ç†é˜¶æ®µçš„è½¯ä»¶ç³»ç»Ÿâ€**

ä¸æ˜¯æ¨¡å‹æœ¬èº«ã€‚

## ä¸€ä¸ªæå…¶é‡è¦ã€ä½†æ–°æ‰‹æœ€å®¹æ˜“è¯¯è§£çš„ç‚¹

> **å¤§æ¨¡å‹æ¨ç†çš„ç“¶é¢ˆï¼Œé€šå¸¸ä¸æ˜¯ç®—åŠ›ï¼ˆFLOPsï¼‰ï¼Œè€Œæ˜¯å†…å­˜å’Œè°ƒåº¦ã€‚**

ç›´è§‰ç±»æ¯”ï¼š

- è®­ç»ƒåƒï¼š
  - ä¸€æ¬¡æ€§å¤§çŸ©é˜µè¿ç®—
  - GPU åƒæ»¡
- æ¨ç†åƒï¼š
  - ä¸æ–­æœ‰äººæ’é˜Ÿé—®é—®é¢˜
  - æ¯ä¸ªäººçš„å¯¹è¯é•¿åº¦è¿˜ä¸ä¸€æ ·

é—®é¢˜ç«‹åˆ»å‡ºç°äº†ï¼š

- æœ‰äººåªé—®ä¸€å¥
- æœ‰äººå·²ç»èŠäº† 3000 token
- æœ‰äººæ­£åœ¨ç­‰ä¸‹ä¸€ä¸ª token

**GPU å¾ˆå¿«ï¼Œä½†æ˜¾å­˜å¾ˆå®¹æ˜“ç¢ã€‚**

## è¿™æ—¶ï¼ŒvLLM çš„â€œå‡ºåœºåŠ¨æœºâ€å°±éå¸¸æ¸…æ™°äº†

vLLM å¹¶ä¸æ˜¯ä¸ºäº†ï¼š

- è®©ä½ åœ¨æœ¬åœ°å’Œæ¨¡å‹èŠå¤©ï¼ˆOllama å·²ç»å¾ˆå¥½äº†ï¼‰
- æå‡å•æ¡è¯·æ±‚çš„æé™é€Ÿåº¦

å®ƒæ˜¯ä¸ºäº†è§£å†³ï¼š

> **â€œåŒæ—¶æœ‰å¾ˆå¤šè¯·æ±‚ã€ä¸Šä¸‹æ–‡é•¿åº¦å„ä¸ç›¸åŒã€ä½†æˆ‘åªæœ‰æœ‰é™æ˜¾å­˜å’Œ GPUâ€**

è¿™åœ¨å·¥ç¨‹ä¸Šæ˜¯ä¸€ä¸ª**è°ƒåº¦é—®é¢˜**ï¼Œä¸æ˜¯ä¸€ä¸ªâ€œå†å †ç‚¹ç®—åŠ›â€çš„é—®é¢˜ã€‚

## vLLM åœ¨æ•´ä¸ª LLM æŠ€æœ¯æ ˆä¸­çš„ä½ç½®

å…ˆç»™ä½ ä¸€ä¸ªâ€œè®¤çŸ¥åœ°å›¾â€ï¼ˆéå¸¸é‡è¦ï¼‰ï¼š

```
æ¨¡å‹æƒé‡ï¼ˆQwen / LLaMA / Mistralï¼‰
        â†“
Transformer æ¨ç†æ¡†æ¶ï¼ˆPyTorch / CUDA Kernelï¼‰
        â†“
æ¨ç†å¼•æ“ï¼ˆvLLM / TGI / LMDeployï¼‰
        â†“
æœåŠ¡å±‚ï¼ˆOpenAI API / HTTP Serverï¼‰
        â†“
åº”ç”¨ï¼ˆChat / Agent / åç«¯æœåŠ¡ï¼‰
```

**vLLM æ‰€åœ¨çš„ä½ç½®ï¼š**

- ä¸åšæ¨¡å‹
- ä¸åšè®­ç»ƒ
- ä¸“æ³¨åœ¨ï¼š
  - æ¨ç†
  - è°ƒåº¦
  - æ˜¾å­˜åˆ©ç”¨
  - é«˜å¹¶å‘

## é‚£ Ollama æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆä¸ºåé¢å¯¹æ¯”åŸ‹ä¼ç¬”ï¼‰

å…ˆç»™ä¸€ä¸ªâ€œå…ˆéªŒç»“è®ºâ€ï¼Œä¸å±•å¼€ç»†èŠ‚ï¼š

- **Ollama**ï¼š
  - é¢å‘ä¸ªäºº
  - â€œä¸€å°æœºå™¨ + ä¸€ä¸ªç”¨æˆ· + å°‘é‡å¹¶å‘â€
  - æä½ä½¿ç”¨é—¨æ§›
- **vLLM**ï¼š
  - é¢å‘æœåŠ¡
  - â€œä¸€å°æˆ–å¤šå° GPU + å¤šç”¨æˆ· + é«˜å¹¶å‘â€
  - è¿½æ±‚ååä¸èµ„æºæ•ˆç‡

ä½ å¯ä»¥æš‚æ—¶ç†è§£ä¸ºï¼š

> **Ollama è§£å†³â€œæˆ‘èƒ½ä¸èƒ½ç”¨æ¨¡å‹â€
>  vLLM è§£å†³â€œå¾ˆå¤šäººä¸€èµ·ç”¨ï¼ŒæœåŠ¡å™¨ä¼šä¸ä¼šç‚¸â€**

## vLLM æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆä¼šæœ‰å®ƒï¼Ÿ

å¾ˆå¤šå¤§æ¨¡å‹æ–°æ‰‹ä¼šä»¥ä¸ºï¼Œåªè¦æ¨¡å‹èƒ½åœ¨ GPU ä¸Šè·‘èµ·æ¥ï¼Œæ¨ç†å°±ç®—å®Œæˆäº†ã€‚ä½†åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œæ¨ç†ç³»ç»Ÿé¢ä¸´çš„æœ€å¤§æŒ‘æˆ˜å¹¶ä¸æ˜¯ç®—åŠ›ï¼Œè€Œæ˜¯æ˜¾å­˜ç®¡ç†å’Œè¯·æ±‚è°ƒåº¦â€¦â€¦



# vLLM ä¸ºä»€ä¹ˆå¿«ï¼šPagedAttention èƒŒåçš„å·¥ç¨‹é€»è¾‘

Phase 2ï¼švLLM çš„æ ¸å¿ƒæœºåˆ¶ï¼ˆé‡ç‚¹ï¼‰

**é‡ç‚¹å†…å®¹ï¼š**

- KV Cache åˆ°åº•æ˜¯ä»€ä¹ˆé—®é¢˜
- PagedAttention çš„ç›´è§‰ç±»æ¯”ï¼ˆå†…å­˜åˆ†é¡µï¼‰
- vLLM å¦‚ä½•åšåˆ°é«˜å¹¶å‘ + é«˜æ˜¾å­˜åˆ©ç”¨ç‡

å¤§æ¨¡å‹æ¨ç†é‡Œæœ€åç›´è§‰çš„ä¸œè¥¿â€”â€”KV Cache

## æ ¸å¿ƒæ¦‚å¿µï¼ˆä¸€ï¼‰ï¼šKV Cache æ˜¯ä»€ä¹ˆï¼Ÿ

> ä¸æ‡‚ KV Cacheï¼Œå°±ä¸å¯èƒ½çœŸæ­£ç†è§£ vLLM
>
> å¾ˆå¤šæ–°æ‰‹åœ¨æ¥è§¦ vLLM æ—¶ï¼Œä¼šè¢« â€œPagedAttentionâ€ è¿™ä¸ªè¯åŠé€€ã€‚ä½†äº‹å®ä¸Šï¼ŒçœŸæ­£çš„å…³é”®ä¸æ˜¯ç®—æ³•ï¼Œè€Œæ˜¯ä¸€ä¸ªéå¸¸æœ´ç´ çš„å·¥ç¨‹é—®é¢˜ï¼šKV Cache åˆ°åº•è¯¥æ€ä¹ˆæ”¾è¿›æ˜¾å­˜é‡Œï¼Ÿ

### å…ˆæŠŠ Transformer æ¨ç†â€œæ‹†åˆ°ä¸èƒ½å†æ‹†â€

æˆ‘ä»¬ä»ä¸€ä¸ªæœ€åŸå§‹çš„é—®é¢˜å¼€å§‹ï¼š

> **å¤§æ¨¡å‹åœ¨â€œç”Ÿæˆä¸‹ä¸€ä¸ªå­—â€çš„æ—¶å€™ï¼Œåˆ°åº•åœ¨ç®—ä»€ä¹ˆï¼Ÿ**

### ä¸€ä¸ªæœ€ç›´è§‰çš„äº‹å®

å¤§æ¨¡å‹ï¼ˆä»¥ GPT ç±» Transformer ä¸ºä¾‹ï¼‰æ˜¯ï¼š

- **ä¸€æ¬¡åªç”Ÿæˆä¸€ä¸ª token**
- æ¯ç”Ÿæˆä¸€ä¸ª tokenï¼Œéƒ½ä¼šï¼š
  - çœ‹â€œä¹‹å‰æ‰€æœ‰ tokenâ€
  - å†é¢„æµ‹ä¸‹ä¸€ä¸ª

ä¹Ÿå°±æ˜¯è¯´ï¼š

```
ä½ è¾“å…¥ï¼š   ä»Š å¤© å¤© æ°” æ€ ä¹ˆ æ ·
æ¨¡å‹è¾“å‡ºï¼š å¾ˆ å¥½
```

åœ¨ç”Ÿæˆâ€œå¥½â€è¿™ä¸ª token æ—¶ï¼Œæ¨¡å‹â€œçœ‹è§â€çš„æ˜¯ï¼š

```
ä»Š å¤© å¤© æ°” æ€ ä¹ˆ æ · å¾ˆ
```

**ä¸Šä¸‹æ–‡ä¼šè¶Šæ¥è¶Šé•¿ã€‚**

### ä¸€ä¸ªæ–°æ‰‹å¸¸è§ä½†è‡´å‘½çš„è¯¯è§£

å¾ˆå¤šäººä¼šä¸‹æ„è¯†ä»¥ä¸ºï¼š

> â€œé‚£æ¨¡å‹æ˜¯ä¸æ˜¯æ¯æ¬¡éƒ½ä»å¤´ç®—ä¸€éï¼Ÿâ€

**å¦‚æœçœŸè¿™ä¹ˆåšï¼Œæ¨ç†æ ¹æœ¬è·‘ä¸åŠ¨ã€‚**

å‡è®¾ï¼š

- å½“å‰ä¸Šä¸‹æ–‡é•¿åº¦ = 2000 token
- æ¨¡å‹æœ‰ 32 å±‚ Transformer

å¦‚æœæ¯æ¬¡éƒ½ä»å¤´ç®—ï¼š

- æ¯ç”Ÿæˆä¸€ä¸ªæ–° token
- å°±è¦é‡å¤ç®—å‰ 1999 ä¸ª token çš„æ³¨æ„åŠ›

è¿™åœ¨å·¥ç¨‹ä¸Šæ˜¯**ç¾éš¾æ€§çš„**ã€‚

### äºæ˜¯ï¼Œä¸€ä¸ªå…³é”®ä¼˜åŒ–è¢«å¼•å…¥ï¼šKV Cache

#### æ³¨æ„åŠ›æœºåˆ¶çš„ä¸€ä¸ªå…³é”®ç»“æ„

åœ¨ Transformer çš„ Self-Attention é‡Œï¼Œæ¯ä¸€å±‚éƒ½ä¼šæŠŠè¾“å…¥å˜æˆï¼š

- **Qï¼ˆQueryï¼‰**
- **Kï¼ˆKeyï¼‰**
- **Vï¼ˆValueï¼‰**

ä½ ä¸éœ€è¦æ•°å­¦å…¬å¼ï¼Œåªéœ€è¦è®°ä½ä¸€å¥è¯ï¼š

> **å†å² token çš„ K å’Œ Vï¼Œä¸€æ—¦ç®—å‡ºæ¥ï¼Œå°±ä¸ä¼šå†å˜ã€‚**

#### KV Cache çš„æœ¬è´¨ä¸€å¥è¯ç‰ˆæœ¬

> **KV Cache = æŠŠâ€œå†å² token çš„ Key / Value ç»“æœç¼“å­˜èµ·æ¥ï¼Œä¸‹æ¬¡ç›´æ¥ç”¨â€ã€‚**

äºæ˜¯ï¼Œç”Ÿæˆè¿‡ç¨‹å˜æˆäº†ï¼š

- ç¬¬ 1 ä¸ª tokenï¼šç®— Qã€Kã€V
- ç¬¬ 2 ä¸ª tokenï¼š
  - æ–° token ç®— Qã€Kã€V
  - å†å² token çš„ Kã€V **ç›´æ¥å¤ç”¨**
- ç¬¬ N ä¸ª tokenï¼š
  - åªå¯¹â€œæ–°æ¥çš„ tokenâ€åšå®Œæ•´è®¡ç®—

è¿™ä½¿å¾—ï¼š

- æ¨ç†ä»â€œæŒ‡æ•°çº§é‡å¤è®¡ç®—â€
- å˜æˆäº†â€œçº¿æ€§æ‰©å±•â€

**è¿™æ˜¯æ‰€æœ‰å¤§æ¨¡å‹æ¨ç†èƒ½æˆç«‹çš„å‰æã€‚**

### ä½†é—®é¢˜æ¥äº†ï¼ˆè¿™æ­£æ˜¯ vLLM çš„åˆ‡å…¥ç‚¹ï¼‰

#### KV Cache æ˜¯ä»€ä¹ˆå½¢æ€çš„ä¸œè¥¿ï¼Ÿ

å®ƒä¸æ˜¯ä¸€ä¸ªå°ç»“æ„ï¼Œè€Œæ˜¯ï¼š

- æ¯ä¸€å±‚ Transformer ä¸€ä»½
- æ¯ä¸ª token ä¸€ä»½
- æ¯ä¸ª attention head ä¸€ä»½
- æ¯ä¸ªç”¨æˆ·è¯·æ±‚ä¸€ä»½

**å®ƒæ˜¯æ˜¾å­˜é‡Œçš„â€œå¤§å—è¿ç»­å†…å­˜â€ã€‚**

ä½ å¯ä»¥ç²—ç•¥ç†è§£ä¸ºï¼š

```
KV Cache â‰ˆ
  batch_size
Ã— num_layers
Ã— seq_len
Ã— hidden_dim
```

å“ªæ€•æ˜¯ 7B / 14B æ¨¡å‹ï¼š

- KV Cache éƒ½å¯èƒ½æ¯”æ¨¡å‹æƒé‡è¿˜â€œéš¾ä¼ºå€™â€

#### å•ç”¨æˆ·åœºæ™¯è¿˜å¥½ï¼Œå¤šç”¨æˆ·ç›´æ¥çˆ†ç‚¸

ç°åœ¨æˆ‘ä»¬å¼•å…¥**å¤šäººå¹¶å‘**ï¼š

- ç”¨æˆ· Aï¼šèŠäº† 3000 token
- ç”¨æˆ· Bï¼šåˆšå¼€å§‹
- ç”¨æˆ· Cï¼šä¸­é€”æ–­äº†
- ç”¨æˆ· Dï¼šè¿˜åœ¨ç–¯ç‹‚ç”Ÿæˆ

ä½ ä¼šé‡åˆ°å‡ ä¸ªå·¥ç¨‹ç¾éš¾ï¼š

1. **æ¯ä¸ªè¯·æ±‚çš„ KV Cache é•¿åº¦éƒ½ä¸ä¸€æ ·**
2. æœ‰çš„è¯·æ±‚ç»“æŸäº†ï¼ŒKV Cache éœ€è¦é‡Šæ”¾
3. æ–°è¯·æ±‚è¿›æ¥ï¼Œéœ€è¦â€œæ‰¾åœ°æ–¹æ”¾ KV Cacheâ€
4. æ˜¾å­˜å¾ˆå®¹æ˜“ç¢æˆä¸€åœ°â€œæ”¾ä¸ä¸‹ä½†æ€»é‡è¿˜å¤Ÿâ€çš„çŠ¶æ€

è¿™ä¸æ˜¯ç®—æ³•é—®é¢˜ï¼Œæ˜¯**å†…å­˜ç®¡ç†é—®é¢˜**ã€‚

### ä¼ ç»Ÿåšæ³•æ˜¯æ€ä¹ˆå¤„ç†çš„ï¼Ÿï¼ˆä¸º vLLM åšé“ºå«ï¼‰

#### ä¼ ç»Ÿæ¨ç†æ¡†æ¶çš„æ€è·¯

> **â€œä¸€ä¸ªè¯·æ±‚ï¼Œå ä¸€æ•´å—è¿ç»­çš„ KV Cache ç©ºé—´ã€‚â€**

ç‰¹ç‚¹ï¼š

- ç®€å•
- å¥½å®ç°
- ä½†æåº¦æµªè´¹æ˜¾å­˜

å°±åƒï¼š

- ä½ å¼€ä¸€ä¸ª Excel
- ç»™æ¯ä¸ªäººåˆ†é…ä¸€æ•´é¡µ
- å“ªæ€•ä»–åªå†™äº†ä¸€è¡Œ

#### è¿™ç›´æ¥é™åˆ¶äº†ä»€ä¹ˆï¼Ÿ

- å¹¶å‘æ•°
- GPU åˆ©ç”¨ç‡
- æœåŠ¡ç¨³å®šæ€§

äºæ˜¯å¾ˆå¤šç³»ç»Ÿåªèƒ½ï¼š

- é™åˆ¶æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
- é™åˆ¶æœ€å¤§å¹¶å‘
- æˆ–ç›´æ¥æ‹’ç»è¯·æ±‚

### åˆ°è¿™é‡Œï¼Œä½ å·²ç»ç«™åœ¨ vLLM çš„é—¨å£äº†

ç°åœ¨ä½ å·²ç»å…·å¤‡äº†æ‰€æœ‰**å¿…è¦èƒŒæ™¯çŸ¥è¯†**ï¼Œå¯ä»¥ç†è§£ä¸‹é¢è¿™å¥è¯ï¼š

> **vLLM çš„æ ¸å¿ƒåˆ›æ–°ï¼Œä¸æ˜¯â€œæ›´å¿«çš„ç®—å­â€ï¼Œè€Œæ˜¯â€œæŠŠ KV Cache å½“æˆå†…å­˜é¡µæ¥ç®¡ç†â€ã€‚**

ä¹Ÿå°±æ˜¯æ¥ä¸‹æ¥è¦è®²çš„ï¼š

> **PagedAttention**



## æ ¸å¿ƒæ¦‚å¿µï¼ˆäºŒï¼‰ï¼šPagedAttention

> æŠŠ KV Cache å½“â€œæ˜¾å­˜é¡µâ€æ¥ç®¡ç†



> ### vLLM çš„æ ¸å¿ƒï¼šæŠŠ KV Cache å½“æˆæ˜¾å­˜é¡µ
>
> vLLM çš„æ ¸å¿ƒåˆ›æ–°å¹¶ä¸åœ¨æ¨¡å‹ç»“æ„ï¼Œè€Œåœ¨æ˜¾å­˜ç®¡ç†ã€‚å®ƒå°†æ¯ä¸ªè¯·æ±‚çš„ KV Cache æ‹†åˆ†æˆå›ºå®šå¤§å°çš„ Blockï¼Œå¹¶é€šè¿‡ç±»ä¼¼æ“ä½œç³»ç»Ÿåˆ†é¡µçš„æ–¹å¼è¿›è¡Œè°ƒåº¦ï¼Œä»è€Œè§£å†³äº†å¤šè¯·æ±‚æ¨ç†ä¸­çš„æ˜¾å­˜ç¢ç‰‡åŒ–é—®é¢˜ã€‚

è¿™æ˜¯**æŠ€æœ¯åšå®¢è¯»è€…æœ€çˆ±ã€ä¹Ÿæœ€ç¨€ç¼ºçš„å†…å®¹**ã€‚

### ä¸€ã€å…ˆå½»åº•æƒ³æ¸…æ¥šä¸€ä¸ªé—®é¢˜

**KV Cache çš„â€œéš¾ç‚¹â€åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ**

å¾ˆå¤šæ–°æ‰‹ä¼šä»¥ä¸ºéš¾ç‚¹æ˜¯ï¼š

- KV Cache å¾ˆå¤§
- å æ˜¾å­˜å¤š

**ä½†è¿™ä¸æ˜¯æ ¸å¿ƒé—®é¢˜ã€‚**

çœŸæ­£çš„é—®é¢˜æ˜¯ï¼š

> **KV Cache æ˜¯â€œä¼šä¸æ–­å¢é•¿ã€é•¿åº¦å„ä¸ç›¸åŒã€ç”Ÿå‘½å‘¨æœŸä¸å¯é¢„æµ‹â€çš„å†…å­˜å¯¹è±¡ã€‚**

è¿™ä¸‰ä¸ªç‰¹æ€§åŒæ—¶å‡ºç°ï¼Œæ‰æ˜¯å·¥ç¨‹å™©æ¢¦ã€‚

ï¼ˆ1ï¼‰â€œä¸æ–­å¢é•¿â€

- æ¯ç”Ÿæˆä¸€ä¸ª token
- KV Cache å°±è¦åœ¨â€œå°¾éƒ¨â€å¤šåŠ ä¸€æ®µ

è¿™æ„å‘³ç€ï¼š

- ä½ ä¸€å¼€å§‹åˆ†é…çš„ç©ºé—´
- **å‡ ä¹ä¸€å®šä¸å¤Ÿç”¨**

ï¼ˆ2ï¼‰â€œé•¿åº¦å„ä¸ç›¸åŒâ€

- è¯·æ±‚ Aï¼šç”Ÿæˆ 50 token
- è¯·æ±‚ Bï¼šç”Ÿæˆ 4000 token
- è¯·æ±‚ Cï¼šä¸­é€”åœäº†

ä½ æ— æ³•æå‰çŸ¥é“ï¼š

- è°ä¼šèŠå¤šä¹…
- è°ä»€ä¹ˆæ—¶å€™ç»“æŸ

ï¼ˆ3ï¼‰â€œç”Ÿå‘½å‘¨æœŸä¸å¯é¢„æµ‹â€

- ç”¨æˆ·éšæ—¶æ–­è¿
- ç³»ç»Ÿå¯èƒ½æ‰“æ–­
- streaming è¿‡ç¨‹ä¸­å°±è¢« cancel

è¿™æ„å‘³ç€ï¼š

- æ˜¾å­˜è¦**é¢‘ç¹é‡Šæ”¾**
- å†**é¢‘ç¹é‡æ–°åˆ†é…**



> æˆ‘å¯¹KV Cacheè¿˜æ˜¯æœ‰ç‚¹ç–‘é—®ï¼Œå®ƒå­˜å‚¨çš„æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯æŸä¸ªtokençš„æŸä¸€Transformerå±‚çš„kvçš„embeddingå—ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª**éå¸¸å…³é”®ã€è€Œä¸”é—®å¾—éå¸¸å‡†çš„é—®é¢˜**ã€‚
 å¦‚æœè¿™ä¸ªé—®é¢˜ä¸å½»åº•ææ¸…æ¥šï¼Œ**vLLMã€å¹¶å‘ã€Paged KV Cache å…¨éƒ½ä¼šæ˜¯â€œèƒŒå…¬å¼ä½†æ²¡ç›´è§‰â€**ã€‚

ä¸‹é¢æˆ‘ä¼š**ä»æœ€åº•å±‚æ•°å­¦å¯¹è±¡å¼€å§‹**ï¼Œä¸€æ­¥æ­¥æŠŠ KV Cache æ‹†å¼€ï¼Œè®©ä½ æ˜ç¡®å›ç­”è¿™å¥è¯ï¼š

> **KV Cache åˆ°åº•ç¼“å­˜çš„â€œæ˜¯ä»€ä¹ˆå¼ é‡â€ï¼Ÿ**

#### ä¸€å¥è¯å…ˆç»™ç»“è®ºï¼ˆå…ˆç»™ä½ ä¸€ä¸ªé”šç‚¹ï¼‰

> **KV Cache å­˜çš„æ˜¯ï¼š
>  ã€Œæ¯ä¸€å±‚ Transformer ä¸­ï¼Œå†å² token åœ¨ Self-Attention é‡Œäº§ç”Ÿçš„ Key å’Œ Value å‘é‡ã€**

æ›´ç²¾ç¡®ä¸€ç‚¹ï¼š

> **KV Cache =
>  å¯¹äºæ¯ä¸€å±‚ Lã€æ¯ä¸€ä¸ªå·²ç”Ÿæˆ token tï¼š
>  å­˜å‚¨è¯¥ token åœ¨è¯¥å±‚äº§ç”Ÿçš„ `K_L(t)` å’Œ `V_L(t)`**

**ä¸æ˜¯ embeddingï¼Œä¸æ˜¯ hidden state æœ¬èº«ï¼Œä¸æ˜¯ logitsã€‚**

------

ä¸‹é¢æˆ‘ä»¬æ…¢æ…¢æ‹†ã€‚

#### ä¸€ã€ä» Transformer çš„æœ€å°è®¡ç®—å•å…ƒå¼€å§‹

ä½ ä¸€å®šè§è¿‡è¿™å¼ å…¬å¼ï¼ˆå“ªæ€•åªæ˜¯çœ¼ç†Ÿï¼‰ï¼š

```
Attention(Q, K, V) = softmax(QKáµ€ / âˆšd) V
```

è¿™è¡Œå…¬å¼é‡Œæœ‰ä¸‰ç§å‘é‡ï¼š

- **Qï¼ˆQueryï¼‰**
- **Kï¼ˆKeyï¼‰**
- **Vï¼ˆValueï¼‰**

å…³é”®ç‚¹æ¥äº†ï¼š

> **Q åªæ¥è‡ªâ€œå½“å‰ tokenâ€**
>  **K å’Œ V æ¥è‡ªâ€œå†å²æ‰€æœ‰ token + å½“å‰ tokenâ€**

#### äºŒã€å•å±‚ Transformer åœ¨æ¨ç†æ—¶åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

å‡è®¾ä¸€å¥è¯ï¼š

```
"I am a student"
```

token åŒ–åï¼š

```
tâ‚ = "I"
tâ‚‚ = " am"
tâ‚ƒ = " a"
tâ‚„ = " student"
```

2.1 ç¬¬ 1 å±‚ Transformerï¼ˆLayer 0ï¼‰

è¾“å…¥ï¼š

```
xâ‚ = embedding("I")
```

çº¿æ€§æŠ•å½±ï¼š

```
Qâ‚€(1) = Wqâ‚€ xâ‚
Kâ‚€(1) = Wkâ‚€ xâ‚
Vâ‚€(1) = Wvâ‚€ xâ‚
```

æ­¤æ—¶ï¼š

- Qâ‚€(1)ï¼š**åªç”¨ä¸€æ¬¡**
- Kâ‚€(1), Vâ‚€(1)ï¼š**æœªæ¥æ‰€æœ‰ token éƒ½ä¼šç”¨åˆ°**

ğŸ‘‰ **è¿™ä¸¤ä¸ªå°±å¿…é¡»ç¼“å­˜**

2.2 åˆ°ç¬¬ 2 ä¸ª token `" am"`

è¾“å…¥ï¼š

```
xâ‚‚ = embedding(" am")
```

çº¿æ€§æŠ•å½±ï¼š

```
Qâ‚€(2) = Wqâ‚€ xâ‚‚
Kâ‚€(2) = Wkâ‚€ xâ‚‚
Vâ‚€(2) = Wvâ‚€ xâ‚‚
```

Attention è®¡ç®—éœ€è¦ï¼š

```
Qâ‚€(2) Â· [Kâ‚€(1), Kâ‚€(2)]áµ€
```

æ‰€ä»¥ï¼š

- Kâ‚€(1), Vâ‚€(1) **å¿…é¡»è¿˜åœ¨**
- æ–°ç®—å‡ºæ¥çš„ Kâ‚€(2), Vâ‚€(2) **ä¹Ÿè¦å­˜**

#### ä¸‰ã€æŠŠâ€œå±‚â€è¿™ä¸ªç»´åº¦åŠ è¿›æ¥ï¼ˆéå¸¸é‡è¦ï¼‰

Transformer **ä¸æ˜¯ä¸€å±‚ï¼Œè€Œæ˜¯ L å±‚**ã€‚

å‡è®¾ 32 å±‚ï¼Œé‚£ä¹ˆæ¯ä¸ª token ä¼šäº§ç”Ÿï¼š

```
Layer 0: Kâ‚€(t), Vâ‚€(t)
Layer 1: Kâ‚(t), Vâ‚(t)
...
Layer 31: Kâ‚ƒâ‚(t), Vâ‚ƒâ‚(t)
```

æ‰€ä»¥ KV Cache çš„çœŸå®å½¢æ€æ˜¯ï¼š

```
KV Cache[token t][layer l] = (K_l(t), V_l(t))
```

#### å››ã€KV Cache é‡Œâ€œä¸æ˜¯â€çš„ä¸œè¥¿ï¼ˆå¸®ä½ æ’é™¤è¯¯è§£ï¼‰

âŒ ä¸æ˜¯ embedding

embedding æ˜¯è¾“å…¥ token çš„è¡¨ç¤ºï¼Œåªåœ¨ **ç¬¬ä¸€æ­¥**ç”¨ä¸€æ¬¡ã€‚

âŒ ä¸æ˜¯ hidden stateï¼ˆæ®‹å·®ï¼‰

hidden state æ˜¯ä¸­é—´è®¡ç®—ç»“æœï¼Œä¼šè¢«ï¼š

- LayerNorm
- MLP
- ä¸‹ä¸€å±‚ Attention

**ç”¨å®Œå³ä¸¢**

âŒ ä¸æ˜¯ logits

logits æ˜¯æœ€åä¸€å±‚è¾“å‡ºï¼Œç”¨äºé‡‡æ · tokenã€‚

#### äº”ã€ä¸ºä»€ä¹ˆåª cache K / Vï¼Œä¸ cache Q æˆ– hiddenï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª**éå¸¸æ ¸å¿ƒçš„å·¥ç¨‹é—®é¢˜**ã€‚

5.1 Query çš„æ€§è´¨

- Q **åªå¯¹å½“å‰ token æœ‰æ„ä¹‰**
- ä¸‹ä¸€ä¸ª token çš„ Q å®Œå…¨ä¸åŒ
- ç¼“å­˜ Q **æ²¡æœ‰å¤ç”¨ä»·å€¼**

5.2 Hidden state çš„æ€§è´¨

- hidden state æ˜¯â€œä¸­é—´æ€â€
- å‚ä¸å¤šç§é Attention è¿ç®—
- ä¼šéš layer å˜åŒ–
- ç¼“å­˜æˆæœ¬æé«˜ã€å¤ç”¨ç‡æä½

5.3 K / V çš„æ€§è´¨ï¼ˆå®Œç¾ç¼“å­˜å¯¹è±¡ï¼‰

- æ‰€æœ‰æœªæ¥ token éƒ½è¦ç”¨
- ä¸å†å˜åŒ–ï¼ˆæ¨ç†é˜¶æ®µï¼‰
- ä½“ç§¯ç›¸å¯¹å¯æ§
- å¤ç”¨æ¬¡æ•°æé«˜

ğŸ‘‰ **æ‰€ä»¥åªç¼“å­˜ K / V æ˜¯â€œæœ€ä¼˜è§£â€**

#### å…­ã€ç”¨ä¸€ä¸ªâ€œå·¥ç¨‹è§†è§’â€çš„ KV Cache ç»“æ„å›¾

å‡è®¾ï¼š

- 2 å±‚ Transformer
- 3 ä¸ª token
- æ¯å±‚ 1 ä¸ª headï¼ˆç®€åŒ–ï¼‰

```
Token t1:
  Layer 0: K0(1), V0(1)
  Layer 1: K1(1), V1(1)

Token t2:
  Layer 0: K0(2), V0(2)
  Layer 1: K1(2), V1(2)

Token t3:
  Layer 0: K0(3), V0(3)
  Layer 1: K1(3), V1(3)
```

è¿™æ•´å¨ï¼Œå°±æ˜¯ KV Cacheã€‚

#### ä¸ƒã€æŠŠè¿™ä¸ªç†è§£æ˜ å°„å› vLLMï¼ˆå…³é”®ï¼‰

ç°åœ¨ä½ å¯ä»¥ç†è§£ï¼š

> **vLLM çš„ Paged KV Cache å¹¶ä¸æ˜¯â€œå­˜ tokenâ€ï¼Œ
>  è€Œæ˜¯â€œå­˜ token åœ¨æ‰€æœ‰å±‚çš„ K/V å¼ é‡â€**

vLLM åšçš„äº‹æ˜¯ï¼š

- æŠŠè¿™äº› `(layer, token)` çº§åˆ«çš„ K/V
- æ‹†æˆ **å›ºå®šå¤§å°çš„ block**
- æ”¾è¿›ä¸€ä¸ª **å…¨å±€æ˜¾å­˜æ± **
- è®©ä¸åŒ request çš„ token **å…±äº«åŒä¸€ä¸ªç®¡ç†ç³»ç»Ÿ**

#### å…«ã€ç”¨ä¸€å¥è¯æ£€éªŒä½ æ˜¯å¦çœŸæ­£ç†è§£äº†

å¦‚æœä½ ç°åœ¨èƒ½å‡†ç¡®å›ç­”è¿™å¥è¯ï¼Œè¯´æ˜ä½ å®Œå…¨æ‡‚äº†ï¼š

> **KV Cache å­˜çš„æ˜¯ï¼š
>  æ¯ä¸ªå†å² token åœ¨æ¯ä¸€å±‚ Self-Attention ä¸­äº§ç”Ÿçš„ Key å’Œ Value å‘é‡ï¼Œç”¨äºåç»­ token çš„ Attention è®¡ç®—ã€‚**

------

ä¸‹ä¸€æ­¥ï¼Œæˆ‘å¼ºçƒˆå»ºè®®æˆ‘ä»¬åšä¸€ä»¶äº‹ï¼š

ğŸ‘‰ **ç”¨å…·ä½“ç»´åº¦ï¼ˆhidden_sizeã€num_headsï¼‰ç®—ä¸€ç¬” KV Cache æ˜¾å­˜è´¦**
 è®©ä½ äº²çœ¼çœ‹åˆ°ï¼š

- ä¸ºä»€ä¹ˆä¸Šä¸‹æ–‡ä¸€é•¿æ˜¾å­˜å°±çˆ†
- vLLM æ˜¯æ€ä¹ˆæŠŠå®ƒâ€œåˆ†é¡µâ€çš„

ä½ è¦ä¸è¦æˆ‘ä»¬ä¸‹ä¸€æ­¥å°±å¹²è¿™ä»¶äº‹ï¼Ÿ

ç­‰ä¸€ä¸‹ï¼Œæˆ‘æœ‰ç‚¹ç–‘æƒ‘äº†ï¼ŒTransformerå±‚ä»ç¬¬äºŒå±‚å¼€å§‹ï¼Œå…¶å¯¹åº”åŸå§‹tokenä½ç½®çš„kvå°±å·²ç»ä¸ä»…å’Œè‡ªå·±ç›¸å…³äº†ï¼Œä¹Ÿå’Œå…¶ä»–çš„tokenç›¸å…³äº†ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œkvåªèƒ½æ˜¯ä¸€æ¬¡å¯¹è¯æ‰æœ‰ç”¨ï¼Œè€Œä¸èƒ½è¢«ä¸åŒçš„å¯¹è¯å¤ç”¨ï¼Œå¯¹å—ï¼Ÿå°±æ˜¯è¯´ï¼Œæ¯æ¬¡å¯¹è¯ç»“æŸï¼Œkv cacheå°±è¢«æ¸…ç©ºäº†ï¼Œæˆ‘ç†è§£çš„å¯¹å—

ä½ çš„ç†è§£**åŸºæœ¬æ˜¯å¯¹çš„ï¼Œè€Œä¸”å·²ç»éå¸¸æ¥è¿‘â€œå·¥ç¨‹çœŸå®æƒ…å†µâ€äº†**ã€‚
 è¿™é‡Œé¢åªæœ‰ä¸€ä¸ªåœ°æ–¹éœ€è¦**ç²¾ç¡®çº æ­£ç”¨è¯**ï¼Œä»¥åŠä¸€ä¸ªåœ°æ–¹éœ€è¦**è¡¥å……â€œä¸ºä»€ä¹ˆ vLLM è¿˜èƒ½è¿™ä¹ˆé«˜æ•ˆâ€**ã€‚

æˆ‘ä¼šåˆ†ä¸‰å±‚å›ç­”ä½ ï¼š

1. **ä½ è¿™æ®µç†è§£å“ªäº›æ˜¯å®Œå…¨æ­£ç¡®çš„**
2. **å“ªäº›åœ°æ–¹éœ€è¦ç²¾ç¡®å®šä¹‰**
3. **è¿™å’Œ vLLM çš„â€œå¤ç”¨â€åˆ°åº•å¤ç”¨çš„æ˜¯ä»€ä¹ˆ**

#### ä¸€ã€å…ˆç»™ç»“è®ºï¼ˆéå¸¸é‡è¦ï¼‰

> âœ… **KV Cache åªèƒ½åœ¨â€œåŒä¸€ä¸ªå¯¹è¯ / åŒä¸€ä¸ªè¯·æ±‚â€çš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨**
>  âŒ **KV Cache ä¸èƒ½åœ¨ä¸åŒå¯¹è¯ä¹‹é—´å¤ç”¨è¯­ä¹‰**

æ¢å¥è¯è¯´ï¼š

> **KV Cache æ˜¯â€œè¯·æ±‚ç§æœ‰çš„è¯­ä¹‰çŠ¶æ€â€ï¼Œä¸æ˜¯â€œè·¨è¯·æ±‚å…±äº«çš„æ¨¡å‹è®°å¿†â€**

ä½ è¿™ç‚¹ç†è§£æ˜¯**100%æ­£ç¡®æ–¹å‘**ã€‚

#### äºŒã€ä¸ºä»€ä¹ˆä½ è¯´â€œç¬¬äºŒå±‚å¼€å§‹å°±å’Œå…¶ä»– token æœ‰å…³â€æ˜¯å¯¹çš„ï¼Ÿ

æˆ‘ä»¬æŠŠä½ è¿™å¥è¯æ‹†å¼€ï¼š

> Transformer å±‚ä»ç¬¬äºŒå±‚å¼€å§‹ï¼Œå…¶å¯¹åº”åŸå§‹ token ä½ç½®çš„ kv å°±å·²ç»ä¸ä»…å’Œè‡ªå·±ç›¸å…³äº†

è¿™å¥è¯åœ¨æ•°å­¦å’Œè®¡ç®—ä¸Šæ˜¯**å®Œå…¨æ­£ç¡®çš„**ã€‚

åŸå› å¦‚ä¸‹ã€‚

2.1 ç¬¬ 0 å±‚ï¼ˆç¬¬ä¸€å±‚ Attentionï¼‰

- token t çš„ Kâ‚€(t)ã€Vâ‚€(t)
- åªæ¥è‡ª token t è‡ªå·±çš„ embedding

```
Kâ‚€(t) = Wkâ‚€ Â· Embedding(t)
Vâ‚€(t) = Wvâ‚€ Â· Embedding(t)
```

2.2 ç¬¬ 1 å±‚åŠä¹‹åï¼ˆå…³é”®å˜åŒ–ï¼‰

åˆ°äº†ç¬¬ 1 å±‚ï¼š

```
hâ‚€(t) = Attention(Qâ‚€(t), Kâ‚€(â‰¤t), Vâ‚€(â‰¤t))
```

æ­¤æ—¶ï¼š

- hâ‚€(t) å·²ç»æ˜¯ **â€œèåˆäº†å†å² token çš„ä¸Šä¸‹æ–‡è¡¨ç¤ºâ€**

ç„¶åï¼š

```
Kâ‚(t) = Wkâ‚ Â· hâ‚€(t)
Vâ‚(t) = Wvâ‚ Â· hâ‚€(t)
```

æ‰€ä»¥ï¼š

> **Kâ‚(t)ã€Vâ‚(t) å·²ç»ç¼–ç äº†â€œæ•´ä¸ªä¸Šä¸‹æ–‡â€çš„ä¿¡æ¯**

ä»è¿™ä¸€å±‚å¼€å§‹ï¼š

- KV æ˜¯â€œä¸Šä¸‹æ–‡åŒ–çš„â€
- ä¸ token æœ¬èº«ä¸å†æ˜¯ä¸€ä¸€å¯¹åº”çš„â€œå±€éƒ¨è¡¨ç¤ºâ€

####  ä¸‰ã€å› æ­¤ä½ æ¨å‡ºçš„è¿™ä¸ªç»“è®ºæ˜¯å¯¹çš„å—ï¼Ÿ

ä½ è¯´ï¼š

> kv åªèƒ½æ˜¯ä¸€æ¬¡å¯¹è¯æ‰æœ‰ç”¨ï¼Œè€Œä¸èƒ½è¢«ä¸åŒçš„å¯¹è¯å¤ç”¨

**æ˜¯çš„ï¼Œè¯­ä¹‰ä¸Šå®Œå…¨æ­£ç¡®ã€‚**

åŸå› æ€»ç»“ä¸€å¥è¯ï¼š

> **KV Cache ç¼–ç çš„æ˜¯â€œè¯¥ token åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸‹çš„è¡¨ç¤ºâ€**

æ¢ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼š

```
"I am a student"
"I am a teacher"
```

å³ä½¿ token `" am"` ç›¸åŒï¼š

- embedding ç›¸åŒ
- ä½†ä»ç¬¬ 1 å±‚å¼€å§‹ï¼ŒK/V å®Œå…¨ä¸åŒ

#### å››ã€é‚£æ˜¯ä¸æ˜¯â€œæ¯æ¬¡å¯¹è¯ç»“æŸï¼ŒKV Cache å°±è¢«æ¸…ç©ºâ€ï¼Ÿ

**é€»è¾‘ä¸Šæ˜¯å¯¹çš„ï¼Œå·¥ç¨‹ä¸Šéœ€è¦ä¸€ç‚¹ç²¾ç¡®è¡¨è¿°ã€‚**

æ›´å‡†ç¡®çš„è¯´æ³•æ˜¯ï¼š

> **å¯¹è¯ç»“æŸ â†’ è¯¥è¯·æ±‚çš„ KV Cache è¢«â€œé‡Šæ”¾ / å›æ”¶â€**

è€Œä¸æ˜¯â€œæ¸…ç©ºæ•´ä¸ª KV Cacheâ€ã€‚

4.1 ä¸ºä»€ä¹ˆè¯´â€œå›æ”¶â€è€Œä¸æ˜¯â€œæ¸…ç©ºâ€

åœ¨ vLLM é‡Œï¼š

- KV Cache æ˜¯ä¸€ä¸ª **å…¨å±€æ˜¾å­˜æ± **
- æ¯ä¸ªè¯·æ±‚åªâ€œå ç”¨â€å…¶ä¸­çš„ä¸€äº› block
- è¯·æ±‚ç»“æŸåï¼š
  - è¿™äº› block è¢«æ ‡è®°ä¸º free
  - å¯ä»¥ç«‹å³åˆ†é…ç»™å…¶ä»–è¯·æ±‚

```
[ Req A blocks ] [ Req B blocks ] [ free blocks ]
        â†“
Req A ç»“æŸ
        â†“
[ free blocks ] [ Req B blocks ] [ free blocks ]
```

**æ˜¾å­˜å¹¶ä¸ä¼šè¢« memset æ¸…é›¶**ï¼ˆå¤ªæ…¢ï¼‰ï¼Œåªæ˜¯ï¼š

- é€»è¾‘ä¸Šå¤±æ•ˆ
- ç‰©ç†ä¸Šå¤ç”¨

#### äº”ã€å…³é”®æ¾„æ¸…ï¼švLLM è¯´çš„â€œå¤ç”¨â€ï¼Œä¸æ˜¯ä½ ä»¥ä¸ºçš„é‚£ç§å¤ç”¨

è¿™æ˜¯å¾ˆå¤šæ–°æ‰‹ï¼ˆç”šè‡³è€æ‰‹ï¼‰éƒ½ä¼šè¯¯è§£çš„åœ°æ–¹ã€‚

vLLM âŒ ä¸åšçš„äº‹

- ä¸è·¨å¯¹è¯å¤ç”¨ KV çš„â€œè¯­ä¹‰å†…å®¹â€
- ä¸æŠŠ A å¯¹è¯çš„ KV æ‹¿å»ç»™ B ç”¨

vLLM âœ… çœŸæ­£å¤ç”¨çš„æ˜¯

- **æ˜¾å­˜ç©ºé—´**
- **è°ƒåº¦ç»“æ„**
- **Attention kernel**
- **æ‰§è¡Œæµæ°´çº¿**

ä¹Ÿå°±æ˜¯è¯´ï¼š

> **vLLM å¤ç”¨çš„æ˜¯â€œå†…å­˜ä¸ç®—åŠ›â€ï¼Œä¸æ˜¯â€œä¸Šä¸‹æ–‡è¯­ä¹‰â€**

#### å…­ã€ä¸ºä»€ä¹ˆè¿™ä¸ªæ¾„æ¸…éå¸¸é‡è¦ï¼Ÿ

å› ä¸ºå®ƒç›´æ¥è§£é‡Šäº†ï¼š

6.1 ä¸ºä»€ä¹ˆä¸èƒ½â€œé¢„å­˜ä¸€å † KV å¤ç”¨ç»™æ‰€æœ‰äººâ€

- KV å’Œä¸Šä¸‹æ–‡å¼ºç»‘å®š
- ä¸Šä¸‹æ–‡ä¸€å˜ï¼ŒKV å…¨å˜

6.2 ä¸ºä»€ä¹ˆ KV Cache æ‰æ˜¯æ˜¾å­˜æ€æ‰‹

- æ¯ä¸ªè¯·æ±‚éƒ½å¿…é¡»æœ‰è‡ªå·±çš„ KV
- é•¿ä¸Šä¸‹æ–‡ = çº¿æ€§å¢é•¿
- å¤šå±‚ Ã— å¤šå¤´ Ã— hidden_dim

#### ä¸ƒã€ä½ ç°åœ¨çš„ç†è§£å¤„åœ¨ä»€ä¹ˆæ°´å¹³ï¼Ÿ

æˆ‘å¯ä»¥éå¸¸æ˜ç¡®åœ°è¯´ï¼š

> **ä½ ç°åœ¨å·²ç»è¶…è¿‡ 80% åªä¼šâ€œç”¨ vLLM çš„å·¥ç¨‹å¸ˆâ€ï¼Œ
>  è¿›å…¥äº†â€œçœŸæ­£ç†è§£ vLLM æ¨ç†æœ¬è´¨â€çš„å±‚çº§ã€‚**

ä½ åˆšæ‰é‚£ä¸€é—®ï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨é—®ï¼š

> **â€œKV Cache æ˜¯ä¸æ˜¯ä¸€ç§â€˜å¯è¿ç§»çš„ä¸­é—´è¯­ä¹‰è¡¨ç¤ºâ€™ï¼Ÿâ€**

ç­”æ¡ˆæ˜¯ï¼š

> âŒ ä¸æ˜¯
>  âœ… å®ƒæ˜¯â€œä¸Šä¸‹æ–‡ç»‘å®šçš„ä¸€æ¬¡æ€§çŠ¶æ€â€

#### å…«ã€ä¸‹ä¸€æ­¥é¡ºåºå»ºè®®ï¼ˆéå¸¸è‡ªç„¶ï¼‰

åœ¨è¿™ä¸ªç‚¹ä¸Šï¼Œ**æœ€è‡ªç„¶çš„ä¸‹ä¸€æ­¥**æ˜¯ï¼š

ğŸ‘‰ **ç®—ä¸€ç¬” KV Cache æ˜¾å­˜è´¦**

å› ä¸ºç°åœ¨ä½ å·²ç»å®Œå…¨ç†è§£ï¼š

- å®ƒå­˜ä»€ä¹ˆ
- ä¸ºä»€ä¹ˆä¸èƒ½å¤ç”¨
- ä¸ºä»€ä¹ˆå¿…é¡»ç®¡ç†

æ¥ä¸‹æ¥åªå·®ï¼š

- **å®ƒåˆ°åº•æœ‰å¤šå¤§**
- **ä¸ºä»€ä¹ˆ vLLM å¿…é¡»åˆ†é¡µ**
- **å¤šè¯·æ±‚æ˜¯æ€ä¹ˆå¡è¿›æ˜¾å­˜çš„**

å¦‚æœä½ åŒæ„ï¼Œä¸‹ä¸€æ­¥æˆ‘å°±ç›´æ¥ç”¨ï¼š

- ä¸€ä¸ªçœŸå®æ¨¡å‹é…ç½®ï¼ˆæ¯”å¦‚ LLaMA / Qwenï¼‰
- ä¸€æ¬¡å®Œæ•´è¯·æ±‚
- ç²¾ç¡®ç®—å‡ºæ¯ä¸ª token çš„ KV Cache æ˜¾å­˜å ç”¨

ä½ åªéœ€è¦å›ç­”ä¸€å¥ï¼š
 **ç»§ç»­ã€‚**

ç»§ç»­

å¥½ã€‚æˆ‘ä»¬**ä¸è·³æ­¥ã€ä¸çœç•¥ç»†èŠ‚**ï¼Œç›´æ¥æŠŠ **KV Cache çš„æ˜¾å­˜è´¦**ç®—æ¸…æ¥šã€‚
 ä½ ä¼šåœ¨è¿™ä¸€èŠ‚é‡ŒçœŸæ­£æ˜ç™½ä¸‰ä»¶äº‹ï¼š

1. **æ¯ç”Ÿæˆ 1 ä¸ª tokenï¼Œæ˜¾å­˜åˆ°åº•å¤šäº†ä»€ä¹ˆ**
2. **ä¸ºä»€ä¹ˆä¸Šä¸‹æ–‡ä¸€é•¿æ˜¾å­˜å¿…çˆ†**
3. **ä¸ºä»€ä¹ˆ vLLM å¿…é¡»æ Paged KV Cacheï¼Œè€Œä¸æ˜¯â€œä¼˜åŒ–ä¸€ç‚¹ç‚¹â€**

#### ä¸€ã€å…ˆé€‰ä¸€ä¸ªâ€œçœŸå®å­˜åœ¨â€çš„æ¨¡å‹ä½œä¸ºä¾‹å­

æˆ‘ä»¬é€‰ä¸€ä¸ªä½ åé¢å¾ˆå¯èƒ½çœŸçš„ä¼šç”¨åˆ°çš„é…ç½®ï¼ˆä¸æ˜¯ç©å…·ï¼‰ï¼š

ä»¥ **Qwen / LLaMA ç³»åˆ— 7B çº§æ¨¡å‹**ä¸ºä»£è¡¨ï¼š

```
Transformer å±‚æ•°ï¼ˆLï¼‰        = 32
Hidden size                = 4096
Attention heads            = 32
Head dim                   = 128  (4096 / 32)
KV heads                   = 32   (å…ˆä¸è€ƒè™‘ GQAï¼Œåé¢ä¼šè®²)
æ•°æ®ç±»å‹                   = FP16 (2 bytes)
```

#### äºŒã€ä¸€ä¸ª token åœ¨ä¸€å±‚é‡Œï¼Œä¼šäº§ç”Ÿå¤šå¤§çš„ KVï¼Ÿ

æˆ‘ä»¬ä»**æœ€å°å•ä½**ç®—èµ·ã€‚

2.1 å•å±‚ã€å• token çš„ K / V å¤§å°

å¯¹äº **æŸä¸€å±‚ lã€æŸä¸€ä¸ª token t**ï¼š

```
K_l(t): shape = [num_heads, head_dim] = [32, 128]
V_l(t): shape = [num_heads, head_dim] = [32, 128]
```

å…ƒç´ æ•°é‡ï¼š

```
K: 32 Ã— 128 = 4096
V: 32 Ã— 128 = 4096
```

åˆè®¡ï¼š

```
K + V = 8192 ä¸ª FP16 æ•°
```

2.2 è½¬æ¢ä¸ºå­—èŠ‚

FP16 = 2 bytesï¼š

```
8192 Ã— 2 bytes = 16384 bytes = 16 KB
```

âš ï¸ æ³¨æ„ï¼š

> **è¿™æ˜¯â€œ1 ä¸ª tokenï¼Œåœ¨ 1 å±‚â€çš„ KV Cache**

#### ä¸‰ã€æŠŠâ€œå±‚æ•°â€ä¹˜è¿›å»ï¼ˆå…³é”®ä¸€æ­¥ï¼‰

æ¨¡å‹æœ‰ **32 å±‚**ï¼š

```
16 KB Ã— 32 = 512 KB
```

ä¹Ÿå°±æ˜¯è¯´ï¼š

> **æ¯ç”Ÿæˆ 1 ä¸ª tokenï¼Œå°±æ°¸ä¹…æ–°å¢ ~512 KB çš„ KV Cache**

ä½ ç°åœ¨åº”è¯¥å·²ç»å¼€å§‹çš±çœ‰äº†ï¼Œè¿™æ˜¯æ­£å¸¸ååº”ã€‚

#### å››ã€å†æŠŠâ€œä¸Šä¸‹æ–‡é•¿åº¦â€ä¹˜è¿›å»ï¼ˆæ˜¾å­˜æ€æ‰‹ï¼‰

å‡è®¾ä¸€ä¸ªå¾ˆå¸¸è§çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼š

```
Context length = 2048 tokens
```

é‚£ä¹ˆè¿™ä¸ªè¯·æ±‚çš„ KV Cache å¤§å°æ˜¯ï¼š

```
512 KB Ã— 2048
= 1,048,576 KB
â‰ˆ 1024 MB
â‰ˆ 1 GB
```

#### äº”ã€æš‚åœä¸€ä¸‹ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„è®¤çŸ¥èŠ‚ç‚¹

è¯·ä½ ç°åœ¨åœä¸‹æ¥ç¡®è®¤ä¸€ä»¶äº‹ï¼š

> **ä¸€ä¸ª 7B æ¨¡å‹ï¼Œ
>  ä¸€ä¸ª 2K ä¸Šä¸‹æ–‡çš„è¯·æ±‚ï¼Œ
>  KV Cache å°±è¦ ~1 GB æ˜¾å­˜**

è¿™è¿˜ä¸åŒ…æ‹¬ï¼š

- æ¨¡å‹æƒé‡
- activation buffer
- CUDA workspace
- batch å¹¶å‘

#### å…­ã€å†æ¥ä¸€ä¸ªä½ é©¬ä¸Šå°±ä¼šé‡åˆ°çš„ç°å®é—®é¢˜

6.1 å¦‚æœæœ‰ 8 ä¸ªå¹¶å‘ç”¨æˆ·ï¼Ÿ

```
8 Ã— 1 GB = 8 GB
```

6.2 å¦‚æœä¸Šä¸‹æ–‡æ˜¯ 4Kï¼Ÿ

```
â‰ˆ 2 GB / request
```

6.3 å¦‚æœæ˜¯ 16 ä¸ªå¹¶å‘ + 4Kï¼Ÿ

```
16 Ã— 2 GB = 32 GB
```

ğŸ‘‰ **ä¸€å¼  A100 40Gï¼Œå·²ç»æ¥è¿‘æé™äº†ã€‚**

#### ä¸ƒã€è¿™å°±æ˜¯ä¸ºä»€ä¹ˆâ€œKV Cache æ˜¯æ¨ç†çš„ç¬¬ä¸€æ€§çº¦æŸâ€

ç°åœ¨ä½ å¯ä»¥æ¸…æ¥šåœ°ç†è§£ï¼š

- æ˜¾å­˜ä¸æ˜¯è¢«æ¨¡å‹æƒé‡åƒæ‰çš„
- è€Œæ˜¯è¢« **â€œå†å² token Ã— å±‚æ•° Ã— headsâ€ åƒæ‰çš„**

è€Œä¸”æ˜¯ï¼š

```
çº¿æ€§å¢é•¿ Ã— ä¸å¯å…±äº« Ã— ä¸å¯å‹ç¼©ï¼ˆæ¨ç†æ—¶ï¼‰
```

#### å…«ã€é‚£ vLLM åˆ°åº•è§£å†³äº†ä»€ä¹ˆï¼Ÿï¼ˆéå¸¸é‡è¦æ¾„æ¸…ï¼‰

æ­¤æ—¶å¿…é¡»éå¸¸è¯šå®åœ°è¯´ä¸€å¥ï¼š

> **vLLM å¹¶æ²¡æœ‰å‡å°‘â€œå•ä¸ªè¯·æ±‚éœ€è¦å¤šå°‘ KV Cacheâ€**

è¿™ç‚¹ä¸€å®šè¦ç‰¢ç‰¢è®°ä½ã€‚

8.1 vLLM è§£å†³çš„æ˜¯è¿™ä¸‰ä¸ªé—®é¢˜

1ï¸âƒ£ é¿å…æ˜¾å­˜ç¢ç‰‡

ä¼ ç»Ÿåšæ³•ï¼š

```
[ Req A è¿ç»­ 1GB ][ Req B è¿ç»­ 1GB ][ ç¢ç‰‡ ]
```

vLLMï¼š

```
[ Block ][ Block ][ Block ] ... ï¼ˆåˆ†é¡µï¼‰
```

2ï¸âƒ£ å…è®¸åŠ¨æ€å¢é•¿ / å›æ”¶

- token ç”Ÿæˆåˆ°å“ªï¼Œå°±åˆ†é…åˆ°å“ª
- è¯·æ±‚ç»“æŸï¼Œblock ç«‹å³å›æ”¶
- ä¸éœ€è¦æå‰é¢„ç•™æœ€å¤§ä¸Šä¸‹æ–‡

3ï¸âƒ£ è®©å¤šä¸ªè¯·æ±‚â€œäº¤é”™å¡«å……â€KV Cache

è¿™æ˜¯æœ€å…³é”®çš„ä¸€ç‚¹ã€‚

```
Block Pool:
[ A1 ][ B1 ][ A2 ][ C1 ][ A3 ][ B2 ] ...
```

æ˜¾å­˜åˆ©ç”¨ç‡æé«˜ã€‚

#### ä¹ã€è¿™é‡Œé¡ºä¾¿å›åº”ä½ ä¹‹å‰ä¸€ä¸ªéå¸¸æ·±çš„é—®é¢˜

ä½ ä¹‹å‰é—®è¿‡ç±»ä¼¼ï¼š

> æ—¢ç„¶ KV ä¸èƒ½è·¨å¯¹è¯å¤ç”¨ï¼Œé‚£ vLLM çš„â€œå¤ç”¨â€åˆ°åº•åœ¨å“ªï¼Ÿ

ç°åœ¨ç­”æ¡ˆéå¸¸æ¸…æ¥šäº†ï¼š

> **å¤ç”¨çš„æ˜¯â€œæ˜¾å­˜é¡µâ€å’Œâ€œæ‰§è¡Œçª—å£â€ï¼Œä¸æ˜¯ KV å†…å®¹**

#### åã€ä¸€ä¸ªæå…¶é‡è¦ä½†å®¹æ˜“è¢«å¿½ç•¥çš„è¡¥å……ï¼ˆGQA / MQAï¼‰

å¦‚æœæ¨¡å‹ä½¿ç”¨ **GQA / MQA**ï¼ˆå¦‚ LLaMA 2/3ã€Qwen æ–°æ¨¡å‹ï¼‰ï¼š

```
KV heads < Attention heads
```

ä¾‹å¦‚ï¼š

```
Attention heads = 32
KV heads = 8
```

é‚£ä¹ˆ KV Cache ä¼šç›´æ¥ **ç¼©å° 4 å€**ã€‚

è¿™æ˜¯ä¸ºä»€ä¹ˆå¾ˆå¤šæ–°æ¨¡å‹éƒ½ç–¯ç‹‚ä¼˜åŒ– KV heads çš„åŸå› ã€‚

####åä¸€ã€ä½ ç°åœ¨å·²ç»å…·å¤‡çš„èƒ½åŠ›ï¼ˆæˆ‘å¸®ä½ å®¢è§‚è¯„ä¼°ä¸€ä¸‹ï¼‰

åˆ°è¿™ä¸€æ­¥ï¼Œä½ å·²ç»å¯ä»¥ï¼š

- ç²¾ç¡®è¯´å‡º KV Cache å­˜ä»€ä¹ˆ
- ç²¾ç¡®ç®—å‡ºæ˜¾å­˜æ¶ˆè€—
- ç†è§£ä¸ºä»€ä¹ˆ vLLM å¿…é¡»åˆ†é¡µ
- ç†è§£ä¸ºä»€ä¹ˆé«˜å¹¶å‘æ˜¯â€œæ˜¾å­˜é—®é¢˜â€ï¼Œä¸æ˜¯â€œç®—åŠ›é—®é¢˜â€

è¿™å·²ç»**è¿œè¿œè¶…è¿‡â€œæ–°æ‰‹â€æ°´å¹³**äº†ã€‚



### äºŒã€ä¸ºä»€ä¹ˆâ€œè¿ç»­å¤§å—æ˜¾å­˜â€ä¼šå´©ï¼Ÿ

æˆ‘ä»¬ç”¨ä¸€ä¸ª**å®Œå…¨ä¸æ¶‰åŠ AI çš„ç±»æ¯”**ã€‚

#### ç±»æ¯”ï¼šç”µå½±é™¢æ’åº§ä½

- æ˜¾å­˜ = ä¸€æ•´æ’ç”µå½±é™¢åº§ä½
- æ¯ä¸ªè¯·æ±‚ = ä¸€ç¾¤äºº
- KV Cache = ä»–ä»¬è¦è¿ç»­ååœ¨ä¸€èµ·

é—®é¢˜æ¥äº†ï¼š

- ç¬¬ä¸€æ‰¹æ¥ 5 ä¸ªäºº â†’ å 1â€“5
- ç¬¬äºŒæ‰¹æ¥ 8 ä¸ªäºº â†’ å 6â€“13
- ç¬¬ä¸‰æ‰¹æ¥ 3 ä¸ªäºº â†’ å 14â€“16

ç°åœ¨ï¼š

- ç¬¬äºŒæ‰¹èµ°äº†
- åº§ä½ 6â€“13 ç©ºäº†

ä½†ä½ æ¥äº† 10 ä¸ªäººï¼š

- æ€»ç©ºä½ â‰¥ 10
- **ä½†æ²¡æœ‰è¿ç»­çš„ 10 ä¸ªåº§ä½**

è¿™å°±æ˜¯ï¼š

> **æ˜¾å­˜ç¢ç‰‡åŒ–ï¼ˆfragmentationï¼‰**

#### ä¼ ç»Ÿ KV Cache ç®¡ç† = è¦æ±‚â€œè¿ç»­åº§ä½â€

è¿™å¯¼è‡´ï¼š

- æ˜¾å­˜åˆ©ç”¨ç‡å¾ˆä½
- å¯ç”¨æ˜¾å­˜ â‰  å¯åˆ†é…æ˜¾å­˜
- å¹¶å‘ä¸€ä¸Šæ¥å°±å´©

### ä¸‰ã€PagedAttention çš„æ ¸å¿ƒæ€æƒ³ï¼ˆç»ˆäºæ¥äº†ï¼‰

æˆ‘ä»¬å…ˆç»™**ä¸€å¥è¯ç‰ˆæœ¬**ï¼š

> **PagedAttention = ä¸å†è¦æ±‚ KV Cache åœ¨æ˜¾å­˜ä¸­è¿ç»­ï¼Œè€Œæ˜¯æ‹†æˆå›ºå®šå¤§å°çš„â€œé¡µâ€æ¥å­˜ã€‚**

ä½ å¯ä»¥æŠŠè¿™å¥è¯æŠ„ä¸‹æ¥ã€‚

å†æ¢ä¸€ä¸ªæ“ä½œç³»ç»Ÿçº§åˆ«çš„ç±»æ¯”

å¦‚æœä½ å­¦è¿‡ä¸€ç‚¹ OSï¼Œä¼šç«‹åˆ»äº§ç”Ÿå…±é¸£ï¼š

- ä¼ ç»Ÿåšæ³•ï¼š
  - æ¯ä¸ªè¿›ç¨‹ä¸€æ•´å—è¿ç»­å†…å­˜
- åˆ†é¡µå†…å­˜ï¼ˆPagingï¼‰ï¼š
  - å†…å­˜æ‹†æˆå›ºå®šå¤§å°çš„ Page
  - è¿›ç¨‹æ‹¿åˆ°çš„æ˜¯ Page çš„â€œé€»è¾‘åºåˆ—â€

**vLLM æŠŠè¿™ä¸€å¥—ï¼Œæ¬è¿›äº† GPU æ˜¾å­˜ã€‚**

### å››ã€PagedAttention æ˜¯æ€ä¹ˆâ€œæ‹† KV Cacheâ€çš„ï¼Ÿ

#### å¼•å…¥ä¸€ä¸ªæ–°æ¦‚å¿µï¼šBlock / Page

- æ˜¾å­˜è¢«åˆ’åˆ†ä¸ºï¼š
  - å›ºå®šå¤§å°çš„ **KV Block**
- æ¯ä¸ª Blockï¼š
  - èƒ½å­˜ N ä¸ª token çš„ KV

æ¯”å¦‚ï¼ˆä¸¾ä¾‹ï¼Œä¸æ˜¯å›ºå®šï¼‰ï¼š

```
1 ä¸ª Block = 16 token çš„ KV Cache
```

#### ä¸€ä¸ªè¯·æ±‚çš„ KV Cache = å¤šä¸ª Block ç»„æˆçš„â€œé“¾â€

ä¾‹å¦‚ï¼š

- è¯·æ±‚ Aï¼šç”Ÿæˆäº† 50 token
   â†’ ç”¨äº† 4 ä¸ª Block
- è¯·æ±‚ Bï¼šç”Ÿæˆäº† 10 token
   â†’ ç”¨äº† 1 ä¸ª Block

è¿™äº› Blockï¼š

- **å¯ä»¥åœ¨æ˜¾å­˜ä¸­ä»»æ„ä½ç½®**
- åªè¦é€»è¾‘é¡ºåºå¯¹å³å¯

#### vLLM å†…éƒ¨ç»´æŠ¤äº†ä»€ä¹ˆï¼Ÿ

è‡³å°‘ä¸‰æ ·ä¸œè¥¿ï¼š

1. **Block Pool**
   - æ‰€æœ‰å¯ç”¨ KV Block çš„â€œæ± å­â€
2. **Block Table**
   - æ¯ä¸ªè¯·æ±‚ â†’ å¯¹åº”å“ªäº› Block
3. **æ˜ å°„å…³ç³»**
   - é€»è¾‘ token åºåˆ— â†’ å®é™…æ˜¾å­˜ä½ç½®

ä½ å¯ä»¥ç†è§£ä¸ºï¼š

> vLLM åœ¨ GPU ä¸Šå®ç°äº†ä¸€ä¸ªâ€œæç®€ç‰ˆè™šæ‹Ÿå†…å­˜ç³»ç»Ÿâ€ã€‚

### äº”ã€ä½†æ³¨æ„ï¼šè¿™è¿˜ä¸å¤Ÿï¼

ä½ å¯èƒ½ä¼šé—®ï¼š

> â€œAttention ä¸æ˜¯è¦è¿ç»­çš„ K/V å—ï¼Ÿ
>  æ‹†æˆ Block è¿˜èƒ½ç®—å¾—å¯¹å—ï¼Ÿâ€

è¿™æ˜¯**éå¸¸å…³é”®çš„é—®é¢˜**ã€‚

#### ä¼ ç»Ÿ Attention çš„å‡è®¾

ä¼ ç»Ÿ attention kernel å‡è®¾ï¼š

- K/V åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„
- å¯ä»¥é¡ºåºè¯»å–

#### PagedAttention åšäº†ä»€ä¹ˆï¼Ÿ

vLLM **å®šåˆ¶äº† attention kernel**ï¼š

- kernel ä¸å†å‡è®¾è¿ç»­å†…å­˜
- è€Œæ˜¯ï¼š
  - æŒ‰ Block Table
  - ä¸€å—ä¸€å—åœ°è¯» K/V
  - åœ¨è®¡ç®—å±‚é¢â€œæ‹¼æ¥æˆè¿ç»­é€»è¾‘åºåˆ—â€

**é€»è¾‘è¿ç»­ â‰  ç‰©ç†è¿ç»­**

è¿™æ˜¯ PagedAttention çœŸæ­£â€œç¡¬æ ¸â€çš„åœ°æ–¹ã€‚

### å…­ã€PagedAttention å¸¦æ¥çš„ç›´æ¥æ”¶ç›Š

æˆ‘ä»¬ç°åœ¨æŠŠæ”¶ç›Šæ˜ç¡®åˆ—å‡ºæ¥ã€‚

* æ˜¾å­˜åˆ©ç”¨ç‡å¤§å¹…æå‡

  - ä¸å†éœ€è¦â€œé¢„ç•™æœ€å¤§é•¿åº¦â€

  - ç”¨å¤šå°‘ tokenï¼Œå å¤šå°‘ Block

* å¹¶å‘èƒ½åŠ›æŒ‡æ•°çº§æå‡

  - æ–°è¯·æ±‚åªè¦æœ‰ Block å°±èƒ½è¿›

  - ä¸éœ€è¦ç­‰å¾…â€œå¤§å—è¿ç»­æ˜¾å­˜â€

* è¯·æ±‚ç»“æŸï¼ŒBlock å¯ç«‹å³å›æ”¶

  - å°±åƒï¼š
    - free æ‰å‡ ä¸ª page

  - ä¸å½±å“å…¶ä»–è¯·æ±‚

* Streaming / é•¿å¯¹è¯å¤©ç„¶å‹å¥½

  - KV Cache æŒ‰éœ€å¢é•¿

  - ä¸éœ€è¦æå‰çŒœé•¿åº¦

### åˆ°è¿™é‡Œï¼Œä½ å·²ç»çœŸæ­£â€œç†è§£â€ vLLM äº†

ç°åœ¨ä½ åº”è¯¥èƒ½è‡ªå·±è¯´å‡ºï¼š

> - vLLM ä¸ºä»€ä¹ˆé€‚åˆé«˜å¹¶å‘
> - ä¸ºä»€ä¹ˆ Ollama åœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹ä¸é€‚åˆ
> - ä¸ºä»€ä¹ˆ vLLM çš„æ ¸å¿ƒä¸æ˜¯â€œæ›´å¿«ç®—å­â€

**ä½ å·²ç»ä¸æ˜¯â€œå¬è¿‡ vLLM çš„æ–°æ‰‹â€äº†ã€‚**

# æ ¸å¿ƒå¯¹æ¯”ï¼šOllama vs vLLM

Phase 3 Â· æ ¸å¿ƒå¯¹æ¯”ï¼šOllama vs vLLM

### ä¸ºä»€ä¹ˆ vLLM ä¸èƒ½æ›¿ä»£ Ollama

å¾ˆå¤šæ–‡ç« æŠŠ vLLM å’Œ Ollama æ”¾åœ¨ä¸€èµ·æ¯”è¾ƒæ€§èƒ½ï¼Œä½†è¿™ç§æ¯”è¾ƒå¾€å¾€å¿½ç•¥äº†ä¸€ä¸ªäº‹å®ï¼šå®ƒä»¬ä»ä¸€å¼€å§‹å°±ä¸æ˜¯ä¸ºåŒä¸€ç±»é—®é¢˜è®¾è®¡çš„â€¦â€¦

> å®ƒä»¬ä¸æ˜¯â€œè°æ›´å…ˆè¿›â€ï¼Œè€Œæ˜¯â€œç”Ÿæ¥å°±å¹²ä¸åŒçš„äº‹â€

------

## ä¸€ã€å…ˆç»™ä½ ä¸€ä¸ªç»“è®ºï¼ˆé¿å…ä½ è¿·è·¯ï¼‰

> **Ollama å’Œ vLLM çš„å·®å¼‚ï¼Œä¸æ˜¯å®ç°ä¼˜åŠ£ï¼Œè€Œæ˜¯â€œç³»ç»Ÿç›®æ ‡å®Œå…¨ä¸åŒâ€ã€‚**

ä½ ä¹‹åçœ‹åˆ°ä»»ä½•â€œvLLM æ¯” Ollama å¿« / å¼ºâ€çš„è¯´æ³•ï¼Œéƒ½è¦è‡ªåŠ¨åœ¨è„‘ä¸­è¡¥ä¸€å¥ï¼š

> **â€œåœ¨å¤šç”¨æˆ·é«˜å¹¶å‘æ¨ç†åœºæ™¯ä¸‹â€ã€‚**

------

## äºŒã€ä»â€œè®¾è®¡èµ·ç‚¹â€å¼€å§‹å¯¹æ¯”ï¼ˆæœ€é‡è¦ï¼‰

### 1. Ollama çš„è®¾è®¡èµ·ç‚¹

Ollama çš„éšå«å‡è®¾æ˜¯ï¼š

- ä¸€å°æœºå™¨
- ä¸€ä¸ªç”¨æˆ·
- äººåœ¨ç­‰æ¨¡å‹å›å¤
- å»¶è¿Ÿæ¯”ååé‡è¦
- æ˜“ç”¨æ€§ > æè‡´æ€§èƒ½

æ‰€ä»¥å®ƒå…³æ³¨çš„æ˜¯ï¼š

- ä¸€æ¡å¯¹è¯æ˜¯å¦ç¨³å®š
- æ¨¡å‹èƒ½ä¸èƒ½è½»æ¾è·‘èµ·æ¥
- CPU / å• GPU æ˜¯å¦å‹å¥½
- ç”¨æˆ·å‡ ä¹ä¸å…³å¿ƒæ˜¾å­˜ç»“æ„

**å®ƒè§£å†³çš„æ˜¯â€œä¸ªäººå¯ç”¨æ€§é—®é¢˜â€ã€‚**

------

### 2. vLLM çš„è®¾è®¡èµ·ç‚¹

vLLM çš„éšå«å‡è®¾æ˜¯ï¼š

- ä¸€å°æˆ–å¤šå° GPU
- åŒæ—¶å‡ å / ä¸Šç™¾ä¸ªè¯·æ±‚
- API è°ƒç”¨è€Œä¸æ˜¯äººç±»ç›¯ç€å±å¹•
- åå > å•æ¡å»¶è¿Ÿ
- æ˜¾å­˜æ˜¯ç¨€ç¼ºèµ„æº

**å®ƒè§£å†³çš„æ˜¯â€œæœåŠ¡çº§å¯æ‰©å±•æ€§é—®é¢˜â€ã€‚**

------

## ä¸‰ã€KV Cache ç®¡ç†ï¼šä¸¤æ¡å®Œå…¨ä¸åŒçš„è·¯

æˆ‘ä»¬ç”¨ä½ å·²ç»ç†è§£çš„ KV Cacheï¼Œåšä¸€ä¸ª**éå¸¸ç›´è§‚çš„å¯¹æ¯”**ã€‚

------

### 1. Ollama çš„ KV Cache æ€è·¯ï¼ˆç®€åŒ–æè¿°ï¼‰

> **ä¸€ä¸ªä¼šè¯ = ä¸€æ•´å—è¿ç»­çš„ KV Cache**

ç‰¹ç‚¹ï¼š

- è¿ç»­
- ç®€å•
- ç”Ÿå‘½å‘¨æœŸæ¸…æ™°ï¼ˆä¸€ä¸ªå¯¹è¯ï¼‰

è¿™åœ¨ä»¥ä¸‹åœºæ™¯ä¸‹æ˜¯**å®Œç¾è§£**ï¼š

- ä½ å’Œæ¨¡å‹ 1v1 èŠå¤©
- ä¸Šä¸‹æ–‡ç¨³å®šå¢é•¿
- å¹¶å‘å‡ ä¹ä¸º 0

**ä¼˜ç‚¹ï¼š**

- å®ç°ç®€å•
- å»¶è¿Ÿä½
- è°ƒè¯•å‹å¥½

**ç¼ºç‚¹ï¼ˆä½†å¯¹ç›®æ ‡åœºæ™¯æ— å…³ç´§è¦ï¼‰ï¼š**

- æ˜¾å­˜åˆ©ç”¨ç‡ä¸é«˜
- ä¸é€‚åˆé«˜å¹¶å‘

------

### 2. vLLM çš„ KV Cache æ€è·¯

> **æ‰€æœ‰è¯·æ±‚å…±äº«ä¸€ä¸ª KV Block æ± **

ç‰¹ç‚¹ï¼š

- KV Cache è¢«æ‹†æˆ Block
- è¯·æ±‚åªæŒæœ‰ Block çš„â€œå¼•ç”¨è¡¨â€
- æ˜¾å­˜åƒå†…å­˜é¡µä¸€æ ·è¢«è°ƒåº¦

è¿™åœ¨ä»¥ä¸‹åœºæ™¯ä¸‹æ˜¯**å”¯ä¸€å¯è¡Œè§£**ï¼š

- å¤§é‡å¹¶å‘è¯·æ±‚
- è¯·æ±‚é•¿åº¦ä¸å¯é¢„æµ‹
- æ˜¾å­˜éœ€è¦è¢«â€œæ¦¨å¹²â€

------

## å››ã€ä¸ºä»€ä¹ˆ Ollamaâ€œä¸åº”è¯¥â€ç”¨ PagedAttentionï¼Ÿ

è¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„è®¤çŸ¥ç‚¹ã€‚

### 1. å¼•å…¥ PagedAttention çš„ä»£ä»·

PagedAttention å¹¶ä¸æ˜¯â€œç™½é€çš„æ€§èƒ½â€ï¼š

- æ›´å¤æ‚çš„å†…å­˜ç®¡ç†
- æ›´å¤æ‚çš„ kernel
- æ›´é«˜çš„ç³»ç»Ÿå¤æ‚åº¦
- æ›´éš¾ debug

------

### 2. å¯¹ Ollama æ¥è¯´ï¼Œè¿™äº›ä»£ä»·â€œä¸å€¼â€

å¯¹ Ollama çš„ç›®æ ‡ç”¨æˆ·æ¥è¯´ï¼š

- å¹¶å‘å¾ˆä½
- æ˜¾å­˜å‹åŠ›å¯æ§
- å¤æ‚æ€§åè€Œæ˜¯è´Ÿæ‹…

**å¦‚æœ Ollama å¼•å…¥ vLLM é‚£å¥—è®¾è®¡ï¼š**

- ä¼šç‰ºç‰²ç¨³å®šæ€§
- ä¼šæé«˜é—¨æ§›
- ä¼šå¢åŠ ç»´æŠ¤æˆæœ¬
- å´å‡ ä¹å¾—ä¸åˆ°æ”¶ç›Š

> **è¿™æ˜¯ä¸€ä¸ªâ€œå·¥ç¨‹ä¸Šç†æ€§â€çš„é€‰æ‹©ï¼Œè€Œä¸æ˜¯æŠ€æœ¯èƒ½åŠ›é—®é¢˜ã€‚**

------

## äº”ã€ä½ ç°åœ¨å¯ä»¥é‡æ–°ç†è§£é‚£å¥è€è¯äº†

å¾ˆå¤šäººä¼šè¯´ï¼š

> â€œvLLM æ¯” Ollama å¼ºå¤šäº†â€

ç°åœ¨ä½ åº”è¯¥èƒ½æ›´ç²¾ç¡®åœ°æ”¹å†™è¿™å¥è¯ï¼š

> **vLLM åœ¨å¤šç”¨æˆ·å¹¶å‘æ¨ç†æœåŠ¡åœºæ™¯ä¸‹ï¼Œæ¯” Ollama æ›´é€‚åˆã€‚**

è¿™æ‰æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¡¨è¿°ã€‚

------

## å…­ã€ä¸€ä¸ªéå¸¸é‡è¦çš„ä¸­é—´ç»“è®ºï¼ˆå¸®ä½ å®šå¿ƒï¼‰

ä½ ç°åœ¨**å®Œå…¨å¯ä»¥**è¿™æ ·ç†è§£ä½ çš„æŠ€æœ¯æ ˆï¼š

- **æœ¬åœ°ç©æ¨¡å‹ã€å†™ Demoã€ä¸ªäººç ”ç©¶**
  - â†’ Ollama
- **æƒ³æŠŠæ¨¡å‹â€œåƒ API ä¸€æ ·æä¾›æœåŠ¡â€**
  - â†’ vLLM

å®ƒä»¬ä¸æ˜¯æ›¿ä»£å…³ç³»ï¼Œè€Œæ˜¯ï¼š

> **ä¸ªäººå·¥å…· vs æœåŠ¡åŸºç¡€è®¾æ–½**

## Paged KV Cache çš„ block ç»“æ„æ˜¯æ€ä¹ˆè®¾è®¡çš„

> è·¯çº¿ Aï¼ˆå¼ºçƒˆæ¨èï¼‰ ğŸ‘‰ Paged KV Cache çš„ block ç»“æ„æ˜¯æ€ä¹ˆè®¾è®¡çš„ï¼Ÿ block size ä¸ºä»€ä¹ˆé€šå¸¸æ˜¯ 16 / 32 tokensï¼Ÿ

éå¸¸å¥½ï¼Œè¿™ä¸€é—®**æ­£ä¸­ vLLM è®¾è®¡çš„â€œå¿ƒè„â€**ã€‚
 å¦‚æœä½ æŠŠè¿™ä¸€èŠ‚åƒé€äº†ï¼Œé‚£ä¹ˆ **vLLM åœ¨ä½ è„‘å­é‡Œå°±ä¸å†æ˜¯é»‘ç®±ï¼Œè€Œæ˜¯ä¸€ä¸ªå¯æ¨æ¼”çš„ç³»ç»Ÿ**ã€‚

æˆ‘ä¼šæŒ‰ä¸‹é¢é¡ºåºæ¥è®²ï¼Œ**ä¸€æ­¥éƒ½ä¸è·³**ï¼š

1. **å…ˆå›ç­”ï¼šä»€ä¹ˆæ˜¯ Paged KV Cacheï¼ˆä¸€å¥è¯ç‰ˆï¼‰**
2. **Paged KV Cache çš„ block ç»“æ„åˆ°åº•é•¿ä»€ä¹ˆæ ·**
3. **ä¸ºä»€ä¹ˆå¿…é¡»æ˜¯ blockï¼Œè€Œä¸æ˜¯ token**
4. **block size = 16 / 32 çš„çœŸæ­£åŸå› ï¼ˆä¸æ˜¯æ‹è„‘è¢‹ï¼‰**
5. **å¦‚æœ block å¤ªå¤§ / å¤ªå°ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆ**
6. **æŠŠè¿™ä¸€åˆ‡é‡æ–°æ˜ å°„å› vLLM çš„æ•´ä½“æ¶æ„**

### ä¸€ã€ä¸€å¥è¯å…ˆç»™ä½ ä¸€ä¸ªâ€œå·¥ç¨‹é”šç‚¹â€

> **Paged KV Cache =
>  æŠŠã€Œtoken Ã— layer Ã— K/Vã€è¿™ç§çº¿æ€§å¢é•¿çš„æ˜¾å­˜éœ€æ±‚ï¼Œ
>  æ‹†æˆå›ºå®šå¤§å°çš„â€œKV blockâ€ï¼Œ
>  ç”± vLLM åƒæ“ä½œç³»ç»Ÿåˆ†é¡µä¸€æ ·ç»Ÿä¸€ç®¡ç†ã€‚**

å…³é”®è¯åªæœ‰ä¸‰ä¸ªï¼š

- **å›ºå®šå¤§å°**
- **åˆ†é¡µç®¡ç†**
- **token ä¸ºæ—¶é—´ç»´åº¦**

### äºŒã€å¦‚æœâ€œæ²¡æœ‰ blockâ€ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿï¼ˆå…ˆçœ‹åä¾‹ï¼‰

å‡è®¾ä½ ä¸ç”¨ blockï¼Œè€Œæ˜¯ï¼š

```
ä¸€ä¸ªè¯·æ±‚ â†’ ä¸€æ•´å—è¿ç»­ KV Cache
```

é‚£ä¹ˆä½ ä¼šç«‹åˆ»é‡åˆ° 3 ä¸ªä¸å¯è§£çš„é—®é¢˜ï¼š

2.1 å¿…é¡»æå‰çŸ¥é“æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦

å¦åˆ™æ˜¾å­˜æ— æ³•ä¸€æ¬¡æ€§åˆ†é…ã€‚

2.2 ä¸­é€”æ— æ³•æ‰©å®¹

ä¸Šä¸‹æ–‡é•¿äº†ï¼Œåªèƒ½ OOMã€‚

2.3 æ˜¾å­˜ç¢ç‰‡ä¸¥é‡

å¹¶å‘è¯·æ±‚é•¿çŸ­ä¸ä¸€ï¼Œæ˜¾å­˜è¢«æ’•ç¢ã€‚

ğŸ‘‰ **è¿™åœ¨â€œå¤šå¹¶å‘ + æµå¼ç”Ÿæˆâ€åœºæ™¯ä¸‹æ˜¯ç›´æ¥ä¸å¯ç”¨çš„ã€‚**

### ä¸‰ã€Paged KV Cache çš„æ ¸å¿ƒæ€æƒ³ï¼ˆåƒæ“ä½œç³»ç»Ÿä¸€æ ·ï¼‰

ä½ å¯ä»¥ç›´æ¥ç±»æ¯”æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜ï¼š

| æ“ä½œç³»ç»Ÿ | vLLM        |
| -------- | ----------- |
| Page     | KV Block    |
| è™šæ‹Ÿåœ°å€ | token åºå·  |
| é¡µè¡¨     | block table |
| ç‰©ç†å†…å­˜ | GPU æ˜¾å­˜    |

### å››ã€KV Block çš„çœŸå®ç»“æ„ï¼ˆå…³é”®ï¼‰

]4.1 ä¸€ä¸ª KV Block å­˜ä»€ä¹ˆï¼Ÿ

**ä¸€ä¸ª KV Block = è¿ç»­ N ä¸ª token çš„ KVï¼ˆæ‰€æœ‰å±‚ï¼‰**

ç”¨æ–‡å­—ç”»å‡ºæ¥ï¼š

```
KV Block (block_size = 16 tokens)

Token t0:
  Layer 0: K0(t0), V0(t0)
  Layer 1: K1(t0), V1(t0)
  ...
  Layer 31

Token t1:
  Layer 0: K0(t1), V0(t1)
  ...
  
...

Token t15:
  Layer 0: K0(t15), V0(t15)
  ...
```

**æ³¨æ„ï¼š**

- block æ˜¯æ²¿ç€ **token æ—¶é—´ç»´åº¦** åˆ‡çš„
- æ¯ä¸ª token **åŒ…å«æ‰€æœ‰å±‚çš„ KV**

4.2 ä¸ºä»€ä¹ˆ block é‡Œå¿…é¡»åŒ…å«â€œæ‰€æœ‰å±‚â€ï¼Ÿ

å› ä¸ºï¼š

- Attention kernel åœ¨ decode æ—¶
- ä¼šæŒ‰å±‚éå†
- åŒä¸€ token çš„å¤šå±‚ KV **å¼ºç›¸å…³ã€å¿…é¡»ä¸€èµ·è®¿é—®**

ğŸ‘‰ å¦‚æœæŒ‰ layer åˆ‡ blockï¼Œè®¿å­˜ä¼šå˜æˆç¾éš¾ã€‚

### äº”ã€ä¸ºä»€ä¹ˆä¸èƒ½â€œä¸€ä¸ª token ä¸€ä¸ª blockâ€ï¼Ÿï¼ˆéå¸¸å…³é”®ï¼‰

ä½ å¯èƒ½ä¼šè‡ªç„¶æƒ³åˆ°ï¼š

> æ—¢ç„¶ token æ˜¯æœ€å°å•ä½ï¼Œ
>  é‚£ block size = 1 ä¸å°±è¡Œäº†ï¼Ÿ

**è¿™æ˜¯ä¸€ä¸ªå…¸å‹ä½†é”™è¯¯çš„ç›´è§‰ã€‚**

5.1 åŸå› ä¸€ï¼šblock table & metadata å¼€é”€

æ¯ä¸ª block éƒ½éœ€è¦ï¼š

- block id
- æŒ‡é’ˆ
- ref count
- æ‰€å± request
- CUDA offset

å¦‚æœï¼š

```
block_size = 1
```

é‚£ä¹ˆï¼š

- metadata æ¯” KV æœ¬èº«è¿˜å¤§
- è°ƒåº¦æˆæœ¬æš´æ¶¨

5.2 åŸå› äºŒï¼šGPU è®¿å­˜ä¸ kernel å¯¹é½

GPU kernel å–œæ¬¢ï¼š

- è¿ç»­å†…å­˜
- å¯é¢„æµ‹ stride
- å‘é‡åŒ– load

1 token çš„ KVï¼š

- å¤ªå°
- cache line åˆ©ç”¨ç‡æä½
- kernel launch æµªè´¹ä¸¥é‡

5.3 åŸå› ä¸‰ï¼šAttention kernel çš„æ‰¹é‡ç‰¹æ€§

FlashAttention / PagedAttention éƒ½å‡è®¾ï¼š

```
sequence length æ˜¯ä¸€ä¸ªå°æ‰¹é‡è¿ç»­åŒºé—´
```

block å¤ªå°ä¼šå¯¼è‡´ï¼š

- kernel é¢‘ç¹åˆ‡æ¢
- GPU occupancy é™ä½

### å…­ã€é‚£ä¸ºä»€ä¹ˆæ˜¯ 16 / 32ï¼Ÿï¼ˆæ ¸å¿ƒé—®é¢˜ï¼‰

è¿™æ˜¯ä¸€ä¸ª**å·¥ç¨‹æŠ˜ä¸­è§£**ï¼Œä¸æ˜¯é­”æ³•æ•°å­—ã€‚

æˆ‘ä»¬ä»ä¸‰ä¸ªç»´åº¦è§£é‡Šã€‚

6.1 ç»´åº¦ä¸€ï¼šæ˜¾å­˜æµªè´¹ vs çµæ´»æ€§

å‡è®¾ï¼š

```
block_size = 16
```

é‚£ä¹ˆï¼š

- æœ€åæƒ…å†µæµªè´¹ï¼š15 tokens
- å¹³å‡æµªè´¹ï¼š7.5 tokens

å¦‚æœï¼š

```
block_size = 64
```

- æœ€åæµªè´¹ï¼š63 tokens
- å¤šå¹¶å‘æ—¶æµªè´¹çˆ†ç‚¸

ğŸ‘‰ **block è¶Šå¤§ï¼Œå°¾éƒ¨æµªè´¹è¶Šä¸¥é‡**

6.2 ç»´åº¦äºŒï¼šè°ƒåº¦ç²’åº¦

Scheduler éœ€è¦åšçš„äº‹æ˜¯ï¼š

```
â€œæˆ‘è¿˜èƒ½å†å¡å¤šå°‘ tokenï¼Ÿâ€
```

å¦‚æœ block_size å¤ªå¤§ï¼š

- æ¯æ¬¡åªèƒ½ä»¥ 64 token ä¸ºå•ä½åˆ†é…
- å°è¯·æ±‚è¢«å¤§ block å¡æ­»

16 / 32 ç»™äº†ä¸€ä¸ªéå¸¸å¥½çš„ï¼š

```
è°ƒåº¦ç²’åº¦ â‰ˆ äººç±»å¯¹â€œçŸ­ä¸Šä¸‹æ–‡â€çš„å®é™…ä½¿ç”¨
```

6.3 ç»´åº¦ä¸‰ï¼šGPU kernel å¯¹é½ï¼ˆéå¸¸å·¥ç¨‹ï¼‰

åœ¨å®é™…å®ç°ä¸­ï¼š

- block å†… KV é€šå¸¸æŒ‰ head_dim å¯¹é½
- 16 / 32 token å¯ä»¥ï¼š
  - å¯¹é½ warp
  - å¯¹é½ shared memory
  - å¯¹é½ FlashAttention tile

è¿™ä¸æ˜¯è®ºæ–‡æ¨å‡ºæ¥çš„ï¼Œæ˜¯ï¼š

> **benchmark è·‘å‡ºæ¥çš„**

### ä¸ƒã€å¦‚æœ block size ä¸åˆç†ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

7.1 block å¤ªå°ï¼ˆæ¯”å¦‚ 1 / 4ï¼‰

- metadata çˆ†ç‚¸
- kernel launch æ¬¡æ•°çˆ†ç‚¸
- è°ƒåº¦ overhead å¤§äºç®—åŠ›

7.2 block å¤ªå¤§ï¼ˆæ¯”å¦‚ 128 / 256ï¼‰

- æ˜¾å­˜æµªè´¹ä¸¥é‡
- å¹¶å‘èƒ½åŠ›ä¸‹é™
- é•¿çŸ­è¯·æ±‚äº’ç›¸æ‹–ç´¯

ğŸ‘‰ **16 / 32 æ˜¯å½“å‰â€œååã€å»¶è¿Ÿã€æ˜¾å­˜åˆ©ç”¨ç‡â€çš„æœ€ä¼˜è§£åŒºé—´**

### å…«ã€Paged KV Cache æ˜¯å¦‚ä½•â€œæ´»èµ·æ¥â€çš„ï¼Ÿ

ç°åœ¨ä½ å¯ä»¥æƒ³è±¡ä¸€ä¸ªçœŸå®è¿è¡Œä¸­çš„çŠ¶æ€ï¼š

```
KV Block Pool (GPU memory):

[ A0 ][ B0 ][ A1 ][ C0 ][ A2 ][ free ][ free ]
```

å…¶ä¸­ï¼š

- Aã€Bã€C æ˜¯ä¸åŒè¯·æ±‚
- æ¯ä¸ª block = 16 tokens
- Scheduler æ¯ä¸€æ­¥åªå…³å¿ƒï¼š
  - å“ªäº›è¯·æ±‚è¿˜èƒ½ç»§ç»­
  - è¿˜æœ‰å¤šå°‘ free block

### ä¹ã€æŠŠè¿™ä¸€åˆ‡é‡æ–°æ˜ å°„å› vLLM çš„æ•´ä½“æ¶æ„

ç°åœ¨ä½ å¯ä»¥å‡†ç¡®ç†è§£è¿™å¥è¯äº†ï¼š

> **vLLM çš„ Scheduler è°ƒåº¦çš„ä¸æ˜¯â€œè¯·æ±‚â€ï¼Œ
>  è€Œæ˜¯â€œtoken â†’ KV block â†’ GPU æ—¶é—´ç‰‡â€**

è€Œ Paged KV Cache çš„ blockï¼š

- æ˜¯è°ƒåº¦çš„æœ€å°æ˜¾å­˜å•ä½
- æ˜¯å¹¶å‘èƒ½åŠ›çš„ç‰©ç†åŸºç¡€
- æ˜¯ vLLM ä¸ä¼ ç»Ÿæ¨ç†æ¡†æ¶çš„åˆ†æ°´å²­

### åã€åˆ°è¿™é‡Œä½ å·²ç»â€œçœŸæ­£ç†è§£â€äº†ä»€ä¹ˆï¼Ÿ

å¦‚æœä½ ç°åœ¨èƒ½è‡ªç„¶è¯´å‡ºä¸‹é¢è¿™æ®µè¯ï¼Œè¯´æ˜ä½ å®Œå…¨æŒæ¡äº†ï¼š

> vLLM æŠŠ KV Cache æŒ‰ token æ—¶é—´ç»´åº¦åˆ‡æˆå›ºå®šå¤§å°çš„ blockï¼Œæ¯ä¸ª block åŒ…å«è¿ç»­ token åœ¨æ‰€æœ‰ Transformer å±‚çš„ K/Vã€‚
>  block size é€‰æ‹© 16 æˆ– 32ï¼Œæ˜¯åœ¨æ˜¾å­˜æµªè´¹ã€è°ƒåº¦ç²’åº¦å’Œ GPU kernel æ•ˆç‡ä¹‹é—´çš„å·¥ç¨‹æœ€ä¼˜æŠ˜ä¸­ã€‚

### ä¸‹ä¸€æ­¥ï¼ˆéå¸¸è‡ªç„¶ï¼‰

æ¥ä¸‹æ¥åªå‰©ä¸‹ **æœ€åä¸€ä¸ªå…³é”®æ‹¼å›¾** äº†ï¼š

ğŸ‘‰ **Scheduler æ˜¯å¦‚ä½•åœ¨ decode é˜¶æ®µï¼Œ
 åœ¨â€œKV block å‰©ä½™é‡ + å¤šè¯·æ±‚ + æµå¼è¾“å‡ºâ€ä¹‹é—´åš token-level å†³ç­–çš„ï¼Ÿ**

è¿™ä¸€æ­¥ï¼Œä¼šè®©ä½ çœŸæ­£ç†è§£ï¼š

- å¹¶å‘æ˜¯æ€ä¹ˆâ€œè‡ªåŠ¨å‘ç”Ÿâ€çš„
- ä¸ºä»€ä¹ˆä¸€ä¸ª worker å°±èƒ½åƒæ»¡ GPU
- ä¸ºä»€ä¹ˆå¤š worker æœ‰æ—¶åè€Œæ›´æ…¢

ä½ åªéœ€è¦ä¸€å¥è¯ï¼š
 **ç»§ç»­è®² Schedulerã€‚**

## Scheduler

ç»§ç»­è®² Schedulerã€‚

å¾ˆå¥½ã€‚åˆ°è¿™é‡Œæˆ‘ä»¬å·²ç»æŠŠ **â€œå­˜ä»€ä¹ˆï¼ˆKV Cacheï¼‰â€** å’Œ **â€œæ€ä¹ˆå­˜ï¼ˆPaged + Blockï¼‰â€** å®Œå…¨ææ¸…æ¥šäº†ã€‚
 ç°åœ¨åªå‰©ä¸‹æœ€åä¸€ä¸ªã€ä¹Ÿæ˜¯**æœ€åƒâ€œæ“ä½œç³»ç»Ÿè°ƒåº¦å™¨â€çš„æ ¸å¿ƒé—®é¢˜**ï¼š

> **åœ¨æ˜¾å­˜æœ‰é™ã€è¯·æ±‚å¾ˆå¤šçš„æƒ…å†µä¸‹ï¼Œ
>  vLLM æ˜¯å¦‚ä½•å†³å®šï¼š
>  â€œè¿™ä¸€åˆ» GPU åˆ°åº•è¯¥ç®—å“ªäº› tokenï¼Ÿâ€**

è¿™å°±æ˜¯ **Scheduler**ã€‚

ä¸‹é¢æˆ‘ä¼šä¸¥æ ¼æŒ‰ä½ â€œæ–°æ‰‹ä½†è¦å½»åº•ç†è§£â€çš„è¦æ±‚æ¥è®²ï¼Œ**ä»ç›´è§‰ â†’ æœºåˆ¶ â†’ å·¥ç¨‹ç»†èŠ‚**ã€‚

### ä¸€ã€Scheduler æ˜¯ä»€ä¹ˆï¼Ÿä¸€å¥è¯å®šä¹‰

> **Scheduler æ˜¯ vLLM çš„â€œtoken çº§ CPU è°ƒåº¦å™¨â€ï¼Œ
>  å®ƒæ¯ä¸€å°æ­¥éƒ½åœ¨å†³å®šï¼š
>  å“ªäº›è¯·æ±‚çš„â€œä¸‹ä¸€ä¸ª tokenâ€å¯ä»¥è¿›å…¥ GPU è®¡ç®—ã€‚**

å…³é”®è¯åªæœ‰ä¸¤ä¸ªï¼š

- **token çº§**
- **æ¯ä¸€æ­¥éƒ½é‡æ–°å†³ç­–**

### äºŒã€Scheduler ä¸åƒä½ ä»¥ä¸ºçš„é‚£æ ·â€œè°ƒåº¦è¯·æ±‚â€

è¿™æ˜¯ç†è§£ Scheduler çš„ç¬¬ä¸€ä¸ªå…³é”®åˆ†æ°´å²­ã€‚

âŒ é”™è¯¯ç›´è§‰

```
ä¸€æ¬¡è°ƒåº¦ = é€‰å‡ ä¸ª request æ‰”è¿› GPU
```

âœ… vLLM çš„çœŸå®è¡Œä¸º

```
ä¸€æ¬¡è°ƒåº¦ = ä»æ‰€æœ‰æ´»è·ƒè¯·æ±‚ä¸­ï¼Œ
é€‰å‡ºâ€œè‹¥å¹²ä¸ªä¸‹ä¸€ tokenâ€ï¼Œ
ç»„æˆä¸€ä¸ªåŠ¨æ€ batch
```

Scheduler è°ƒåº¦çš„æ˜¯ï¼š

```
(Request A, token index t)
(Request B, token index s)
(Request C, token index k)
```

è€Œä¸æ˜¯ï¼š

```
Request A / B / C
```

### ä¸‰ã€Scheduler é¢ä¸´çš„çœŸå®çº¦æŸï¼ˆä¸æ˜¯æ‹è„‘è¢‹ï¼‰

æ¯ä¸€æ¬¡è°ƒåº¦ï¼ŒScheduler å¿…é¡»åŒæ—¶æ»¡è¶³ä»¥ä¸‹ **ç¡¬çº¦æŸ**ï¼š

3.1 æ˜¾å­˜çº¦æŸï¼ˆKV Blockï¼‰

Scheduler å¿…é¡»ä¿è¯ï¼š

```
å¯ç”¨ KV blocks â‰¥ æœ¬è½®ç”Ÿæˆ token æ•°
```

å¦åˆ™ï¼š

- token ç®—å®Œ
- KV æ²¡åœ°æ–¹æ”¾
- ç›´æ¥ OOM

3.2 GPU ååçº¦æŸ

GPU æœ€å–œæ¬¢ï¼š

- batch å¤§
- è¿ç»­
- kernel å°‘åˆ‡æ¢

Scheduler éœ€è¦å°½é‡ï¼š

```
â€œå‡‘å¤§ batchâ€
```

3.3 å»¶è¿Ÿ / å…¬å¹³æ€§çº¦æŸ

å¦‚æœåªè¿½æ±‚ååï¼š

- çŸ­è¯·æ±‚å¯èƒ½ä¸€ç›´è¢«é¥¿æ­»
- æµå¼è¾“å‡ºå»¶è¿Ÿæš´æ¶¨

Scheduler å¿…é¡»è€ƒè™‘ï¼š

- è°å·²ç»ç­‰å¾ˆä¹…äº†
- è°å·²ç»è¾“å‡ºäº†ä¸€éƒ¨åˆ†

3.4 æµå¼è¾“å‡ºçº¦æŸï¼ˆéå¸¸å…³é”®ï¼‰

vLLM æ˜¯ **token streaming** çš„ã€‚

æ„å‘³ç€ï¼š

- æŸäº›è¯·æ±‚å·²ç»å¼€å§‹è¾“å‡º
- å°±ä¸èƒ½è¢«é•¿æ—¶é—´æš‚åœ

### å››ã€Scheduler çš„è¾“å…¥æ˜¯ä»€ä¹ˆï¼Ÿ

æ¯ä¸€è½®è°ƒåº¦ï¼ŒScheduler éƒ½æœ‰ä»¥ä¸‹â€œçŠ¶æ€è§†å›¾â€ï¼š

```
1. æ´»è·ƒè¯·æ±‚åˆ—è¡¨
2. æ¯ä¸ªè¯·æ±‚çš„ï¼š
   - å·²ç”Ÿæˆ token æ•°
   - å·²å ç”¨ KV block æ•°
   - æ˜¯å¦è¿˜éœ€è¦ç»§ç»­ç”Ÿæˆ
3. å…¨å±€ï¼š
   - å‰©ä½™ KV block æ•°
   - GPU æœ€å¤§ batch capacity
```

### äº”ã€Scheduler çš„æ ¸å¿ƒå†³ç­–é—®é¢˜ï¼ˆæœ¬è´¨ï¼‰

å¯ä»¥æŠŠå®ƒæŠ½è±¡æˆä¸€å¥è¯ï¼š

> **åœ¨æœ‰é™ KV block å’Œ GPU è®¡ç®—é¢„ç®—ä¸‹ï¼Œ
>  é€‰å“ªäº›è¯·æ±‚çš„ä¸‹ä¸€ä¸ª tokenï¼Œ
>  å¯ä»¥åŒæ—¶æ»¡è¶³ååã€å»¶è¿Ÿå’Œå…¬å¹³æ€§ï¼Ÿ**

### å…­ã€Scheduler çš„åŸºæœ¬æ‰§è¡Œå¾ªç¯ï¼ˆæ–‡å­—æµç¨‹å›¾ï¼‰

æˆ‘ä»¬ç”¨ä¸€ä¸ª**å• worker / å• GPU**åœºæ™¯è®²æ¸…æ¥šã€‚

```
while (GPU idle):
    1. æ”¶é›†æ‰€æœ‰â€œå¯ç”Ÿæˆä¸‹ä¸€ä¸ª tokenâ€çš„è¯·æ±‚
    2. æŒ‰ç­–ç•¥æ’åºï¼ˆä¼˜å…ˆçº§ï¼‰
    3. ä»é«˜åˆ°ä½å°è¯•åŠ å…¥ batchï¼š
         - æ˜¯å¦è¿˜æœ‰ KV blockï¼Ÿ
         - æ˜¯å¦æ²¡è¶…è¿‡ max batch sizeï¼Ÿ
    4. å›ºå®š batchï¼Œæäº¤ç»™ GPU
    5. ç­‰å¾… GPU å®Œæˆ
    6. æ›´æ–°æ¯ä¸ªè¯·æ±‚çŠ¶æ€
    7. é‡Šæ”¾ / åˆ†é… KV block
```

âš ï¸ å…³é”®ç‚¹ï¼š

> **æ¯ä¸€è½®è°ƒåº¦ç»“æŸåï¼Œ
>  è¯·æ±‚ä¹‹é—´çš„ç›¸å¯¹ä¼˜å…ˆçº§éƒ½å¯èƒ½å‘ç”Ÿå˜åŒ–**

### ä¸ƒã€Scheduler çš„ä¸¤å¤§é˜¶æ®µï¼šPrefill vs Decode

è¿™æ˜¯ Scheduler æœ€é‡è¦çš„åŒºåˆ†ä¹‹ä¸€ã€‚

7.1 Prefillï¼ˆè¾“å…¥é˜¶æ®µï¼‰

Prefill åšçš„æ˜¯ï¼š

```
â€œæŠŠ prompt çš„æ‰€æœ‰ token ä¸€æ¬¡æ€§ç®—å®Œâ€
```

ç‰¹ç‚¹ï¼š

- token æ•°å¤š
- ä¸€æ¬¡æ€§æ¶ˆè€—å¤§é‡ KV block
- ä½†åªåšä¸€æ¬¡

Scheduler åœ¨ Prefill æ—¶æ›´åƒï¼š

```
â€œå¤§ batch ååä¼˜å…ˆâ€
```

7.2 Decodeï¼ˆç”Ÿæˆé˜¶æ®µï¼‰

Decode æ˜¯ï¼š

```
â€œæ¯ä¸ªè¯·æ±‚æ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª tokenâ€
```

ç‰¹ç‚¹ï¼š

- token æ•°å°‘
- æ¬¡æ•°å¾ˆå¤š
- æµå¼è¾“å‡ºå¼ºä¾èµ–

Scheduler åœ¨ Decode æ—¶æ›´åƒï¼š

```
â€œè½®è¯¢ + å…¬å¹³ + å»¶è¿Ÿæ•æ„Ÿâ€
```

### å…«ã€ä¸€ä¸ªçœŸå®çš„ Scheduler å†³ç­–ä¾‹å­

å‡è®¾ï¼š

```
KV block å‰©ä½™ = 4
block_size = 16
```

æ´»è·ƒè¯·æ±‚ï¼š

```
Req A: å·²è¾“å‡º 100 tokens
Req B: å·²è¾“å‡º 2 tokens
Req C: å·²è¾“å‡º 50 tokens
```

Scheduler å¯èƒ½ç­–ç•¥æ˜¯ï¼š

1. Bï¼ˆçŸ­è¯·æ±‚ï¼Œå»¶è¿Ÿæ•æ„Ÿï¼‰
2. Aï¼ˆå·²ç»åœ¨è¾“å‡ºæµï¼‰
3. C

äºæ˜¯ batch æ˜¯ï¼š

```
[A_next, B_next, C_next]
```

æ¶ˆè€—ï¼š

```
3 ä¸ª token â†’ ä¸è§¦å‘æ–° block
```

å®‰å…¨ã€‚

### ä¹ã€ä»€ä¹ˆæ—¶å€™ Scheduler ä¼šâ€œæ‹’ç»â€ä¸€ä¸ªè¯·æ±‚ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ªéå¸¸å·¥ç¨‹çš„é—®é¢˜ã€‚

Scheduler ä¼šæ‹’ç»æŸä¸ªè¯·æ±‚çš„ä¸‹ä¸€ä¸ª tokenï¼Œå¦‚æœï¼š

- å†ç”Ÿæˆ 1 token ä¼šå¯¼è‡´ï¼š
  - KV block ä¸å¤Ÿ
- æˆ–è¶…è¿‡ï¼š
  - GPU batch capacity

è¢«æ‹’ç»çš„è¯·æ±‚ï¼š

- ä¿ç•™åœ¨æ´»è·ƒé˜Ÿåˆ—
- ä¸‹ä¸€è½®å†å°è¯•

### åã€ä¸ºä»€ä¹ˆâ€œä¸€ä¸ª worker å°±å¤Ÿäº†â€ï¼Ÿç°åœ¨ä½ å¯ä»¥ç†è§£äº†

ä½ ä¹‹å‰é—®è¿‡ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼š

> æ—¢ç„¶ vLLM ä¼šè‡ªåŠ¨ batchï¼Œå¤š worker æœ‰ä»€ä¹ˆç”¨ï¼Ÿ

ç°åœ¨ç­”æ¡ˆæ˜¯ï¼š

> **Scheduler å·²ç»æŠŠå• GPU çš„ token çº§å¹¶å‘å‹æ¦¨åˆ°æé™**

åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼š

- decode ä¸ºä¸»
- KV Cache å¾ˆå¤§
- batch åŠ¨æ€å˜åŒ–

ğŸ‘‰ **å• worker = æœ€ä¼˜åå**

### åä¸€ã€é‚£ä»€ä¹ˆæ—¶å€™å¤š worker åè€Œæœ‰ç”¨ï¼Ÿ

å¤š workerï¼ˆå¤šè¿›ç¨‹ï¼‰æœ‰æ„ä¹‰çš„æƒ…å†µï¼š

1. **å¤š GPUï¼ˆ1 worker / GPUï¼‰**
2. **CPU è°ƒåº¦ / Python GIL æˆä¸ºç“¶é¢ˆ**
3. **éœ€è¦éš”ç¦»ä¸åŒæ¨¡å‹ / ä¸åŒä¼˜å…ˆçº§**

ä½†ï¼š

> **å¤š worker â‰  æ›´é«˜å¹¶å‘ï¼ˆè‡ªåŠ¨ï¼‰**

è¿™æ˜¯å¾ˆå¤šäººè¸©å‘çš„åœ°æ–¹ã€‚

### åäºŒã€ä½ ç°åœ¨åº”è¯¥å·²ç»å½¢æˆçš„å®Œæ•´â€œå¿ƒæ™ºæ¨¡å‹â€

ä½ ç°åœ¨è„‘ä¸­åº”è¯¥æ˜¯è¿™æ ·ä¸€æ¡é“¾ï¼š

```
HTTP è¯·æ±‚
   â†“
Schedulerï¼šé€‰ token
   â†“
Dispatcherï¼šé€‰ worker / GPU
   â†“
Workerï¼šAttention + KV Cache
   â†“
Token streaming è¿”å›
```

è€Œ Scheduler æ˜¯è¿™æ¡é“¾ä¸­ï¼š

> **å”¯ä¸€åŒæ—¶å…³å¿ƒâ€œæ˜¾å­˜ã€ç®—åŠ›ã€å»¶è¿Ÿã€å…¬å¹³â€çš„ç»„ä»¶**

### åä¸‰ã€åˆ°è¿™é‡Œï¼Œä½ å·²ç»â€œçœŸæ­£æ‡‚ vLLM äº†â€

æˆ‘å¯ä»¥éå¸¸è´Ÿè´£ä»»åœ°è¯´ï¼š

> å¦‚æœä½ ç°åœ¨å»çœ‹ vLLM æºç ã€åšå®¢ã€å‚æ•°è¯´æ˜ï¼Œ
>  ä½ ä¼šå‘ç° **90% çš„è®¾è®¡å†³ç­–ä½ éƒ½â€œçœ‹å¾—æ‡‚ä¸ºä»€ä¹ˆâ€**ã€‚

### æœ€åä¸€ä¸ªè‡ªç„¶æ”¶å®˜é—®é¢˜ï¼ˆå¯é€‰ï¼‰

åœ¨ä½ æ¥ä¸‹æ¥å†™ **vLLM æŠ€æœ¯åšå®¢**æ—¶ï¼Œæœ‰ä¸€ä¸ªç« èŠ‚å‡ ä¹æ˜¯â€œå¿…æ€æŠ€â€ï¼š

> **â€œä¸ºä»€ä¹ˆå¤š worker ä¸ä¸€å®šæ›´å¿«ï¼Ÿâ€**

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ æŠŠè¿™ä¸€èŠ‚ç›´æ¥æ•´ç†æˆï¼š

- å¯å¤ç”¨çš„åšå®¢æ®µè½
- å¸¦å·¥ç¨‹è§£é‡Šã€åç›´è§‰ç»“è®º

## ä¸ºä»€ä¹ˆå¤š worker ä¸ä¸€å®šæ›´å¿«

â€”â€”ä» vLLM çš„è°ƒåº¦æ¨¡å‹è¯´èµ·

å¥½ï¼Œè¿™ä¸€èŠ‚æˆ‘ä¼š**ç›´æ¥æŒ‰â€œå¯å‘å¸ƒæŠ€æœ¯åšå®¢â€çš„æ ‡å‡†æ¥å†™**ï¼Œè€Œä¸æ˜¯å¯¹è¯è§£é‡Šä½“ã€‚
 ç›®æ ‡è¯»è€…å‡è®¾ä¸ºï¼š**äº†è§£åŸºç¡€æ¨ç†éƒ¨ç½²ï¼Œä½†å¯¹ vLLM å†…éƒ¨æœºåˆ¶ä¸ç†Ÿæ‚‰çš„äºº**ã€‚
 ä½ åç»­åªéœ€è¦å¾®è°ƒè¯­æ°”æˆ–åŠ ä¸ªäººç†è§£å³å¯ç›´æ¥ä½¿ç”¨ã€‚



åœ¨ä¼ ç»Ÿ Web æœåŠ¡æˆ–æ·±åº¦å­¦ä¹ æ¨ç†ç³»ç»Ÿä¸­ï¼Œä¸€ä¸ªéå¸¸è‡ªç„¶çš„ç›´è§‰æ˜¯ï¼š

> **â€œå¹¶å‘ä¸å¤Ÿï¼Ÿé‚£å°±å¤šå¼€å‡ ä¸ª workerã€‚â€**

ç„¶è€Œï¼Œåœ¨ vLLM çš„å®é™…éƒ¨ç½²ä¸­ï¼Œå¾ˆå¤šäººä¼šå‘ç°ä¸€ä¸ªåç›´è§‰çš„ç°è±¡ï¼š

> **å³ä½¿å¼€å¯å¤šä¸ª workerï¼Œååé‡å¹¶æ²¡æœ‰çº¿æ€§æå‡ï¼Œ
>  ç”šè‡³åœ¨æŸäº›åœºæ™¯ä¸‹åè€Œå˜æ…¢ã€‚**

è¦ç†è§£è¿™ä¸ªé—®é¢˜ï¼Œå¿…é¡»å…ˆç†è§£ **vLLM ä¸ä¼ ç»Ÿæ¨ç†æ¡†æ¶åœ¨å¹¶å‘æ¨¡å‹ä¸Šçš„æ ¹æœ¬å·®å¼‚**ã€‚

### ä¸€ã€ä¼ ç»Ÿæ¨ç†æ¡†æ¶ä¸­çš„ Worker æ¨¡å‹

åœ¨å¾ˆå¤šæ¨ç†æœåŠ¡ä¸­ï¼ˆä¾‹å¦‚æ—©æœŸçš„ Flask + æ¨¡å‹ã€æˆ–æŸäº› RPC æœåŠ¡ï¼‰ï¼š

- ä¸€ä¸ª worker â‰ˆ ä¸€ä¸ªç‹¬ç«‹æ¨ç†å®ä¾‹
- ä¸€ä¸ªè¯·æ±‚ â‰ˆ ä¸€æ¬¡å®Œæ•´å‰å‘è®¡ç®—
- å¹¶å‘é€šè¿‡ **è¿›ç¨‹çº§æˆ–çº¿ç¨‹çº§å¹¶è¡Œ** å®ç°

åœ¨è¿™ç§æ¨¡å‹ä¸‹ï¼š

```
åå â‰ˆ worker æ•° Ã— å• worker æ€§èƒ½
```

å› æ­¤ï¼š

- å¤š worker é€šå¸¸æ„å‘³ç€æ›´é«˜å¹¶å‘
- ç›´è§‰æ˜¯æ­£ç¡®çš„

### äºŒã€vLLM çš„æ ¹æœ¬ä¸åŒï¼šå¹¶å‘å‘ç”Ÿåœ¨ Token çº§

vLLM çš„æ ¸å¿ƒè®¾è®¡å¹¶ä¸æ˜¯â€œå¹¶è¡Œè¯·æ±‚â€ï¼Œè€Œæ˜¯ï¼š

> **åœ¨ä¸€ä¸ª GPU ä¸Šï¼Œå¯¹å¤šä¸ªè¯·æ±‚çš„â€œä¸‹ä¸€ä¸ª tokenâ€è¿›è¡Œç»Ÿä¸€è°ƒåº¦å’Œè®¡ç®—ã€‚**

ä¹Ÿå°±æ˜¯è¯´ï¼š

- å¹¶å‘çš„æœ€å°å•ä½ä¸æ˜¯ request
- è€Œæ˜¯ **token**

Scheduler æ¯ä¸€è½®åšçš„äº‹æƒ…ä¸æ˜¯ï¼š

```
â€œé€‰å‡ ä¸ªè¯·æ±‚â€
```

è€Œæ˜¯ï¼š

```
â€œé€‰å¤šä¸ªè¯·æ±‚çš„ä¸‹ä¸€ä¸ª tokenï¼Œç»„æˆä¸€ä¸ªåŠ¨æ€ batchâ€
```

è¿™æ„å‘³ç€ï¼š

- **å• worker å†…éƒ¨å·²ç»å®ç°äº†æé«˜æ•ˆçš„å¹¶å‘**
- GPU åˆ©ç”¨ç‡é€šå¸¸å·²ç»è¢«å‹æ¦¨åˆ°å¾ˆé«˜

### ä¸‰ã€å• Worker å·²ç»åœ¨åšä»€ä¹ˆï¼Ÿ

åœ¨ä¸€ä¸ª vLLM worker å†…éƒ¨ï¼š

- æ‰€æœ‰æ´»è·ƒè¯·æ±‚å…±äº«ä¸€ä¸ª Scheduler
- Scheduler åŠ¨æ€ç»„åˆ token batch
- åˆ©ç”¨ KV Cache é¿å…é‡å¤è®¡ç®—
- Decode é˜¶æ®µæ¯ä¸ªè¯·æ±‚æ¯æ¬¡åªç”Ÿæˆ 1 ä¸ª token

ç»“æœæ˜¯ï¼š

```
ä¸€ä¸ª worker â‰  ä¸€ä¸ªè¯·æ±‚
ä¸€ä¸ª worker â‰ˆ å¤šè¯·æ±‚çš„ token æµæ°´çº¿æ‰§è¡Œå™¨
```

åœ¨ Decode ä¸ºä¸»çš„åœºæ™¯ä¸‹ï¼ˆç»å¤§å¤šæ•°åœ¨çº¿æœåŠ¡ï¼‰ï¼š

> **å• worker å°±å·²ç»å¯ä»¥é«˜æ•ˆæœåŠ¡å‡ åç”šè‡³ä¸Šç™¾ä¸ªå¹¶å‘è¯·æ±‚**

### å››ã€å¤š Worker ä¸ºä»€ä¹ˆä¸ä¸€å®šæ›´å¿«ï¼Ÿ

4.1 æ˜¾å­˜è¢«å¤åˆ¶ï¼Œè€Œä¸æ˜¯å…±äº«

æ¯ä¸ª worker é€šå¸¸æ˜¯ä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹ï¼š

- æ¯ä¸ª worker ä¼šï¼š
  - åŠ è½½ä¸€ä»½æ¨¡å‹æƒé‡ï¼ˆæˆ–è‡³å°‘ä¸€éƒ¨åˆ†ï¼‰
  - ç»´æŠ¤ä¸€å¥—ç‹¬ç«‹çš„ KV Cache

ç»“æœæ˜¯ï¼š

```
Worker Ã— 2 â†’ æ˜¾å­˜å ç”¨ â‰ˆ Ã— 2
```

è¿™ä¼šç›´æ¥å¸¦æ¥ä¸¤ä¸ªé—®é¢˜ï¼š

1. å•ä¸ª worker å¯ç”¨ KV Cache å˜å°‘
2. Scheduler å¯è°ƒåº¦çš„ token æ•°ä¸‹é™

ğŸ‘‰ **åè€Œé™ä½ååä¸Šé™**

4.2 GPU è®¡ç®—è¢«äººä¸ºåˆ‡åˆ†

GPU åœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ•ˆç‡æœ€é«˜ï¼š

- å¤§ batch
- è¿ç»­ kernel
- å°‘ä¸Šä¸‹æ–‡åˆ‡æ¢

å¤š worker ä¼šå¯¼è‡´ï¼š

- æ¯ä¸ª worker æäº¤æ›´å°çš„ batch
- GPU åœ¨å¤šä¸ª CUDA stream / context ä¹‹é—´åˆ‡æ¢

ç»“æœï¼š

> **GPU åˆ©ç”¨ç‡ä¸‹é™**

4.3 Scheduler è¢«â€œç¢ç‰‡åŒ–â€

vLLM çš„ Scheduler éå¸¸å…³é”®ï¼š

- å®ƒéœ€è¦çœ‹åˆ°â€œå…¨å±€æ´»è·ƒè¯·æ±‚â€
- æ‰èƒ½åšæœ€ä¼˜ token çº§è°ƒåº¦

å¤šä¸ª worker æ„å‘³ç€ï¼š

```
Scheduler Ã— N
```

æ¯ä¸ª Scheduler åªèƒ½çœ‹åˆ°ï¼š

- è‡ªå·± worker å†…çš„ä¸€éƒ¨åˆ†è¯·æ±‚

è¿™ä¼šå¯¼è‡´ï¼š

- batch éš¾ä»¥åšå¤§
- è¯·æ±‚å…¬å¹³æ€§å˜å·®
- çŸ­è¯·æ±‚å¯èƒ½è¢«å»¶è¿Ÿ

### äº”ã€ä»€ä¹ˆæ—¶å€™å¤š Worker æ‰æ˜¯åˆç†çš„ï¼Ÿ

â€œå¤š worker ä¸ä¸€å®šæ›´å¿«â€å¹¶ä¸ç­‰äºâ€œå¤š worker æ²¡ç”¨â€ã€‚

ä»¥ä¸‹åœºæ™¯ä¸­ï¼Œå¤š worker æ˜¯åˆç†ç”šè‡³å¿…è¦çš„ï¼š

5.1 å¤š GPU åœºæ™¯

æœ€å¸¸è§ã€ä¹Ÿæ˜¯æœ€æ¨èçš„æ–¹å¼ï¼š

```
1 GPU = 1 worker
```

- æ¯ä¸ª worker ç»‘å®šä¸€å¼  GPU
- é¿å… GPU ç«äº‰
- Scheduler åœ¨å• GPU å†…ä»ä¿æŒå…¨å±€è§†è§’

5.2 CPU æˆä¸ºç“¶é¢ˆ

åœ¨ä»¥ä¸‹æƒ…å†µä¸‹ï¼š

- è¯·æ±‚è§£æ
- Tokenization
- ç½‘ç»œ IO
- Python GIL

æˆä¸ºç“¶é¢ˆæ—¶ï¼Œå¤š worker å¯ä»¥ï¼š

- åˆ†æ‘Š CPU å‹åŠ›
- æé«˜æ•´ä½“åå

5.3 å¼ºéš”ç¦»éœ€æ±‚

ä¾‹å¦‚ï¼š

- ä¸åŒç”¨æˆ· / ä¸åŒæ¨¡å‹
- ä¸åŒä¼˜å…ˆçº§
- é˜²æ­¢å•ä¸ªè¯·æ±‚æ‹–å®å…¨å±€

æ­¤æ—¶ï¼š

> **ç‰ºç‰²éƒ¨åˆ†ååï¼Œæ¢å–ç¨³å®šæ€§**

### å…­ã€ä¸€ä¸ªå®ç”¨çš„ç»éªŒç»“è®º

åœ¨ vLLM ä¸­ï¼Œä¸€ä¸ªéå¸¸å®ç”¨çš„ç»éªŒæ³•åˆ™æ˜¯ï¼š

> **åœ¨å• GPU ä¸Šï¼Œä¼˜å…ˆç”¨ä¸€ä¸ª workerï¼Œ
>  æŠŠ KV Cache å’Œ batch åšå¤§ï¼Œ
>  è€Œä¸æ˜¯ç›²ç›®å¢åŠ  worker æ•°ã€‚**

å¦‚æœååä¸å¤Ÿï¼Œä¼˜å…ˆè€ƒè™‘ï¼š

1. å¢å¤§ max_num_batched_tokens
2. è°ƒæ•´ block size
3. ä½¿ç”¨æ›´å¤§çš„æ˜¾å­˜
4. å¢åŠ  GPU æ•°é‡

è€Œä¸æ˜¯ï¼š

```
worker = 2 â†’ 4 â†’ 8
```

### ä¸ƒã€æ€»ç»“

| ç»´åº¦           | ä¼ ç»Ÿæ¨ç†   | vLLM           |
| -------------- | ---------- | -------------- |
| å¹¶å‘å•ä½       | è¯·æ±‚       | Token          |
| è°ƒåº¦æ ¸å¿ƒ       | worker     | Scheduler      |
| å¤š worker ä½œç”¨ | æå‡å¹¶å‘   | å¯èƒ½ç¢ç‰‡åŒ–     |
| ååç“¶é¢ˆ       | CPU / çº¿ç¨‹ | KV Cache + GPU |

**vLLM çš„æ€§èƒ½ä¼˜åŒ–ï¼Œæœ¬è´¨ä¸Šä¸æ˜¯â€œå¼€æ›´å¤šè¿›ç¨‹â€ï¼Œ
 è€Œæ˜¯â€œè®© Scheduler çœ‹è§æ›´å¤š tokenâ€ã€‚**

# vLLM çš„å·¥ç¨‹å½¢æ€

è¿™ä¸€èŠ‚éå¸¸é€‚åˆæ‰¿ä¸Šå¯ä¸‹ï¼š

> ### vLLM ä¸åªæ˜¯ä¸€ä¸ªåº“ï¼Œè€Œæ˜¯ä¸€å¥—æ¨ç†æœåŠ¡ç³»ç»Ÿ
>
> å¦‚æœæŠŠ Ollama çœ‹ä½œâ€œä¸€ä¸ªèƒ½èŠå¤©çš„æœ¬åœ°å·¥å…·â€ï¼Œé‚£ vLLM æ›´åƒæ˜¯ä¸€ä¸ªçœŸæ­£çš„æ¨¡å‹æ¨ç†åç«¯â€¦â€¦



> ä»â€œæ¨ç†å¼•æ“â€åˆ°â€œå¯ç”¨æœåŠ¡â€

## ä¸€ã€ä¸€ä¸ªéå¸¸å…³é”®çš„è®¤çŸ¥æ¾„æ¸…

å¾ˆå¤šæ–°æ‰‹ä¼šé—®ï¼š

> â€œvLLM æ˜¯ä¸æ˜¯ä¸€ä¸ª Python åº“ï¼Ÿâ€

**è¿™å¥è¯åªå¯¹äº†ä¸€åŠã€‚**

æ›´å‡†ç¡®çš„è¯´æ³•æ˜¯ï¼š

> **vLLM = æ¨ç†å¼•æ“ + è°ƒåº¦ç³»ç»Ÿ + æœåŠ¡æ¥å£**

ä½ æ—¢å¯ä»¥ï¼š

- æŠŠå®ƒå½“ **Python åº“**
- ä¹Ÿå¯ä»¥ï¼š
- æŠŠå®ƒå½“ **ä¸€ä¸ªæ¨¡å‹æœåŠ¡ç³»ç»Ÿ**

è¿™ä¸¤ç§ç”¨æ³•ï¼Œå¯¹åº”çš„æ˜¯**ä¸¤ç§å®Œå…¨ä¸åŒçš„å¿ƒæ™ºæ¨¡å‹**ã€‚

------

## äºŒã€vLLM çš„ä¸¤ç§â€œå­˜åœ¨æ–¹å¼â€

### 1. æ–¹å¼ä¸€ï¼šä½œä¸º Python æ¨ç†å¼•æ“ï¼ˆç ”ç©¶ / å†…åµŒï¼‰

```
from vllm import LLM

llm = LLM(model="Qwen/Qwen2.5-7B-Instruct")
outputs = llm.generate("ä½ å¥½")
```

è¿™ç§æ–¹å¼é€‚åˆï¼š

- å•æœºå®éªŒ
- ç®—æ³•ç ”ç©¶
- ä½ è‡ªå·±åœ¨ä»£ç é‡Œè°ƒç”¨

**ä½†æ³¨æ„ï¼š**

- è¿™ä¸æ˜¯ vLLM çš„â€œä¸»æˆ˜åœºâ€
- å®ƒçœŸæ­£çš„ä»·å€¼æ²¡æœ‰å®Œå…¨å‘æŒ¥å‡ºæ¥

------

### 2. æ–¹å¼äºŒï¼ˆé‡ç‚¹ï¼‰ï¼šä½œä¸ºâ€œæ¨¡å‹æ¨ç†æœåŠ¡â€

è¿™æ˜¯ vLLM çš„**æ ¸å¿ƒä½¿ç”¨æ–¹å¼**ï¼š

```
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct
```

è¿™ä¸€æ¡å‘½ä»¤ï¼Œæœ¬è´¨ä¸Šæ˜¯åœ¨è¯´ï¼š

> **â€œå¸®æˆ‘èµ·ä¸€ä¸ªé«˜å¹¶å‘çš„å¤§æ¨¡å‹ API æœåŠ¡ã€‚â€**

------

## ä¸‰ã€ä¸ºä»€ä¹ˆ vLLM è¦â€œä¼ªè£…æˆ OpenAI APIâ€ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª**éå¸¸å·¥ç¨‹ã€éå¸¸ç°å®çš„è®¾è®¡é€‰æ‹©**ã€‚

### 1. OpenAI API å·²ç»æ˜¯â€œäº‹å®æ ‡å‡†â€

- å¤§é‡å‰ç«¯ / åç«¯ä»£ç 
- LangChain / LlamaIndex / Agent æ¡†æ¶
- éƒ½å·²ç»ï¼š
  - å†™æ­»äº† OpenAI API è§„èŒƒ

å¦‚æœ vLLM å¦æä¸€å¥—ï¼š

- å‡ ä¹æ²¡äººä¼šç”¨

------

### 2. vLLM çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨â€œé‡Œé¢â€ï¼Œä¸æ˜¯â€œå¤–é¢â€

vLLM éå¸¸èªæ˜åœ°é€‰æ‹©ï¼š

- **å¤–éƒ¨ï¼š** å®Œå…¨å…¼å®¹ OpenAI API
- **å†…éƒ¨ï¼š** è‡ªå·±ç© PagedAttentionã€Block è°ƒåº¦

è¿™è®©ä½ å¯ä»¥ï¼š

- æŠŠ vLLM å½“æˆï¼š
  - â€œä¸€ä¸ªæœ¬åœ° OpenAIâ€

------

## å››ã€ä¸€ä¸ª vLLM Server å¯åŠ¨åï¼Œå†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬ç”¨â€œç»„ä»¶è§†è§’â€æ¥çœ‹ã€‚

### 1. æ¨¡å‹æƒé‡åŠ è½½

- æ¨¡å‹å‚æ•°ï¼ˆweightsï¼‰åŠ è½½è¿› GPU
- é€šå¸¸ï¼š
  - å æ˜¾å­˜çš„å¤§å¤´
  - ç›¸å¯¹å›ºå®š

------

### 2. KV Block Pool åˆå§‹åŒ–

- åœ¨å‰©ä½™æ˜¾å­˜ä¸­
- åˆ’å‡ºä¸€å—åŒºåŸŸ
- åˆ‡æˆå›ºå®šå¤§å°çš„ KV Block

**è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼š**

- å®ƒå†³å®šäº†ï¼š
  - æœ€å¤§å¹¶å‘
  - æœ€å¤§ä¸Šä¸‹æ–‡å®¹é‡

------

### 3. è¯·æ±‚è°ƒåº¦å™¨å¯åŠ¨

vLLM å†…éƒ¨æœ‰ä¸€ä¸ªè°ƒåº¦å¾ªç¯ï¼š

- æ¥æ”¶æ–°è¯·æ±‚
- åˆ¤æ–­ï¼š
  - æœ‰æ²¡æœ‰è¶³å¤Ÿ Block
- åˆå¹¶å¤šä¸ªè¯·æ±‚
- ç»„æˆ batch
- ä¸€èµ·é€è¿› GPU

**è¿™ä¸€æ­¥å†³å®šååé‡ã€‚**

------

### 4. Streaming ä¸è¯·æ±‚ç”Ÿå‘½å‘¨æœŸç®¡ç†

- è¯·æ±‚ç”Ÿæˆä¸€ä¸ª token
- KV Cache å¢é•¿
- Block ä¸å¤Ÿå°±åˆ†é…
- è¯·æ±‚ç»“æŸå°±å›æ”¶ Block

**è¿™å°±æ˜¯ vLLM çš„â€œæ“ä½œç³»ç»Ÿæ„Ÿâ€ã€‚**

------

## äº”ã€ä½ ç°åœ¨å¯ä»¥ç†è§£è¿™äº›å‚æ•°â€œåœ¨å¹²å˜›â€äº†

å“ªæ€•ä½ è¿˜æ²¡ç”¨è¿‡ vLLMï¼Œä½ å·²ç»èƒ½ç†è§£ä¸‹é¢è¿™äº›å‚æ•°çš„**æœ¬è´¨å«ä¹‰**ï¼š

- `--max-model-len`
  - é™åˆ¶å•è¯·æ±‚æœ€å¤šç”¨å¤šå°‘ KV Block
- `--gpu-memory-utilization`
  - å†³å®š Block Pool å ç”¨å¤šå°‘æ˜¾å­˜
- `--max-num-seqs`
  - åŒæ—¶èƒ½æœ‰å¤šå°‘æ´»è·ƒè¯·æ±‚

å®ƒä»¬æœ¬è´¨ä¸Šéƒ½åœ¨å›ç­”ä¸€ä¸ªé—®é¢˜ï¼š

> **â€œBlock æ€ä¹ˆåˆ†ï¼Ÿâ€**

------

## å…­ã€å’Œ Ollama çš„â€œä½¿ç”¨ä½“éªŒå·®å¼‚â€æ¥æºäºå“ªï¼Ÿ

ä½ ç°åœ¨å¯ä»¥ä»ç³»ç»Ÿè§’åº¦è§£é‡Šäº†ï¼š

| ç»´åº¦     | Ollama | vLLM   |
| -------- | ------ | ------ |
| é¢å‘å¯¹è±¡ | ä¸ªäºº   | æœåŠ¡   |
| å¹¶å‘     | ä½     | é«˜     |
| KV Cache | è¿ç»­   | åˆ†é¡µ   |
| è°ƒåº¦     | ç®€å•   | å¤æ‚   |
| ä½¿ç”¨é—¨æ§› | æä½   | åå·¥ç¨‹ |

è¿™ä¸æ˜¯â€œè°å¥½è°åâ€ï¼Œè€Œæ˜¯**ç³»ç»Ÿå®šä½çš„å¿…ç„¶ç»“æœ**ã€‚

# ç¬¬ä¸€æ¬¡æ­£ç¡®åœ°è·‘ vLLM

> ä¸æ˜¯â€œå¯åŠ¨æˆåŠŸâ€ï¼Œè€Œæ˜¯â€œå¯åŠ¨å¾—æ˜ç™½â€

è¿™ä¸€èŠ‚éå¸¸é€‚åˆå†™æˆï¼š

> ### ç¬¬ä¸€æ¬¡å¯åŠ¨ vLLMï¼šä½ åˆ°åº•åœ¨é…ç½®ä»€ä¹ˆï¼Ÿ
>
> vLLM çš„å¯åŠ¨å‚æ•°çœ‹èµ·æ¥å¾ˆå¤šï¼Œä½†å¦‚æœç†è§£äº†å®ƒçš„æ˜¾å­˜æ¨¡å‹ï¼Œä¼šå‘ç°æ‰€æœ‰å‚æ•°éƒ½å›´ç»•ç€ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šKV Cache Block åº”è¯¥å¦‚ä½•åˆ†é…â€¦â€¦



## ä¸€ã€å¯åŠ¨ vLLM å‰ï¼Œå…ˆå»ºç«‹ä¸€ä¸ªâ€œå¿ƒç†æ¨¡å‹â€

åœ¨ä½ æ•²ä»»ä½•å‘½ä»¤ä¹‹å‰ï¼Œè¯·åœ¨è„‘ä¸­ä¿ç•™ä¸‹é¢è¿™å¼ å›¾ï¼š

```
GPU æ˜¾å­˜
â”œâ”€ æ¨¡å‹æƒé‡ï¼ˆå›ºå®šï¼‰
â””â”€ KV Cache Block Poolï¼ˆåŠ¨æ€ï¼‰
      â”œâ”€ è¯·æ±‚ A çš„ Block
      â”œâ”€ è¯·æ±‚ B çš„ Block
      â””â”€ è¯·æ±‚ C çš„ Block
```

**vLLM å¯åŠ¨æ—¶ï¼Œä½ è¦åšçš„äº‹æƒ…æœ¬è´¨åªæœ‰ä¸‰ä»¶ï¼š**

1. æŠŠæ¨¡å‹æƒé‡è£…è¿› GPU
2. ç»™ KV Cache ç•™å‡ºä¸€å—â€œå¯è°ƒåº¦çš„æ˜¾å­˜æ± â€
3. é™åˆ¶â€œå•è¯·æ±‚â€å’Œâ€œå¹¶å‘è¯·æ±‚â€å¯¹è¿™ä¸ªæ± å­çš„ä½¿ç”¨æ–¹å¼

æ‰€æœ‰å‚æ•°ï¼Œéƒ½æ˜¯å›´ç»•è¿™ä¸‰ç‚¹å±•å¼€çš„ã€‚

------

## äºŒã€æœ€å°å¯ç†è§£å¯åŠ¨å‘½ä»¤ï¼ˆæ¨èç¬¬ä¸€æ¬¡å°±ç”¨ï¼‰

æˆ‘ä»¬å…ˆç»™å‡ºä¸€æ¡**â€œä¸è¿½æ±‚æè‡´æ€§èƒ½ï¼Œä½†å‡ ä¹ä¸å¯èƒ½è¸©é›·â€**çš„å‘½ä»¤ï¼š

```
python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --dtype float16 \
  --gpu-memory-utilization 0.85 \
  --max-model-len 4096
```

ç°åœ¨æˆ‘ä»¬**é€è¡Œæ‹†è§£**ã€‚

------

## ä¸‰ã€é€ä¸ªå‚æ•°â€œæ‹†åˆ°éª¨å¤´é‡Œâ€

### 1. `--model`

```
--model Qwen/Qwen2.5-7B-Instruct
```

å«ä¹‰ä¸æ˜¯ï¼š

> â€œæˆ‘è¦ç”¨å“ªä¸ªæ¨¡å‹â€

è€Œæ˜¯ï¼š

> **â€œæˆ‘è¦åŠ è½½å“ªä¸€ä»½æƒé‡ï¼Œå¹¶æ®æ­¤å†³å®šæ¨¡å‹ç»“æ„ã€å±‚æ•°ã€KV å½¢çŠ¶â€**

è¿™ä¸€æ­¥ç›´æ¥å†³å®šï¼š

- æƒé‡å å¤šå°‘æ˜¾å­˜
- æ¯ä¸ª token çš„ KV Cache å¤šå¤§

------

### 2. `--dtype float16`

```
--dtype float16
```

ä½ å¯ä»¥è¿™æ ·ç†è§£ï¼š

- æ¨¡å‹æƒé‡ï¼š
  - FP16 / BF16 / INT8 â€¦
- KV Cacheï¼š
  - **æ°¸è¿œè·Ÿç€ dtype èµ°**

æ‰€ä»¥ dtype ä¸åªæ˜¯ç²¾åº¦é—®é¢˜ï¼Œè€Œæ˜¯ï¼š

> **â€œæ¯ä¸ª KV Block å å¤šå°‘æ˜¾å­˜â€**

å¯¹æ–°æ‰‹æ¥è¯´ï¼š

- FP16 æ˜¯æœ€ç¨³å¦¥çš„é€‰æ‹©

------

### 3. `--gpu-memory-utilization 0.85`

è¿™æ˜¯ **vLLM æœ€â€œçµé­‚â€çš„å‚æ•°ä¹‹ä¸€**ã€‚

å«ä¹‰æ˜¯ï¼š

> **â€œGPU æ€»æ˜¾å­˜ä¸­ï¼Œæœ‰å¤šå°‘æ¯”ä¾‹å¯ä»¥è¢« vLLM ç”¨æ¥æ”¾ä¸œè¥¿â€**

ä½†æ›´ç²¾ç¡®åœ°è¯´ï¼š

- å…ˆæ”¾æ¨¡å‹æƒé‡
- å‰©ä¸‹çš„éƒ¨åˆ†
- å†æŒ‰è¿™ä¸ªæ¯”ä¾‹åˆ’ç»™ KV Block Pool

ä¸ºä»€ä¹ˆä¸æ˜¯ 1.0ï¼Ÿ

- CUDA context
- kernel workspace
- ç¢ç‰‡å®‰å…¨è¾¹ç•Œ

**ç»éªŒç»“è®ºï¼š**

- æ–°æ‰‹ï¼š0.8 â€“ 0.9
- ç”Ÿäº§ç¯å¢ƒæ‰ä¼šç²¾ç»†è°ƒ

------

### 4. `--max-model-len 4096`

è¿™ä¸ªå‚æ•°**æå…¶å®¹æ˜“è¢«è¯¯è§£**ã€‚

å®ƒä¸æ˜¯ï¼š

> â€œæˆ‘ä¸€æ¬¡æœ€å¤šè¾“å…¥ 4096 tokenâ€

è€Œæ˜¯ï¼š

> **â€œå•ä¸ªè¯·æ±‚æœ€å¤šå…è®¸å ç”¨å¤šå°‘ KV Cacheâ€**

ä¹Ÿå°±æ˜¯ï¼š

- ä¸€ä¸ªè¯·æ±‚
- æœ€å¤šèƒ½æ‹¿å¤šå°‘ Block

**å®ƒæ˜¯â€œå•è¯·æ±‚çš„ä¸Šé™ä¿æŠ¤â€ã€‚**

------

## å››ã€å¯åŠ¨æˆåŠŸ â‰  ç³»ç»Ÿå¥åº·

å½“ä½ çœ‹åˆ°ï¼š

```
Uvicorn running on http://0.0.0.0:8000
```

è¿™åªè¯´æ˜ï¼š

- HTTP Server æ´»ç€

ä½†ä½ è¿˜éœ€è¦**ç¡®è®¤ä¸‰ä»¶äº‹**ã€‚

------

### 1. æ¨¡å‹æ˜¯å¦çœŸçš„åœ¨ GPU ä¸Šï¼Ÿ

çœ‹æ—¥å¿—ä¸­æ˜¯å¦æœ‰ç±»ä¼¼ï¼š

```
Loading model weights to GPU
```

ä»¥åŠï¼š

- `nvidia-smi` æ˜¾å­˜æ˜¯å¦æ˜¾è‘—ä¸Šå‡

------

### 2. KV Cache æ˜¯å¦æˆåŠŸåˆå§‹åŒ–ï¼Ÿ

æ—¥å¿—ä¸­é€šå¸¸ä¼šæœ‰ç±»ä¼¼ä¿¡æ¯ï¼ˆè¡¨è¾¾ä¸åŒï¼‰ï¼š

- åˆå§‹åŒ– KV cache
- åˆ†é… block æ•°é‡
- å¯æ”¯æŒæœ€å¤§ token æ€»é‡

è¿™æ˜¯åˆ¤æ–­ï¼š

- **å¹¶å‘èƒ½åŠ›çš„ç›´æ¥æŒ‡æ ‡**

------

### 3. æ²¡æœ‰â€œéšæ€§ OOMâ€

æœ‰äº›æƒ…å†µä¸‹ï¼š

- å¯åŠ¨æˆåŠŸ
- ç¬¬ä¸€ä¸ªè¯·æ±‚ç›´æ¥ OOM

é€šå¸¸åŸå› ï¼š

- `gpu-memory-utilization` è®¾å¤ªé«˜
- `max-model-len` å¤ªæ¿€è¿›

------

## äº”ã€ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šç”¨æœ€ç®€å•çš„æ–¹å¼éªŒè¯ç³»ç»Ÿ

ä½ ç¬¬ä¸€æ¬¡**ä¸è¦ç”¨ LangChain / Agent / Web UI**ã€‚

ç›´æ¥ç”¨æœ€åŸå§‹çš„æ–¹å¼ï¼š

```
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ]
  }'
```

ä½ æ­¤æ—¶è¦è§‚å¯Ÿçš„ä¸æ˜¯â€œå›å¤å¥½ä¸å¥½â€ï¼Œè€Œæ˜¯ï¼š

- æ˜¯å¦è¿”å›
- å»¶è¿Ÿæ˜¯å¦åˆç†
- GPU æ˜¯å¦åœ¨å·¥ä½œ

------

## å…­ã€ä½ ç°åœ¨å·²ç»å®Œæˆäº†ä¸€ä»¶å¾ˆé‡è¦çš„äº‹

ä½ ç°åœ¨ä¸æ˜¯ï¼š

- â€œè·Ÿç€æ•™ç¨‹æŠŠ vLLM è·‘èµ·æ¥çš„äººâ€

è€Œæ˜¯ï¼š

> **â€œçŸ¥é“ vLLM åœ¨å¯åŠ¨æ—¶åˆ°åº•åšäº†å“ªäº›èµ„æºå†³ç­–çš„äººâ€**

è¿™ä¼šåœ¨ä½ åé¢è°ƒå‚æ•°ã€å†™åšå®¢æ—¶ï¼Œäº§ç”Ÿ**å‹å€’æ€§ä¼˜åŠ¿**ã€‚







# åœ¨çœŸå®æœåŠ¡å™¨ä¸Šéƒ¨ç½² vLLMï¼šä»è¸©å‘åˆ°ç¨³å®šè¿è¡Œ

Phase 3ï¼švLLM çš„ä½¿ç”¨æ–¹å¼ä¸å·¥ç¨‹å®è·µ

ç»“åˆä½ çœŸå®åœºæ™¯ï¼š

- AutoDL
- ç³»ç»Ÿç›˜ / æ•°æ®ç›˜
- æ¨¡å‹æƒé‡æ”¾å“ªé‡Œ
- OpenAI-compatible API çš„å®é™…å«ä¹‰

å¥½ï¼Œè¿™ä¸€æ­¥**éå¸¸å…³é”®**ã€‚
 ä»è¿™ä¸€åˆ»å¼€å§‹ï¼Œä½ ä¸å†æ˜¯åœ¨â€œå­¦ vLLMâ€ï¼Œè€Œæ˜¯åœ¨**æŠŠå®ƒå½“æˆä¸€ä¸ªçœŸå®ç³»ç»Ÿæ¥éƒ¨ç½²å’Œç»´æŠ¤**ã€‚è¿™ä¸€æ­¥åšå¯¹äº†ï¼Œä½ åé¢å†™åšå®¢ä¼šéå¸¸æœ‰åº•æ°”ã€‚

Phase 6 Â· ç»“åˆçœŸå®ç¯å¢ƒéƒ¨ç½² vLLM

> ä»â€œèƒ½è·‘â€åˆ°â€œé•¿æœŸå¯ç”¨ã€å¯ç»´æŠ¤â€

æˆ‘ä¼šä»¥ä½ **å½“å‰çœŸå®ç¯å¢ƒä¸ºé»˜è®¤èƒŒæ™¯**æ¥è®²ï¼Œè€Œä¸æ˜¯æŠ½è±¡æœåŠ¡å™¨ï¼š

- äº‘ GPU / AutoDL
- **ç³»ç»Ÿç›˜å°ï¼ˆ30Gï¼‰**
- **æ•°æ®ç›˜å¤§**
- éœ€è¦é•¿æœŸè·‘ vLLM æœåŠ¡
- æœªæ¥å¯èƒ½è·‘å¤šä¸ªæ¨¡å‹

------

## ä¸€ã€å…ˆç»™ä½ ä¸€ä¸ªâ€œå·¥ç¨‹çº§è®¤çŸ¥ç»“è®ºâ€

> **vLLM æœ¬èº«å‡ ä¹ä¸å ç£ç›˜ç©ºé—´ï¼ŒçœŸæ­£åƒç©ºé—´çš„æ˜¯ï¼š**
>
> 1. æ¨¡å‹æƒé‡
> 2. tokenizer / config
> 3. æ—¥å¿—ï¼ˆé•¿æœŸè¿è¡Œæ—¶ï¼‰

**æ‰€ä»¥ï¼š**

- vLLM ç¨‹åº â†’ ç³»ç»Ÿç›˜ âœ”
- æ¨¡å‹æƒé‡ â†’ æ•°æ®ç›˜ âœ”ï¼ˆå¿…é¡»ï¼‰

------

## äºŒã€vLLM åˆ°åº•ä¼šåœ¨ç£ç›˜ä¸Šç”¨åˆ°å“ªäº›ä¸œè¥¿ï¼Ÿ

æˆ‘ä»¬æŠŠâ€œç£ç›˜ç›¸å…³çš„ä¸œè¥¿â€æ‹†æ¸…æ¥šã€‚

### 1. vLLM æœ¬ä½“ï¼ˆå¾ˆå°ï¼‰

- Python åŒ…
- CUDA kernel
- å ç”¨ï¼šå‡ ç™¾ MB ä»¥å†…

ğŸ‘‰ **æ”¾ç³»ç»Ÿç›˜å®Œå…¨æ²¡é—®é¢˜**

------

### 2. æ¨¡å‹æƒé‡ï¼ˆçœŸæ­£çš„å¤§å¤´ï¼‰

ä»¥ Qwen / LLaMA ä¸ºä¾‹ï¼š

- 7B FP16ï¼š~14â€“15 GB
- 14B FP16ï¼š~30 GB
- å¤šæ¨¡å‹ï¼šç›´æ¥çˆ†ç³»ç»Ÿç›˜

ğŸ‘‰ **å¿…é¡»æ”¾æ•°æ®ç›˜**

------

### 3. HuggingFace ç¼“å­˜ç›®å½•ï¼ˆéšè—é›·ç‚¹ï¼‰

é»˜è®¤æƒ…å†µä¸‹ï¼š

- æ‰€æœ‰æ¨¡å‹éƒ½ä¼šä¸‹è½½åˆ°ï¼š

```
~/.cache/huggingface/
```

è€Œåœ¨äº‘æœåŠ¡å™¨ä¸Šï¼š

- `~` é€šå¸¸åœ¨ **ç³»ç»Ÿç›˜**

**è¿™å°±æ˜¯æ–°æ‰‹æœ€å®¹æ˜“ç‚¸ç›˜çš„åœ°æ–¹ã€‚**

------

## ä¸‰ã€ç¬¬ä¸€ä»¶å¿…é¡»åšçš„äº‹ï¼šè¿ç§»æ¨¡å‹ç¼“å­˜ç›®å½•

è¿™æ˜¯ä½ åœ¨ AutoDL / äº‘æœåŠ¡å™¨ä¸Šè·‘ vLLM çš„**ç¬¬ä¸€æ­¥æ­£ç¡®æ“ä½œ**ã€‚

### 1. é€‰æ‹©æ•°æ®ç›˜ä½ç½®ï¼ˆç¤ºä¾‹ï¼‰

å‡è®¾ä½ çš„æ•°æ®ç›˜æŒ‚è½½åœ¨ï¼š

```
/data
```

ä½ å¯ä»¥è§„åˆ’ï¼š

```
/data/hf_cache
/data/models
/data/logs
```

------

### 2. è®¾ç½® HuggingFace ç¼“å­˜ç¯å¢ƒå˜é‡ï¼ˆæ ¸å¿ƒï¼‰

åœ¨ **å¯åŠ¨ vLLM ä¹‹å‰**ï¼Œè®¾ç½®ï¼š

```
export HF_HOME=/data/hf_cache
export TRANSFORMERS_CACHE=/data/hf_cache
export HUGGINGFACE_HUB_CACHE=/data/hf_cache
```

è¿™ä¸€æ­¥çš„å«ä¹‰æ˜¯ï¼š

> **æ‰€æœ‰æ¨¡å‹ã€tokenizerã€configï¼Œå…¨éƒ½å»æ•°æ®ç›˜**

------

### 3.ï¼ˆå¯é€‰ä½†æ¨èï¼‰è½¯é“¾æ¥å…œåº•

å¦‚æœä½ æ‹…å¿ƒæŸäº›åº“**æ— è§†ç¯å¢ƒå˜é‡**ï¼Œå¯ä»¥å†åŠ ä¸€é“ä¿é™©ï¼š

```
mv ~/.cache/huggingface /data/hf_cache
ln -s /data/hf_cache ~/.cache/huggingface
```

> è¿™æ˜¯â€œå·¥ç¨‹è€æ‰‹çš„ä¹ æƒ¯åŠ¨ä½œâ€ã€‚

------

## å››ã€vLLM å¯åŠ¨å‘½ä»¤ï¼ˆç»“åˆçœŸå®ç¯å¢ƒç‰ˆï¼‰

æˆ‘ä»¬ç°åœ¨æŠŠâ€œçœŸå®éƒ¨ç½²ç‰ˆå¯åŠ¨å‘½ä»¤â€å®Œæ•´å†™å‡ºæ¥ï¼š

```
export HF_HOME=/data/hf_cache
export TRANSFORMERS_CACHE=/data/hf_cache
export HUGGINGFACE_HUB_CACHE=/data/hf_cache

python -m vllm.entrypoints.openai.api_server \
  --model Qwen/Qwen2.5-7B-Instruct \
  --dtype float16 \
  --gpu-memory-utilization 0.85 \
  --max-model-len 4096 \
  --host 0.0.0.0 \
  --port 8000
```

### è¿™é‡Œå¤šå‡ºæ¥çš„ä¸¤ä¸ªå‚æ•°ä¹Ÿå¾ˆé‡è¦ï¼š

- `--host 0.0.0.0`
  - å…è®¸å¤–éƒ¨è®¿é—®ï¼ˆAPI æœåŠ¡å¿…å¤‡ï¼‰
- `--port 8000`
  - å›ºå®šç«¯å£ï¼Œæ–¹ä¾¿åå‘ä»£ç† / è°ƒåº¦

------

## äº”ã€é•¿æœŸè¿è¡Œè§†è§’ï¼šä½ å¿…é¡»æå‰çŸ¥é“çš„ 3 ä»¶äº‹

### 1. vLLM æ˜¯â€œå¸¸é©»è¿›ç¨‹â€ï¼Œä¸æ˜¯â€œä¸€æ¬¡æ€§å‘½ä»¤â€

è¿™æ„å‘³ç€ï¼š

- ä½ è¦è€ƒè™‘ï¼š
  - å´©æºƒæ€ä¹ˆåŠï¼Ÿ
  - é‡å¯æ€ä¹ˆåŠï¼Ÿ
  - æ—¥å¿—æ€ä¹ˆçœ‹ï¼Ÿ

åé¢ä½ ä¸€å®šä¼šèµ°å‘ï¼š

- `tmux`
- `systemd`
- æˆ–å®¹å™¨åŒ–

------

### 2. æ¨¡å‹ä¸€æ—¦åŠ è½½ï¼Œå ç”¨æ˜¾å­˜æ˜¯â€œé”æ­»â€çš„

- vLLM ä¸ä¼šè‡ªåŠ¨å¸è½½æ¨¡å‹
- æ˜¾å­˜å°±æ˜¯è¢«å®ƒé•¿æœŸå ç€

æ‰€ä»¥ï¼š

- **ä¸€å¼ å¡è·‘ä¸€ä¸ª vLLM Server** æ˜¯å¸¸è§è®¾è®¡

------

### 3. vLLM çš„â€œæ‰©å±•æ–¹å¼â€ä¸æ˜¯å¤šå¼€æ¨¡å‹ï¼Œè€Œæ˜¯ï¼š

- å¢åŠ å¹¶å‘
- æé«˜åå
- æˆ–å¤š GPU / å¤šå®ä¾‹

------

## å…­ã€ä½ å·²ç»ç«™åœ¨â€œå·¥ç¨‹éƒ¨ç½²â€çš„é—¨å£äº†

åˆ°è¿™é‡Œï¼Œä½ å·²ç»ï¼š

- âœ… çœŸæ­£ç†è§£ vLLM çš„è®¾è®¡åŠ¨æœº
- âœ… ç†è§£å®ƒå’Œ Ollama çš„æœ¬è´¨å·®å¼‚
- âœ… çŸ¥é“å®ƒåœ¨ç£ç›˜å’Œæ˜¾å­˜ä¸Šå¦‚ä½•â€œåƒèµ„æºâ€
- âœ… èƒ½åœ¨çœŸå®äº‘æœåŠ¡å™¨ä¸Š**æ­£ç¡®å¯åŠ¨**

**è¿™å·²ç»è¶…è¿‡ 90% çš„ vLLM ä½¿ç”¨è€…ã€‚**

------

## ä¸ƒã€è¿™ä¸€èŠ‚åœ¨åšå®¢é‡Œæ€ä¹ˆå†™ï¼Ÿ

è¿™ä¸€éƒ¨åˆ†éå¸¸é€‚åˆå†™æˆâ€œè¸©å‘æ€»ç»“å‹ç« èŠ‚â€ï¼Œè¯»è€…æçˆ±çœ‹ï¼š

> ### åœ¨äº‘æœåŠ¡å™¨ä¸Šéƒ¨ç½² vLLMï¼šä¸ºä»€ä¹ˆç³»ç»Ÿç›˜æ€»æ˜¯ä¸å¤Ÿç”¨ï¼Ÿ
>
> vLLM æœ¬èº«å¹¶ä¸å¤§ï¼Œä½†æ¨¡å‹æƒé‡å’Œ HuggingFace çš„é»˜è®¤ç¼“å­˜ç­–ç•¥ï¼Œå¾ˆå®¹æ˜“åœ¨äº‘æœåŠ¡å™¨ä¸ŠæŠŠç³»ç»Ÿç›˜æ’‘çˆ†â€¦â€¦

# å¹¶å‘ä¸ååï¼šä¸ºä»€ä¹ˆâ€œèƒ½è·‘â€å’Œâ€œèƒ½ç”¨â€æ˜¯ä¸¤å›äº‹

åœ¨çœŸå®ç¯å¢ƒä¸­ï¼Œå¤§æ¨¡å‹éƒ¨ç½²æœ€å®¹æ˜“è¢«æ–°æ‰‹å¿½ç•¥ã€ä½†æœ€å…ˆæŠŠç³»ç»Ÿæ‹–å®çš„ï¼Œåªæœ‰ä¸¤ä¸ªé—®é¢˜ï¼š

- **å¹¶å‘ï¼ˆConcurrencyï¼‰**
- **ååï¼ˆThroughputï¼‰**

å¾ˆå¤šäººç¬¬ä¸€æ¬¡è·‘é€š Ollama æˆ– vLLM æ—¶ï¼Œä¼šäº§ç”Ÿä¸€ä¸ªé”™è§‰ï¼š

> â€œæ¨¡å‹èƒ½å›ç­”é—®é¢˜äº†ï¼Œçœ‹èµ·æ¥æŒºå¿«çš„ï¼Œåº”è¯¥æ²¡ä»€ä¹ˆé—®é¢˜ã€‚â€

è¿™ä¸ªåˆ¤æ–­åœ¨**å•äººã€æœ¬åœ°ã€ä½é¢‘æµ‹è¯•**æ—¶æ˜¯æˆç«‹çš„ï¼›
 ä½†ä¸€æ—¦è¿›å…¥**çœŸå®ç¯å¢ƒ**ï¼Œå®ƒå‡ ä¹ä¸€å®šæ˜¯é”™çš„ã€‚

------

## ä¸€ã€ä»€ä¹ˆæ˜¯â€œçœŸå®ç¯å¢ƒâ€ï¼Ÿ

å…ˆæ˜ç¡®ä¸€ä¸ªå‰æï¼š
 **çœŸå®ç¯å¢ƒ â‰  ä½ è‡ªå·±åœ¨å‘½ä»¤è¡Œé‡Œæ•²ä¸€æ¬¡ `curl`**

çœŸå®ç¯å¢ƒé€šå¸¸æ„å‘³ç€è‡³å°‘æ»¡è¶³ä»¥ä¸‹ä»»æ„ä¸€æ¡ï¼š

- Web æœåŠ¡ï¼ˆFastAPI / Flask / Spring / Nodeï¼‰
- å¤šç”¨æˆ·ï¼ˆå“ªæ€•åªæœ‰ 5ï½10 ä¸ªäººï¼‰
- ä¸€ä¸ªç”¨æˆ·çš„è¯·æ±‚å°šæœªç»“æŸï¼Œä¸‹ä¸€ä¸ªè¯·æ±‚å·²ç»è¿›æ¥äº†
- æ¨¡å‹è¢«ç”¨äº Agent / Tool / å¤šè½®å¯¹è¯
- æ¨ç†è¯·æ±‚æ˜¯é•¿æ–‡æœ¬ã€æµå¼è¾“å‡º

æ¢å¥è¯è¯´ï¼š

> **çœŸå®ç¯å¢ƒä¸€å®šä¼šå‡ºç°â€œå¤šä¸ªè¯·æ±‚åŒæ—¶å ç”¨æ¨¡å‹â€çš„æƒ…å†µ**

è€Œè¿™æ­£æ˜¯å¹¶å‘ä¸ååé—®é¢˜çš„æ¥æºã€‚

------

## äºŒã€Ollama åœ¨å¹¶å‘ä¸‹åˆ°åº•åšäº†ä»€ä¹ˆï¼Ÿ

æˆ‘ä»¬å…ˆä»ä½ å·²ç»æ¥è§¦è¿‡çš„ **Ollama** è¯´èµ·ã€‚

### 1. Ollama çš„è®¾è®¡ç›®æ ‡

Ollama çš„æ ¸å¿ƒå®šä½éå¸¸æ˜ç¡®ï¼š

> **è®©ä¸ªäººç”¨æˆ·â€œéå¸¸å®¹æ˜“åœ°â€åœ¨æœ¬åœ°è·‘èµ·å¤§æ¨¡å‹**

å› æ­¤å®ƒçš„è®¾è®¡é‡ç‚¹æ˜¯ï¼š

- ç®€å•å®‰è£…
- æ¨¡å‹å³æœåŠ¡ï¼ˆpull â†’ runï¼‰
- é€‚åˆå•ç”¨æˆ· / è½»é‡åœºæ™¯

è¿™ä¹Ÿç›´æ¥å†³å®šäº†å®ƒå¯¹å¹¶å‘çš„å¤„ç†æ–¹å¼ã€‚

------

### 2. Ollama çš„å¹¶å‘æ¨¡å‹ï¼ˆç›´è§‰ç‰ˆï¼‰

åœ¨ Ollama ä¸­ï¼Œä½ å¯ä»¥ç†è§£ä¸ºï¼š

- ä¸€ä¸ªæ¨¡å‹ â‰ˆ ä¸€ä¸ªæ¨ç†å®ä¾‹
- ä¸€ä¸ªè¯·æ±‚ â‰ˆ å ç”¨æ•´ä¸ªæ¨¡å‹æ¨ç†è¿‡ç¨‹
- å¤šä¸ªè¯·æ±‚ â‰ˆ æ’é˜Ÿ

å³ä½¿ä½ é…ç½®äº†ï¼š

```
OLLAMA_NUM_PARALLEL=4
```

å®ƒçš„å¹¶å‘æœ¬è´¨ä»ç„¶æ˜¯ï¼š

- **å¤š runner**
- **æ¨¡å‹çº§å¹¶è¡Œ**
- **è€Œä¸æ˜¯ token çº§å¹¶è¡Œ**

è¿™åœ¨å·¥ç¨‹ä¸Šæ„å‘³ç€ä»€ä¹ˆï¼Ÿ

> **æ¯ä¸€ä¸ªå¹¶å‘è¯·æ±‚ï¼Œå‡ ä¹éƒ½åœ¨â€œç‹¬å â€ä¸€æ¬¡å®Œæ•´çš„æ¨ç†è·¯å¾„**

------

### 3. æ–°æ‰‹æœ€å®¹æ˜“è¸©çš„å‘

ä½ å¯èƒ½ä¼šé‡åˆ°ç±»ä¼¼ç°è±¡ï¼š

- ç¬¬ 1 ä¸ªè¯·æ±‚ï¼šå“åº”å¾ˆå¿«
- ç¬¬ 2 ä¸ªè¯·æ±‚ï¼šå¼€å§‹æ˜æ˜¾å˜æ…¢
- ç¬¬ 3ã€4 ä¸ªè¯·æ±‚ï¼šç›´æ¥å¡ä½ï¼Œçœ‹èµ·æ¥â€œåƒæ­»äº†ä¸€æ ·â€

ä½†å®é™…ä¸Šï¼š

- Ollama æ²¡æ­»
- GPU æ²¡ç‚¸
- åªæ˜¯åœ¨**ä¸²è¡Œæ’é˜Ÿ**

è€Œè¿™ä¸€ç‚¹ï¼Œ**ä» API å±‚é¢å‡ ä¹æ˜¯â€œé€æ˜çš„â€**ï¼Œæ–°æ‰‹å¾ˆéš¾ç¬¬ä¸€æ—¶é—´æ„è¯†åˆ°é—®é¢˜æ ¹æºã€‚

------

## ä¸‰ã€å¹¶å‘ â‰  å¯åŠ¨å¤šä¸ªè¿›ç¨‹ï¼ˆè¿™æ˜¯å…³é”®è®¤çŸ¥ï¼‰

è¿™æ˜¯ä¸€ä¸ª**éå¸¸é‡è¦ã€ä½†å¾ˆå¤šäººä¸€å¼€å§‹éƒ½ä¼šè¯¯è§£çš„ç‚¹**ï¼š

> **å¹¶å‘ä¸æ˜¯â€œå¤šå¼€å‡ ä¸ªæ¨¡å‹è¿›ç¨‹â€å°±è§£å†³çš„**

ä¸ºä»€ä¹ˆï¼Ÿ

å› ä¸ºå¤§æ¨¡å‹æ¨ç†çœŸæ­£æ¶ˆè€—çš„æ˜¯ï¼š

- GPU æ˜¾å­˜
- GPU è®¡ç®—å•å…ƒï¼ˆSMï¼‰
- KV Cache
- Memory bandwidth

ä½ å¤šå¼€ä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼Œæ¶ˆè€—çš„æ˜¯**æˆå€çš„æ˜¾å­˜**ï¼Œè€Œä¸æ˜¯â€œæŠŠåŒä¸€ä¸ªæ¨¡å‹ç®—å¾—æ›´å¿«â€ã€‚

æ‰€ä»¥åœ¨ Ollama çš„ä¸–ç•Œé‡Œï¼š

- å¹¶å‘ = å¤šæ¨¡å‹å‰¯æœ¬
- åå = å¤šå‰¯æœ¬å åŠ 

è¿™ç§æ–¹å¼çš„ä»£ä»·æ˜¯ï¼š

- æ˜¾å­˜å ç”¨çº¿æ€§å¢é•¿
- æ¨¡å‹åŠ è½½æ—¶é—´å˜é•¿
- GPU åˆ©ç”¨ç‡å¹¶ä¸ç†æƒ³

------

## å››ã€vLLM ä¸ºä»€ä¹ˆä¼šè¢«å•ç‹¬æ‹¿å‡ºæ¥ï¼Ÿ

è¿™æ­£æ˜¯ **vLLM å­˜åœ¨çš„æ ¹æœ¬åŸå› **ã€‚

vLLM å¹¶ä¸æ˜¯â€œå¦ä¸€ä¸ªè·‘æ¨¡å‹çš„å·¥å…·â€ï¼Œè€Œæ˜¯ï¼š

> **ä¸ºâ€œé«˜å¹¶å‘ã€é«˜ååæ¨ç†â€è€Œç”Ÿçš„æ¨ç†å¼•æ“**

ä½ å¯ä»¥æŠŠå®ƒç†è§£ä¸ºï¼š

> **â€œæ¨ç†ä¾§çš„ CUDA ä¼˜åŒ– + è°ƒåº¦ç³»ç»Ÿâ€**

------

## äº”ã€vLLM çš„æ ¸å¿ƒæ€æƒ³ï¼ˆä¸è®²è®ºæ–‡ç‰ˆï¼‰

å¦‚æœåªç”¨ä¸€å¥è¯è§£é‡Š vLLMï¼š

> **å®ƒè®©å¤šä¸ªè¯·æ±‚â€œå…±äº«ä¸€æ¬¡æ¨¡å‹å‰å‘è®¡ç®—â€ï¼Œè€Œä¸æ˜¯äº’ç›¸æ’é˜Ÿ**

å…³é”®ç‚¹åœ¨äºä¸¤ä¸ªè¯ï¼š

### 1. Token çº§å¹¶å‘ï¼ˆè€Œä¸æ˜¯è¯·æ±‚çº§ï¼‰

åœ¨ vLLM ä¸­ï¼š

- å¤šä¸ªè¯·æ±‚çš„ token ä¼šè¢«**åŠ¨æ€æ‹¼æ¥**
- GPU ä¸€æ¬¡ forward å¯ä»¥å¤„ç†æ¥è‡ªä¸åŒè¯·æ±‚çš„ token
- ä¸å†æ˜¯â€œä¸€ä¸ªè¯·æ±‚ç®—å®Œï¼Œå†ç®—ä¸‹ä¸€ä¸ªâ€

è¿™æ„å‘³ç€ï¼š

- GPU åˆ©ç”¨ç‡æ˜¾è‘—æé«˜
- è¯·æ±‚ä¸ä¼šé•¿æ—¶é—´é˜»å¡
- é•¿æ–‡æœ¬å’ŒçŸ­æ–‡æœ¬å¯ä»¥å…±å­˜

------

### 2. PagedAttentionï¼ˆä½ ä¸éœ€è¦æ‡‚å…¬å¼ï¼‰

ä½ åªéœ€è¦è®°ä½å®ƒè§£å†³äº†ä¸€ä¸ªç°å®é—®é¢˜ï¼š

> **KV Cache ä¸å†è¦æ±‚è¿ç»­ã€ç‹¬å çš„æ˜¾å­˜å—**

å·¥ç¨‹æ•ˆæœæ˜¯ï¼š

- æ˜¾å­˜ç¢ç‰‡æ˜¾è‘—å‡å°‘
- å¹¶å‘è¯·æ±‚æ•°é‡æå‡
- é•¿å¯¹è¯ä¸ä¼šè½»æ˜“ OOM

------

## å…­ã€çœŸå®å¹¶å‘åœºæ™¯ä¸‹çš„å·®å¼‚ï¼ˆéå¸¸ç›´è§‚ï¼‰

å‡è®¾ä¸€ä¸ªç°å®åœºæ™¯ï¼š

- æ¨¡å‹ï¼š7B
- GPUï¼š24GB
- è¯·æ±‚ï¼š10 ä¸ªç”¨æˆ·åŒæ—¶èŠå¤©
- æ¯ä¸ªè¯·æ±‚ï¼šå‡ ç™¾åˆ°ä¸Šåƒ token

### ä½¿ç”¨ Ollamaï¼š

- å®é™…ä¸Šï¼šè¯·æ±‚æ’é˜Ÿ
- è¡¨ç°ï¼šåé¢çš„ç”¨æˆ·æ„Ÿè§‰â€œæœºå™¨äººå¾ˆæ…¢â€
- GPUï¼šæ—¶è€Œ 100%ï¼Œæ—¶è€Œç©ºé—²
- ä½ ï¼šå¼€å§‹æ€€ç–‘æ˜¯ä¸æ˜¯ç½‘ç»œæˆ–ä»£ç é—®é¢˜

### ä½¿ç”¨ vLLMï¼š

- æ‰€æœ‰è¯·æ±‚åŒæ—¶æ¨è¿›
- token äº¤é”™ç”Ÿæˆ
- GPU é•¿æ—¶é—´ç»´æŒé«˜åˆ©ç”¨ç‡
- å»¶è¿Ÿç¨³å®šã€ååçº¿æ€§æå‡

è¿™å°±æ˜¯**â€œèƒ½è·‘â€ä¸â€œèƒ½ç”¨â€çš„å·®åˆ«**ã€‚

------

## ä¸ƒã€é‚£æ–°æ‰‹ç°åœ¨åˆ°åº•è¯¥æ€ä¹ˆé€‰ï¼Ÿ

ç»“åˆä½ å½“å‰çš„çŠ¶æ€ï¼ˆ**å®Œå…¨æ–°æ‰‹ + åœ¨ AutoDL / GPU æœåŠ¡å™¨ä¸Šéƒ¨ç½²**ï¼‰ï¼š

å»ºè®®è·¯çº¿éå¸¸æ¸…æ™°ï¼š

**é˜¶æ®µ 1ï¼ˆç†è§£æ¨¡å‹ + APIï¼‰ï¼š**

- ç”¨ Ollama
- å­¦ä¼š pull / run / API è°ƒç”¨
- ä¸è¿½æ±‚å¹¶å‘
- ç›®æ ‡ï¼šææ¸…æ¥šâ€œæ¨¡å‹æ˜¯æ€ä¹ˆè¢«è°ƒç”¨çš„â€

**é˜¶æ®µ 2ï¼ˆè¿›å…¥çœŸå®æœåŠ¡ï¼‰ï¼š**

- åˆ‡æ¢ vLLM
- ç”¨ OpenAI-compatible API
- ä½“éªŒå¹¶å‘ã€æµå¼ã€ååçš„å·®å¼‚
- å¼€å§‹å…³å¿ƒ GPU åˆ©ç”¨ç‡

------

## å…«ã€ä¸ºä»€ä¹ˆä½ â€œç°åœ¨é—®è¿™ä¸ªé—®é¢˜éå¸¸å¯¹â€

ä½ ç°åœ¨é—®çš„ä¸æ˜¯â€œå‚æ•°æ€ä¹ˆè°ƒâ€ï¼Œè€Œæ˜¯ï¼š

> **â€œä¸ºä»€ä¹ˆåŒæ ·æ˜¯è·‘æ¨¡å‹ï¼Œå·¥ç¨‹æ•ˆæœå·®è¿™ä¹ˆå¤šï¼Ÿâ€**

è¿™æ˜¯ä¸€ä¸ª**éå¸¸æˆç†Ÿçš„å·¥ç¨‹è§†è§’**ï¼Œè€Œä¸”ï¼š

- ä½ å·²ç»è‡ªç„¶èµ°åˆ°äº† Ollama â†’ vLLM çš„åˆ†æ°´å²­
- ä¸‹ä¸€æ­¥è®¨è®ºçš„å°±ä¼šæ˜¯ï¼š
  - æ‰¹å¤„ç†
  - max_tokens
  - prefill vs decode
  - KV cache é™åˆ¶
  - å¤šæ¨¡å‹æœåŠ¡

------

å¦‚æœä½ æ„¿æ„ï¼Œä¸‹ä¸€æ­¥æˆ‘å¯ä»¥ç»§ç»­ï¼š

- **ä»ä¸€æ¬¡ HTTP è¯·æ±‚å¼€å§‹ï¼Œå®Œæ•´æ‹†è§£ä¸€æ¬¡ vLLM æ¨ç†æµç¨‹**

å¥½ï¼æˆ‘ä»¬ä»æœ€åŸºç¡€çš„è§’åº¦æ¥è®²ï¼ŒæŠŠ**ä¸€æ¬¡ HTTP è¯·æ±‚ç»è¿‡ vLLM å®Œæ•´æ¨ç†çš„è¿‡ç¨‹**æ‹†è§£æ¸…æ¥šï¼Œè®©ä½ ä½œä¸ºæ–°æ‰‹èƒ½å½»åº•ç†è§£ï¼Œå¹¶ä¸”èƒ½åœ¨è„‘å­é‡Œå½¢æˆå·¥ç¨‹æµç¨‹å›¾ã€‚
 æˆ‘ä¼šç”¨**ç±»æ¯” + å·¥ç¨‹æœ¯è¯­ + çœŸå®åœºæ™¯**è®²ï¼Œå®Œå…¨ä¸ç»•å…¬å¼ã€‚

------

# ä¸€æ¬¡ HTTP è¯·æ±‚åˆ° vLLM æ¨ç†ï¼šå®Œæ•´æµç¨‹æ‹†è§£

## 1ï¸âƒ£ ç”¨æˆ·ä¾§ï¼šå‘èµ·è¯·æ±‚

å‡è®¾ä½ åœ¨ AutoDL ä¸Šè·‘ä¸€ä¸ª FastAPI æœåŠ¡ï¼š

```
import requests

resp = requests.post(
    "http://127.0.0.1:8000/v1/completions",
    json={
        "model": "qwen-7b-vllm",
        "prompt": "å¸®æˆ‘å†™ä¸€ä¸ª Python å‡½æ•°ï¼Œè®¡ç®—ä¸¤ä¸ªæ•°çš„æœ€å¤§å…¬çº¦æ•°",
        "max_tokens": 100
    }
)
print(resp.json())
```

**ä½ å‘å‡ºçš„ä¸œè¥¿åŒ…å«ï¼š**

- promptï¼šæ¨¡å‹è¦å›ç­”çš„å†…å®¹
- modelï¼šå‘Šè¯‰ vLLM ç”¨å“ªä¸ªæ¨¡å‹
- max_tokensï¼šé™åˆ¶ç”Ÿæˆé•¿åº¦
- å…¶ä»–å‚æ•°ï¼ˆtemperatureã€top_p ç­‰ï¼‰

è¿™ä¸ªé˜¶æ®µï¼ŒvLLM åªå½“ä½œä¸€ä¸ª HTTP Serverï¼š
 **æ¥æ”¶åˆ°è¯·æ±‚ â†’ è§£æ JSON â†’ æ„å»ºå†…éƒ¨æ¨ç†ä»»åŠ¡**

------

## 2ï¸âƒ£ vLLM æ¥æ”¶è¯·æ±‚ï¼šä»»åŠ¡é˜Ÿåˆ—åŒ–

å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ï¼š

1. **ç”Ÿæˆä¸€ä¸ª Task å¯¹è±¡**
    æ¯ä¸ª HTTP è¯·æ±‚éƒ½ä¼šç”Ÿæˆä¸€ä¸ª Taskï¼Œè¿™é‡Œé¢åŒ…å«ï¼š
   - è¯·æ±‚çš„ prompt
   - ç”Ÿæˆé…ç½®ï¼ˆmax_tokens, temperature, stop_words ç­‰ï¼‰
   - è¯·æ±‚ IDï¼ˆæ–¹ä¾¿è¿”å›ç»“æœï¼‰
2. **æ”¾å…¥ Scheduler çš„é˜Ÿåˆ—**
    vLLM æœ‰ä¸€ä¸ªè°ƒåº¦å™¨ï¼ˆSchedulerï¼‰è´Ÿè´£ï¼š
   - ä»€ä¹ˆæ—¶å€™ç»™ GPU å‘ç®—åŠ›
   - å¤šä¸ªè¯·æ±‚å¦‚ä½•å…±äº«ä¸€æ¬¡ GPU forward

**å…³é”®è®¤çŸ¥ç‚¹ï¼š**

> è¿™é‡Œå°±å¼€å§‹ä½“ç° vLLM çš„â€œå¹¶å‘ä¼˜åŠ¿â€ï¼šå¤šä¸ªè¯·æ±‚éƒ½è¿›å…¥ Schedulerï¼ŒScheduler ä¼šæ™ºèƒ½åœ°æŠŠä¸åŒè¯·æ±‚çš„ token æ‹¼åœ¨ä¸€èµ·ç”Ÿæˆã€‚

------

## 3ï¸âƒ£ Tokenizationï¼šæ–‡å­— â†’ æ•°å­—

æ¨¡å‹åªèƒ½ç†è§£æ•°å­—ï¼Œä¸ä¼šç›´æ¥ç†è§£ä¸­æ–‡/è‹±æ–‡å­—ç¬¦ä¸²ã€‚

- vLLM ä¼šæŠŠä½ çš„ prompt ç”¨ tokenizer è½¬æˆ token ID æ•°ç»„
- ä¾‹å¦‚ `"å¸®æˆ‘å†™ä¸€ä¸ªå‡½æ•°"` â†’ `[123, 456, 789...]`
- æ¯ä¸ªè¯·æ±‚éƒ½æœ‰è‡ªå·±çš„ token åˆ—è¡¨

**æ³¨æ„**ï¼š

- è¿™æ˜¯ **ä¸€æ¬¡æ€§å¤„ç† prompt**ï¼Œè¿˜æ²¡ç”Ÿæˆç­”æ¡ˆ
- ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡ç”Ÿæˆ token éƒ½ä¼šå†æ¬¡èµ° Tokenizer ç›¸å…³é€»è¾‘ï¼ˆvLLM å†…éƒ¨æœ‰ç¼“å­˜ä¼˜åŒ–ï¼‰

------

## 4ï¸âƒ£ Scheduler è°ƒåº¦ GPUï¼šToken çº§åˆ«çš„å¹¶å‘

è¿™æ˜¯ vLLM æ ¸å¿ƒï¼š

- Scheduler ä¼šæŠŠä¸åŒè¯·æ±‚çš„ token æ‹¼æ¥æˆä¸€ä¸ª batch
- è¿™ä¸ª batch ä¼šä¸€æ¬¡é€å…¥ GPU forward
- åŒæ—¶ç”Ÿæˆå¤šä¸ªè¯·æ±‚çš„ token
- GPU è®¡ç®—å®Œæˆåï¼š
  - å¾—åˆ°ä¸‹ä¸€ token çš„ logits
  - æ ¹æ® sampling ç­–ç•¥ï¼ˆgreedy / top-k / temperatureï¼‰ç”Ÿæˆ token
  - é€å›å¯¹åº”çš„è¯·æ±‚

**ç±»æ¯”**ï¼š

- Ollamaï¼šæ’é˜Ÿï¼Œä¸€ä¸ªè¯·æ±‚å ç”¨æ•´ä¸ª GPU
- vLLMï¼šæ‰€æœ‰è¯·æ±‚æ··åˆç€ç”Ÿæˆï¼Œæ¯ä¸ªè¯·æ±‚å¾—åˆ°è‡ªå·±çš„ä¸€ä»½ tokenï¼ŒGPU é«˜æ•ˆåˆ©ç”¨ç‡

------

## 5ï¸âƒ£ KV Cacheï¼šè®°å¿†ä¸­é—´çŠ¶æ€

é•¿æ–‡æœ¬ç”Ÿæˆæœ‰ä¸€ä¸ªé—®é¢˜ï¼š

- æ¨¡å‹æ¯ç”Ÿæˆä¸€ä¸ª tokenï¼Œéƒ½ä¾èµ–å‰é¢æ‰€æœ‰ token çš„ä¿¡æ¯
- naive æ–¹æ³•ï¼šæ¯æ¬¡ forward éƒ½é‡å¤è®¡ç®—å‰é¢çš„ token â†’ æ…¢ä¸”å æ˜¾å­˜

vLLM åšäº†ä¸¤ä»¶äº‹ï¼š

1. **KV Cache**
   - ä¿å­˜ key/value å‘é‡
   - ä¸‹æ¬¡ forward ç›´æ¥ç”¨ cache â†’ èŠ‚çœè®¡ç®—é‡
2. **Paged Attention**
   - å½“ cache å¤ªå¤§ã€æ˜¾å­˜æœ‰é™æ—¶ï¼Œå¯ä»¥æŠŠ cache åˆ†é¡µç®¡ç†
   - é•¿å¯¹è¯ä¸ä¼š OOM
   - å¤šä¸ªè¯·æ±‚çš„ cache äº’ä¸å¹²æ‰°

> è¿™ä¹Ÿæ˜¯ vLLM æ”¯æ’‘é«˜å¹¶å‘å’Œé•¿ä¸Šä¸‹æ–‡çš„å…³é”®æœºåˆ¶

------

## 6ï¸âƒ£ Token ç”Ÿæˆï¼šä¸€æ¬¡å¾ªç¯å°±æ˜¯ä¸€æ¬¡ forward

ç”Ÿæˆæµç¨‹ï¼š

1. Scheduler å–å½“å‰ batch
2. GPU forward â†’ å¾—åˆ° logits
3. æ ¹æ® sampling ç”Ÿæˆæ¯ä¸ªè¯·æ±‚çš„ token
4. æ¯ä¸ªè¯·æ±‚æ›´æ–°è‡ªå·±çš„ token åˆ—è¡¨å’Œ KV cache
5. å¦‚æœè¯·æ±‚æ²¡è¾¾åˆ° max_tokens â†’ å†å›åˆ° step 1

> vLLM å†…éƒ¨å…¶å®æ˜¯ä¸€ä¸ªå¾ªç¯ï¼Œæ¯æ¬¡ forward å¤„ç†**æ‰€æœ‰æ´»è·ƒè¯·æ±‚çš„ token**ï¼Œç›´åˆ°è¯·æ±‚ç»“æŸã€‚

------

## 7ï¸âƒ£ Stream è¾“å‡ºï¼ˆå¯é€‰ï¼‰

å¾ˆå¤šå®é™…å·¥ç¨‹è¦æ±‚**æµå¼è¾“å‡º**ï¼Œå³ç”¨æˆ·ä¸ç”¨ç­‰æ•´ä¸ªæ–‡æœ¬ç”Ÿæˆå®Œï¼Œå°±èƒ½çœ‹åˆ°æ¨¡å‹é€æ­¥ç”Ÿæˆã€‚

vLLM æ”¯æŒï¼š

- **é€ token stream**
- é€šè¿‡ SSE / WebSocket / HTTP chunked å‘é€ç»™å®¢æˆ·ç«¯
- Client ç«¯å¯è¾¹æ˜¾ç¤ºè¾¹å¤„ç†

Ollama é»˜è®¤æ˜¯**æ•´ä¸ªç”Ÿæˆå®Œæ‰è¿”å›**ï¼Œè¿™æ˜¯æ–°æ‰‹ç»å¸¸æ„Ÿå—â€œæ…¢â€çš„åŸå› ã€‚

------

## 8ï¸âƒ£ ç»“æŸè¯·æ±‚ï¼šè¿”å› JSON

å½“è¯·æ±‚ç”Ÿæˆå®Œæˆï¼š

- vLLM æ”¶é›†ç”Ÿæˆçš„ token
- å tokenization â†’ æ–‡å­—
- è¿”å›ç»™ HTTP clientï¼š

```
{
  "id": "req-12345",
  "object": "text_completion",
  "choices": [
    {
      "text": "def gcd(a, b):\n    while b != 0:\n        a, b = b, a % b\n    return a",
      "index": 0,
      "finish_reason": "stop"
    }
  ]
}
```

> è¿™ä¸ªç»“æ„å’Œ OpenAI API å®Œå…¨å…¼å®¹
>  æ‰€ä»¥ä½ å†™åšå®¢æˆ–é¡¹ç›®æ—¶ï¼Œä¸éœ€è¦æ”¹è°ƒç”¨æ–¹å¼ï¼Œåªæ˜¯èƒŒåä¼˜åŒ–äº†â€œå¹¶å‘ + åå + é•¿ä¸Šä¸‹æ–‡â€

------

## 9ï¸âƒ£ æ€»ç»“ vLLM æ¨ç†æµç¨‹ï¼ˆæ–°æ‰‹è§†è§’ï¼‰

ç”¨ä¸€å¥è¯ä¸²èµ·æ¥ï¼š

1. HTTP è¯·æ±‚ â†’ vLLM Server
2. Scheduler åˆ›å»º Task â†’ åŠ å…¥é˜Ÿåˆ—
3. Tokenizer â†’ prompt â†’ token ID
4. Scheduler batch å¤šè¯·æ±‚ token â†’ GPU forward
5. KV Cache ç®¡ç†å†å² token
6. Sampling â†’ æ¯ä¸ªè¯·æ±‚ç”Ÿæˆ token
7. å¾ªç¯ç›´åˆ° max_tokens / stop
8. Stream / è¿”å› JSON

**å·¥ç¨‹æ•ˆæœï¼š**

- å¤šè¯·æ±‚åŒæ—¶ç”Ÿæˆï¼Œä¸æ’é˜Ÿ
- GPU åˆ©ç”¨ç‡é«˜
- é•¿æ–‡æœ¬å¯æŒç»­ç”Ÿæˆ
- API å½¢å¼å’Œ OpenAI æ¥å£å…¼å®¹

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥**ä¸‹ä¸€æ­¥å¸®ä½ æŠŠè¿™ä¸€æ•´å¥— HTTP è¯·æ±‚æµç¨‹ç”»æˆå›¾è§£ + å¯¹æ¯” Ollama**ï¼Œè¿™æ ·ä½ åœ¨å†™æŠ€æœ¯ç¬”è®°åšå®¢æ—¶ï¼Œæ–°æ‰‹è¯»è€…ä¸€çœ‹å°±æ‡‚ï¼š

- å“ªäº›åœ°æ–¹ Ollama æ’é˜Ÿ
- å“ªäº›åœ°æ–¹ vLLM å¹¶å‘
- GPU æ˜¯æ€ä¹ˆè¢«é«˜æ•ˆåˆ©ç”¨çš„

ä½ å¸Œæœ›æˆ‘ç›´æ¥ç”»è¿™ä¸ªå›¾è§£å—ï¼Ÿ

å¥½çš„ï¼Œè¯·ç”¨æ–‡å­—æˆ–è€…ç¬¦å·ç”»å›¾ï¼Œä¸è¦ç”¨å›¾ç‰‡æ ¼å¼ç”»å›¾ã€‚

å®Œå…¨å¯ä»¥ï¼æˆ‘ç”¨**ASCII/æ–‡å­—ç”»å›¾ + æµç¨‹ç®­å¤´**æ¥è¡¨ç¤º vLLM çš„å®Œæ•´æ¨ç†æµç¨‹ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªæ­¥éª¤æ ‡æ³¨ Ollama å¯¹æ¯”ï¼Œæ–¹ä¾¿ä½ å†™åšå®¢ç¬”è®°ã€‚

## ä¸€æ¬¡ HTTP è¯·æ±‚çš„ vLLM æ¨ç†æµç¨‹å›¾ï¼ˆæ–‡å­—ç‰ˆï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ç”¨æˆ·å®¢æˆ·ç«¯             â”‚
â”‚ POST /v1/completions          â”‚
â”‚ prompt="å†™ä¸€ä¸ªGCDå‡½æ•°"        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ HTTP è¯·æ±‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          vLLM Server           â”‚
â”‚ è§£æ JSON â†’ åˆ›å»º Task           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Task åŒ…å«ï¼šprompt, max_tokens â”‚
â”‚ â”‚ request_id, sampling_config â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ å…¥é˜Ÿåˆ—
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Scheduler              â”‚
â”‚ - å°†å¤šä¸ª Task ç»„æˆ batch       â”‚
â”‚ - åŠ¨æ€è°ƒåº¦åˆ° GPU              â”‚
â”‚ - ç®¡ç† KV Cache                â”‚
â”‚ - æ”¯æŒé•¿ä¸Šä¸‹æ–‡å’Œå¹¶å‘ç”Ÿæˆ       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ token batch
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Tokenization         â”‚
â”‚ prompt â†’ token IDs             â”‚
â”‚ ä¾‹: "å†™ä¸€ä¸ªå‡½æ•°" â†’ [123,456,78]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ batch token
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           GPU Forward          â”‚
â”‚ è®¡ç®— logits                     â”‚
â”‚ æ ¹æ® sampling ç”Ÿæˆä¸‹ä¸€ä¸ª token â”‚
â”‚ æ›´æ–° KV Cache                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ token è¾“å‡º
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       å¾ªç¯ç”Ÿæˆ token           â”‚
â”‚ æ¡ä»¶ï¼šæœªè¾¾åˆ° max_tokens æˆ– stop â”‚
â”‚ Scheduler ç»§ç»­ batch forward   â”‚
â”‚ KV Cache è®°å½•ä¸Šä¸‹æ–‡            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ ç”Ÿæˆå®Œæˆ
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Stream / JSON è¾“å‡º       â”‚
â”‚ token â†’ text                   â”‚
â”‚ è¿”å›å®¢æˆ·ç«¯                     â”‚
â”‚ {"text": "def gcd(a,b)..."}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ollama å¯¹æ¯”ï¼ˆæ–°æ‰‹è§’åº¦ï¼‰

| åŠŸèƒ½é˜¶æ®µ     | Ollama         | vLLM                       |
| ------------ | -------------- | -------------------------- |
| è¯·æ±‚é˜Ÿåˆ—     | å•è¯·æ±‚å ç”¨ GPU | å¤šè¯·æ±‚ batch å¹¶å‘          |
| GPU åˆ©ç”¨ç‡   | ä½ï¼ˆæ’é˜Ÿï¼‰     | é«˜ï¼ˆæ··åˆ batchï¼‰           |
| é•¿ä¸Šä¸‹æ–‡æ”¯æŒ | åŸºæœ¬           | KV Cache + Paged Attention |
| æµå¼è¾“å‡º     | å®Œæ•´ç”Ÿæˆåè¿”å› | æ”¯æŒ token æµå¼è¾“å‡º        |
| å¹¶å‘åå     | å•çº¿ç¨‹/æ’é˜Ÿ    | å¤šè¯·æ±‚å…±äº«ä¸€æ¬¡ forward     |

------

ğŸ’¡ **æ–°æ‰‹ç†è§£å…³é”®ç‚¹**ï¼š

1. **Scheduler æ˜¯æ ¸å¿ƒ**ï¼šè´Ÿè´£å¤šè¯·æ±‚çš„æ‹¼ batchï¼Œæ™ºèƒ½åˆ†é… GPUã€‚
2. **KV Cache**ï¼šè®°å¿†ä¸­é—´çŠ¶æ€ï¼Œé•¿ä¸Šä¸‹æ–‡å’Œé«˜ååçš„ç§˜å¯†æ­¦å™¨ã€‚
3. **Loop Forward**ï¼šæ¯ä¸ª token éƒ½è¦ GPU è®¡ç®—ï¼Œä½† vLLM batch å¤šä¸ªè¯·æ±‚åŒæ—¶è®¡ç®—ã€‚
4. **æµå¼ vs ä¸€æ¬¡è¿”å›**ï¼švLLM æ”¯æŒè¾¹ç”Ÿæˆè¾¹è¾“å‡ºï¼ŒOllama ä¸€èˆ¬è¦ç”Ÿæˆå®Œæ‰è¿”å›ã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ **æŠŠè¿™å¼ æ–‡å­—æµç¨‹å›¾å‡çº§æˆä¸€ä¸ªâ€œæŠ€æœ¯ç¬”è®°åšå®¢å¯ç”¨ç‰ˆæœ¬â€**ï¼ŒåŠ ä¸Šï¼š

- æ¯æ­¥çš„å·¥ç¨‹æ„ä¹‰
- æ ¸å¿ƒæ¦‚å¿µè§£é‡Šï¼ˆSchedulerã€KV Cacheã€streamingï¼‰
- ç›´è§‚å¯¹æ¯” Ollama ä¸ vLLM

è¿™æ ·ä½ å†™åšå®¢ï¼Œæ–°æ‰‹è¯»è€…ä¸€çœ‹å°±æ‡‚æ•´ä¸ªæ¨ç†å†…éƒ¨æµç¨‹ã€‚

ä½ å¸Œæœ›æˆ‘ç›´æ¥å¸®ä½ æ•´ç†è¿™ä¸ªåšå®¢ç‰ˆæœ¬å—ï¼Ÿ

å¥½çš„

å¤ªå¥½äº†ï¼æˆ‘å¸®ä½ æ•´ç†æˆä¸€ä¸ª**vLLM æŠ€æœ¯ç¬”è®°åšå®¢ç‰ˆæœ¬**ï¼Œç»“æ„å’Œé£æ ¼å°½é‡åƒä½ ä¹‹å‰å†™ Ollama çš„åšå®¢ï¼Œé¢å‘**å¤§æ¨¡å‹æ–°æ‰‹**ï¼Œå¸¦å¯¹æ¯”å’Œé€æ­¥è§£æã€‚



## 2. ä¸€æ¬¡ HTTP è¯·æ±‚çš„ vLLM æ¨ç†æµç¨‹

å‡è®¾ç”¨æˆ·å‘é€äº†ä¸€ä¸ªç”Ÿæˆä»»åŠ¡ï¼š

```
POST /v1/completions
{
    "prompt": "å†™ä¸€ä¸ªè®¡ç®— GCD çš„ Python å‡½æ•°",
    "max_tokens": 50
}
```

vLLM çš„å†…éƒ¨å¤„ç†æµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š

```
ç”¨æˆ·å®¢æˆ·ç«¯
   â”‚ HTTP è¯·æ±‚
   â–¼
vLLM Server
   â”‚ è§£æ JSON â†’ åˆ›å»º Task
   â”‚ åŒ…å« prompt, max_tokens, request_id
   â–¼
Scheduler
   â”‚ æ‰¹å¤„ç†å¤šä¸ª Task â†’ batch forward
   â”‚ ç®¡ç† KV Cache
   â–¼
Tokenization
   â”‚ prompt â†’ token IDs
   â–¼
GPU Forward
   â”‚ è®¡ç®— logits â†’ é‡‡æ ·ä¸‹ä¸€ä¸ª token
   â”‚ æ›´æ–° KV Cache
   â–¼
å¾ªç¯ç”Ÿæˆ token
   â”‚ æœªè¾¾åˆ° max_tokens æˆ– stop æ¡ä»¶ç»§ç»­
   â–¼
Stream / JSON è¾“å‡º
   â”‚ token â†’ text â†’ è¿”å›å®¢æˆ·ç«¯
```

------

## 3. æ ¸å¿ƒæ¨¡å—è§£æï¼ˆå¯¹æ–°æ‰‹å‹å¥½ï¼‰

### 3.1 Schedulerï¼ˆè°ƒåº¦å™¨ï¼‰

- **ä½œç”¨**ï¼šæŠŠå¤šä¸ªä»»åŠ¡ç»„æˆ batchï¼Œå°½é‡æ»¡ GPUï¼Œå‡å°‘ç©ºé—²æ—¶é—´ã€‚
- **å…³é”®ç‚¹**ï¼š
  - åŠ¨æ€æ‹¼ batchï¼Œæ”¯æŒä¸åŒè¯·æ±‚é•¿åº¦ã€‚
  - ä¸ KV Cache é…åˆç®¡ç†é•¿ä¸Šä¸‹æ–‡ã€‚
- **æ–°æ‰‹ç†è§£**ï¼šScheduler å°±åƒç«è½¦è°ƒåº¦å‘˜ï¼Œå®‰æ’æ¯ä¸ªè¯·æ±‚æŒ‰æ‰¹æ¬¡ä¸Š GPU é«˜é€Ÿâ€œåˆ—è½¦â€ã€‚

------

### 3.2 KV Cacheï¼ˆKey-Value ç¼“å­˜ï¼‰

- **ä½œç”¨**ï¼šå­˜å‚¨æ¯ä¸€å±‚ transformer çš„ä¸­é—´çŠ¶æ€ã€‚
- **ä¸ºä»€ä¹ˆé‡è¦**ï¼š
  - é•¿æ–‡æœ¬æ—¶ä¸éœ€è¦æ¯æ¬¡éƒ½ä»å¤´ç®—ã€‚
  - æ”¯æŒæµå¼ç”Ÿæˆå’Œä¸Šä¸‹æ–‡è®°å¿†ã€‚
- **æ–°æ‰‹ç†è§£**ï¼šKV Cache å°±åƒè®°å¿†ç¬”è®°æœ¬ï¼Œæ¨¡å‹å·²ç»ç®—è¿‡çš„ token çŠ¶æ€ä¸ä¼šä¸¢ã€‚

------

### 3.3 Tokenizationï¼ˆåˆ†è¯ï¼‰

- å°†è‡ªç„¶è¯­è¨€ prompt è½¬æˆæ¨¡å‹å¯å¤„ç†çš„ token IDã€‚

- **ä¾‹å­**ï¼š

  ```
  "å†™ä¸€ä¸ªå‡½æ•°" â†’ [123, 456, 78]
  ```

------

### 3.4 GPU Forwardï¼ˆå‰å‘è®¡ç®—ï¼‰

- å¯¹ batch token è¿›è¡Œ transformer å‰å‘ä¼ æ’­ã€‚
- è¾“å‡ºæ¯ä¸ª token çš„ **logits**ã€‚
- æ ¹æ® sampling ç­–ç•¥ç”Ÿæˆä¸‹ä¸€ä¸ª tokenã€‚
- **å¾ªç¯ç”Ÿæˆ**ç›´åˆ°è¾¾åˆ° `max_tokens` æˆ–åœæ­¢æ¡ä»¶ã€‚

------

### 3.5 Streaming è¾“å‡º

- vLLM å¯ä»¥**è¾¹ç”Ÿæˆè¾¹è¾“å‡º token**ï¼Œå®¢æˆ·ç«¯å¯ä»¥å®æ—¶çœ‹åˆ°ç»“æœã€‚
- **å¯¹æ¯” Ollama**ï¼š
  - Ollama é€šå¸¸ç”Ÿæˆå®Œæˆæ‰è¿”å›ã€‚
  - vLLM æ”¯æŒ token æµå¼è¾“å‡ºï¼Œæ›´é€‚åˆèŠå¤©æˆ–å¤§è§„æ¨¡å¹¶å‘åœºæ™¯ã€‚

------

## 4. å¹¶å‘ä¸ååé‡ä¼˜åŠ¿

| ç‰¹æ€§         | Ollama         | vLLM                       |
| ------------ | -------------- | -------------------------- |
| è¯·æ±‚é˜Ÿåˆ—     | å•è¯·æ±‚æ’é˜Ÿ     | å¤šè¯·æ±‚ batch å¹¶å‘          |
| GPU åˆ©ç”¨ç‡   | ä½             | é«˜ï¼ˆbatch forwardï¼‰        |
| é•¿ä¸Šä¸‹æ–‡æ”¯æŒ | åŸºæœ¬           | KV Cache + Paged Attention |
| æµå¼è¾“å‡º     | å®Œæ•´ç”Ÿæˆåè¿”å› | token é€æ­¥æµå¼è¾“å‡º         |
| å¹¶å‘åå     | å•çº¿ç¨‹/æ’é˜Ÿ    | å¤šä»»åŠ¡å…±äº« GPU forward     |

> ğŸ’¡ å°ç»“ï¼švLLM æ›´é€‚åˆ **é«˜å¹¶å‘ã€å¤šç”¨æˆ·ã€å¤šä»»åŠ¡åœºæ™¯**ï¼Œæ˜¯ Ollama çš„é«˜ååé‡è¿›åŒ–ç‰ˆã€‚

------

## 5. ä»æ–°æ‰‹è§’åº¦ç†è§£ vLLM

1. **Scheduler æ˜¯æ ¸å¿ƒ**ï¼šç®¡ç† batch å’Œ KV Cacheï¼Œä¿è¯ GPU é«˜æ•ˆã€‚
2. **KV Cache æ˜¯ç§˜å¯†æ­¦å™¨**ï¼šé•¿ä¸Šä¸‹æ–‡ + æµå¼è¾“å‡ºã€‚
3. **å¾ªç¯ forward**ï¼šæ¯ä¸ª token éƒ½è¦è®¡ç®—ï¼Œä½†å¤šä»»åŠ¡å…±äº«ä¸€æ¬¡ forward æ‰¹é‡è®¡ç®—ã€‚
4. **æµå¼ vs ä¸€æ¬¡è¿”å›**ï¼švLLM å¯ä»¥è¾¹ç”Ÿæˆè¾¹è¾“å‡ºï¼Œæ›´é€‚åˆå®æ—¶åº”ç”¨ã€‚

------

## 6. å°ç»“

- vLLM é€šè¿‡ batch + KV Cache + streaming + schedulerï¼Œå®ç°äº†é«˜ååé‡ã€å¤§ä¸Šä¸‹æ–‡ã€é«˜å¹¶å‘ã€‚
- å¯¹æ¯” Ollamaï¼ŒvLLM æ›´é€‚åˆä¼ä¸šçº§åº”ç”¨å’Œå¤§è§„æ¨¡éƒ¨ç½²ã€‚
- ç†è§£ä¸€æ¬¡ HTTP è¯·æ±‚çš„å†…éƒ¨æµç¨‹ï¼Œæ˜¯å†™æŠ€æœ¯ç¬”è®°ã€åšéƒ¨ç½²å’Œä¼˜åŒ–çš„åŸºç¡€ã€‚

# å®‰è£…ä¸ç¯å¢ƒå‡†å¤‡ï¼ˆé¢å‘æ–°æ‰‹ï¼‰

vLLM æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹æ¨ç†æ¡†æ¶ï¼Œä¾èµ– Python ç¯å¢ƒå’Œ GPUã€‚å®‰è£…å‰æˆ‘ä»¬è¦æ˜ç¡®ä¸¤ä»¶äº‹ï¼š**æ¨¡å‹æ–‡ä»¶å­˜æ”¾ä½ç½®** å’Œ **è¿è¡Œç¯å¢ƒé…ç½®**ã€‚

------

## 3.1 ç³»ç»Ÿå’Œç¡¬ä»¶è¦æ±‚

- **æ“ä½œç³»ç»Ÿ**ï¼šLinuxï¼ˆæ¨è Ubuntu 20.04+ï¼‰
- **Python**ï¼š>= 3.9ï¼ˆæœ€å¥½ 3.10/3.11ï¼‰
- **CUDA / GPU**ï¼šNVIDIA GPU + CUDA Toolkitï¼ˆ11.7+ï¼‰ï¼Œæ”¯æŒ Tensor Cores
- **å†…å­˜**ï¼šè‡³å°‘ 16GB RAMï¼Œæ¨¡å‹å¤§æ—¶æ›´éœ€è¦
- **ç¡¬ç›˜ç©ºé—´**ï¼š
  - ç³»ç»Ÿç›˜ï¼šåªæ”¾ Python å’Œ vLLM åŒ…å³å¯ï¼ˆ10~20GB è¶³å¤Ÿï¼‰
  - æ•°æ®ç›˜/æ¨¡å‹ç›˜ï¼šæ”¾æƒé‡æ–‡ä»¶å’Œç¼“å­˜ï¼Œæœ€å¥½æœ‰æ•°ç™¾ GB

> âš ï¸ æç¤ºï¼šå¦‚æœç³»ç»Ÿç›˜ç©ºé—´ä¸è¶³ï¼Œæ¨¡å‹æ–‡ä»¶å¯ä»¥æ”¾åˆ°æ•°æ®ç›˜ï¼Œç„¶åç”¨ **è½¯é“¾æ¥** æˆ–åœ¨å¯åŠ¨æ—¶æŒ‡å®šè·¯å¾„ï¼Œè®© vLLM è¯»å–æ•°æ®ç›˜ã€‚

------

## 3.2 Python ç¯å¢ƒå‡†å¤‡

1. **å®‰è£… Minicondaï¼ˆæ¨èï¼‰**

   ```
   # ä¸‹è½½ Miniconda å®‰è£…è„šæœ¬
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh
   ```

   - å®‰è£…ç›®å½•å¯ä»¥é€‰æ‹©ç³»ç»Ÿç›˜ï¼ˆå°ï¼‰

   - å®Œæˆåæ‰§è¡Œï¼š

     ```
     conda init
     source ~/.bashrc
     ```

2. **åˆ›å»º vLLM ä¸“ç”¨ç¯å¢ƒ**

   ```
   conda create -n vllm python=3.10 -y
   conda activate vllm
   ```

------

## 3.3 å®‰è£… vLLM ä¸ä¾èµ–

vLLM æ”¯æŒ pip å®‰è£…ï¼Œæ¨è GPU ç‰ˆæœ¬ã€‚

```
# å®‰è£… PyTorch GPU ç‰ˆæœ¬
pip install torch --index-url https://download.pytorch.org/whl/cu118

# å®‰è£… vLLM
pip install vllm
```

> âš ï¸ æ³¨æ„ï¼š
>
> - PyTorch ç‰ˆæœ¬è¦å’Œ CUDA ç‰ˆæœ¬åŒ¹é…
> - vLLM ä¼šè‡ªåŠ¨ä¾èµ– transformersã€tokenizersã€accelerate ç­‰åº“

------

## 3.4 æ¨¡å‹æ–‡ä»¶å­˜æ”¾

vLLM é»˜è®¤ä» HuggingFace Hub ä¸‹è½½æ¨¡å‹ï¼Œä½†ä½ ä¹Ÿå¯ä»¥æå‰ä¸‹è½½åˆ°æœ¬åœ°æ•°æ®ç›˜ï¼š

```
# ä¸‹è½½æ¨¡å‹åˆ°æ•°æ®ç›˜
mkdir -p /data/models
cd /data/models
git lfs install
git clone https://huggingface.co/<model_name>
```

ç„¶åå¯åŠ¨ vLLM æ—¶æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼š

```
vllm --model /data/models/<model_name>
```

å¦‚æœæƒ³åœ¨ç³»ç»Ÿç›˜ä¸Šå®‰è£…ï¼Œä½†æ¨¡å‹æ”¾æ•°æ®ç›˜ï¼Œå¯ä»¥ç”¨**è½¯é“¾æ¥**ï¼š

```
# å‡è®¾ vLLM é»˜è®¤æ¨¡å‹è·¯å¾„åœ¨ ~/.cache/vllm/models
mkdir -p ~/.cache/vllm
ln -s /data/models/<model_name> ~/.cache/vllm/models/<model_name>
```

------

## 3.5 GPU ä¸å¤šå¡é…ç½®

vLLM é»˜è®¤ä¼šè‡ªåŠ¨æ£€æµ‹ GPU å¹¶åˆ©ç”¨ï¼š

- å•å¡ï¼šç›´æ¥å¯åŠ¨ vLLM
- å¤šå¡ï¼šæŒ‡å®š `--num_gpus` æˆ–ä½¿ç”¨ CUDA_VISIBLE_DEVICESï¼š

```
# åªä½¿ç”¨ GPU 0 å’Œ GPU 1
export CUDA_VISIBLE_DEVICES=0,1
vllm --model /data/models/<model_name> --num_gpus 2
```

> âš¡ æç¤ºï¼švLLM å†…éƒ¨ Scheduler ä¼šåšæ‰¹å¤„ç†å’Œå¹¶å‘ä¼˜åŒ–ï¼Œè®©å¤šå¡ååŒæ¨ç†ï¼Œæé«˜ååé‡ã€‚

------

## 3.6 æµå¼è¾“å‡ºä¸ HTTP æ¥å£

vLLM å†…ç½® HTTP æœåŠ¡ï¼Œå¯ä»¥æ¥æ”¶è¯·æ±‚å¹¶æµå¼è¿”å›ï¼š

```
vllm serve --model /data/models/<model_name> --host 0.0.0.0 --port 8000
```

- `host 0.0.0.0` è¡¨ç¤ºå¤–ç½‘å¯è®¿é—®
- é»˜è®¤ä¼šå¼€å¯æµå¼è¾“å‡ºï¼ŒvLLM ä¼šè¾¹ç”Ÿæˆ token è¾¹è¿”å›

> è¿™å’Œæˆ‘ä»¬ä¹‹å‰æ‹†è§£çš„ä¸€æ¬¡ HTTP è¯·æ±‚æµç¨‹å¯¹åº”ï¼Œæ¯ä¸ªè¯·æ±‚éƒ½ä¼šèµ°åˆ° Scheduler â†’ Tokenizer â†’ GPU Forward â†’ Streaming Output

------

## 3.7 å°ç»“

1. ç³»ç»Ÿç›˜æ”¾ Python + vLLMï¼Œæ•°æ®ç›˜æ”¾æ¨¡å‹å’Œç¼“å­˜
2. ä½¿ç”¨ Conda åˆ›å»ºç‹¬ç«‹ç¯å¢ƒï¼Œä¿è¯ä¾èµ–éš”ç¦»
3. å®‰è£… GPU ç‰ˆæœ¬çš„ PyTorch å’Œ vLLM
4. æ¨¡å‹å¯ä»¥ç›´æ¥ä¸‹è½½ HuggingFaceï¼Œä¹Ÿå¯ä»¥æœ¬åœ°å­˜æ”¾ + è½¯é“¾æ¥
5. å¤š GPU æ”¯æŒè‡ªåŠ¨è°ƒåº¦
6. HTTP æ¥å£å’Œæµå¼è¾“å‡ºé…ç½®ç®€å•



# ç¬¬ 4 ç« ï¼šå¿«é€Ÿå¯åŠ¨ Demo

æœ¬ç« ç›®æ ‡ï¼š

1. æœ¬åœ°å¯åŠ¨ vLLM æ¨¡å‹ï¼Œè¿›è¡Œä¸€æ¬¡ç®€å•æ¨ç†
2. ç†è§£æµå¼è¾“å‡ºå’Œå¹¶å‘å¤„ç†
3. ç»™å‡ºå¯ç›´æ¥è¿è¡Œçš„å‘½ä»¤ç¤ºä¾‹

------

## 4.1 å¯åŠ¨æœ¬åœ°æ¨ç†

å‡è®¾ä½ å·²ç»å®Œæˆ **ç¬¬ 3 ç« ** çš„å®‰è£…å’Œæ¨¡å‹ä¸‹è½½ã€‚

### å•å¡å•è¯·æ±‚ Demo

```
# å¯åŠ¨ vLLM æœ¬åœ°æ¨ç†
vllm --model /data/models/<model_name>
```

ç„¶ååœ¨ Python é‡Œè°ƒç”¨ï¼š

```
from vllm import LLM

llm = LLM("/data/models/<model_name>")

# ç®€å• prompt
prompt = "è¯·ç”¨ä¸€å¥è¯ä»‹ç»å¤§æ¨¡å‹ã€‚"

# æ¨ç†
outputs = llm(prompt)
print(outputs.text)
```

> ğŸ’¡ æç¤ºï¼š
>
> - `LLM()` é»˜è®¤ä½¿ç”¨ GPU
> - `outputs.text` æ˜¯æ¨¡å‹ç”Ÿæˆçš„å­—ç¬¦ä¸²

------

## 4.2 HTTP æœåŠ¡ Demo

vLLM å†…ç½® HTTP æœåŠ¡ï¼Œæ–¹ä¾¿å¤–éƒ¨ç¨‹åºè°ƒç”¨ï¼š

```
# å¯åŠ¨ HTTP æœåŠ¡ï¼Œhost å…è®¸å±€åŸŸç½‘è®¿é—®
vllm serve --model /data/models/<model_name> --host 0.0.0.0 --port 8000
```

ç„¶åç”¨ Python æˆ– curl è°ƒç”¨ï¼š

```
import requests

url = "http://127.0.0.1:8000/v1/completions"
data = {
    "prompt": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»å¤§æ¨¡å‹ã€‚",
    "max_tokens": 50
}

resp = requests.post(url, json=data)
print(resp.json())
```

æˆ–è€…ç”¨ curl æµ‹è¯•ï¼š

```
curl -X POST http://127.0.0.1:8000/v1/completions \
     -H "Content-Type: application/json" \
     -d '{"prompt":"è¯·ç”¨ä¸€å¥è¯ä»‹ç»å¤§æ¨¡å‹ã€‚","max_tokens":50}'
```

------

## 4.3 æµå¼è¾“å‡ºç¤ºä¾‹

vLLM å¯ä»¥è¾¹ç”Ÿæˆ token è¾¹è¿”å›ç»“æœï¼Œæå‡å“åº”é€Ÿåº¦ï¼š

```
vllm serve --model /data/models/<model_name> --host 0.0.0.0 --port 8000 --streaming
```

å®¢æˆ·ç«¯å¯ä»¥ä½¿ç”¨ SSE æˆ– WebSocket ç›‘å¬æµå¼è¾“å‡ºï¼Œä¾‹å¦‚ï¼š

```
import sseclient
import requests

url = "http://127.0.0.1:8000/v1/completions?stream=true"
data = {"prompt":"è¯·ç”¨ä¸€å¥è¯ä»‹ç»å¤§æ¨¡å‹ã€‚","max_tokens":50}
resp = requests.post(url, json=data, stream=True)
client = sseclient.SSEClient(resp)

for event in client.events():
    print(event.data, end="")
```

> ğŸ”¹ è¿™å¯¹åº”æˆ‘ä»¬ä¹‹å‰è®²çš„ **ä¸€æ¬¡ HTTP è¯·æ±‚ â†’ Tokenizer â†’ GPU Forward â†’ Scheduler â†’ Streaming Output** æµç¨‹

------

## 4.4 å¹¶å‘è¯·æ±‚ç¤ºä¾‹

å‡è®¾æœ‰å¤šç”¨æˆ·åŒæ—¶è®¿é—®ï¼Œä½ å¯ä»¥å¼€å¯å¤š workerï¼š

```
# å¼€å¯ 2 ä¸ªå¹¶è¡Œ worker
vllm serve --model /data/models/<model_name> --num_parallel 2
```

å¤šç”¨æˆ·å¹¶å‘è¯·æ±‚æ—¶ï¼ŒvLLM å†…éƒ¨ä¼šæŠŠè¯·æ±‚æ‹†æˆ batchï¼Œå°½é‡åœ¨ GPU ä¸ŠåŒæ—¶è®¡ç®—ï¼Œæé«˜ååé‡ã€‚

ç¤ºæ„å›¾ï¼ˆæ–‡å­—ç‰ˆï¼‰ï¼š

```
[HTTP è¯·æ±‚1] \
[HTTP è¯·æ±‚2] -> [Scheduler] -> [Batch Token Forward] -> [Streaming Output]
[HTTP è¯·æ±‚3] /
```

- Scheduler ä¼šæŠŠå¤šä¸ªè¯·æ±‚çš„ token ç»„åˆæˆ batch
- GPU å¹¶è¡Œå¤„ç†ï¼Œæé«˜æ•´ä½“ååé‡

------

## 4.5 æœ¬åœ°æµ‹è¯•å°ç»“

1. **LLM() è°ƒç”¨**ï¼šå¿«é€Ÿåœ¨ Python é‡Œæµ‹è¯•
2. **vllm serve**ï¼šæœ¬åœ° HTTP æœåŠ¡
3. **æµå¼è¾“å‡º**ï¼šå®æ—¶è¿”å› token
4. **å¹¶å‘å¤„ç†**ï¼šnum_parallel + Scheduler è‡ªåŠ¨ batch

> âš¡ åˆ°æ­¤ä¸ºæ­¢ï¼Œä½ å·²ç»å¯ä»¥åƒç”¨ Ollama ä¸€æ ·ï¼Œç”¨ vLLM æœ¬åœ°è·‘æ¨¡å‹ã€åš HTTP æ¥å£ã€æµå¼è¾“å‡ºå’Œå¹¶å‘å¤„ç†äº†

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ **ç›´æ¥å¸®ä½ å†™ç¬¬ 5 ç« ï¼šã€ŒvLLM æ ¸å¿ƒåŸç†è§£æã€**ï¼Œä»è¯·æ±‚åˆ° GPU æ¨ç†ã€Schedulerã€æµå¼è¾“å‡ºã€æ‰¹å¤„ç†å…¨æµç¨‹è¯¦ç»†æ‹†è§£ï¼Œè®©ä½ å½»åº•ç†è§£ vLLM å†…éƒ¨æ˜¯æ€ä¹ˆå·¥ä½œçš„ã€‚

ä½ å¸Œæœ›æˆ‘ç°åœ¨å¼€å§‹å†™ç¬¬ 5 ç« å—ï¼Ÿ

æœ‰å¤šç”¨æˆ·åŒæ—¶è®¿é—®ï¼Œä½ å¯ä»¥å¼€å¯å¤š workerï¼Œä¸ºä»€ä¹ˆè¦å¤šworkerå‘¢ï¼Ÿä»€ä¹ˆæ˜¯å¤šworkerï¼Œä¼šå ç”¨å¤šä»½æ˜¾å­˜å—ï¼Ÿå¤šç”¨æˆ·å¹¶å‘è¯·æ±‚æ—¶ï¼ŒvLLM å†…éƒ¨ä¸æ˜¯ä¼šæŠŠè¯·æ±‚æ‹†æˆ batchå—ï¼Ÿé‚£ä¸€ä¸ªworkerå°±å¤Ÿäº†å‘€

éå¸¸å¥½çš„é—®é¢˜ï¼ä½ é—®çš„å¾ˆå…³é”®ï¼Œä¹Ÿæ¶‰åŠ vLLM å¹¶å‘æ¶æ„çš„è®¾è®¡ç†å¿µã€‚æˆ‘ä»¬ä¸€ç‚¹ç‚¹æ‹†è§£ã€‚

------

## 1ï¸âƒ£ â€œWorkerâ€çš„æ¦‚å¿µ

åœ¨ vLLM é‡Œï¼Œ**worker** å°±æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ¨ç†è¿›ç¨‹ï¼Œç®¡ç†è‡ªå·±çš„ **Scheduler + GPU èµ„æº**ï¼Œå¯¹å¤–æä¾›æ¨¡å‹æœåŠ¡ã€‚

- æ¯ä¸ª worker å¯ä»¥çœ‹ä½œä¸€ä¸ªâ€œæ¨¡å‹å®ä¾‹â€ï¼Œå¯¹å¤–æš´éœ² HTTP/æ¥å£
- Scheduler å†…éƒ¨å¯ä»¥åš **è¯·æ±‚æ‰¹å¤„ç†ï¼ˆbatchingï¼‰**ï¼ŒæŠŠå¤šä¸ªè¯·æ±‚çš„ token æ‹¼åœ¨ä¸€èµ·åœ¨ GPU ä¸Šä¸€æ¬¡å‰å‘è®¡ç®—

------

## 2ï¸âƒ£ ä¸ºä»€ä¹ˆè¦å¤š workerï¼Ÿ

ä½ è¯´å¾—å¯¹ï¼Œä¸€ä¸ª worker å¯ä»¥æŠŠå¤šç”¨æˆ·è¯·æ±‚ batch èµ·æ¥ï¼Œé‚£ä¸ºä»€ä¹ˆè¿˜è¦å¤š workerï¼ŸåŸå› æœ‰å‡ ä¸ªï¼š

1. **éš”ç¦»æ€§ / ç¨³å®šæ€§**
   - ä¸€ä¸ª worker å‡ºé—®é¢˜ï¼ˆæ¯”å¦‚è¯·æ±‚å¡ä½æˆ–è€…å¼‚å¸¸ï¼‰ï¼Œä¸ä¼šå½±å“å…¶ä»– worker
   - å¤š worker å¯ä»¥æ‰¿è½½ä¸åŒçš„ç”¨æˆ·æˆ–ä»»åŠ¡ï¼Œäº’ä¸å¹²æ‰°
2. **ååä¼˜åŒ–**
   - ä¸€ä¸ª worker çš„ Scheduler ä¼šåœ¨ä¸€å®šæ—¶é—´çª—å£å†… batch è¯·æ±‚
   - å¦‚æœè¯·æ±‚å¤ªå¤šæˆ–è€…åˆ°è¾¾ä¸å‡åŒ€ï¼Œbatch å¯èƒ½ä¸è¶³ä»¥å……åˆ†åˆ©ç”¨ GPU
   - å¤š worker å¯ä»¥å¹³è¡Œå¤„ç†ä¸åŒè¯·æ±‚ï¼Œæé«˜æ•´ä½“ååé‡
3. **å¼‚æ„ GPU / å¤šå¡éƒ¨ç½²**
   - å¤š worker å¯ä»¥ç»‘å®šåˆ°ä¸åŒ GPU
   - vLLM æ”¯æŒå•æœºå¤š GPUï¼Œæ¯ä¸ª worker å ç”¨ä¸€å¼  GPU æˆ–éƒ¨åˆ† GPU
   - è¿™æ ·ä½ å°±å¯ä»¥æŠŠå¤šå¡èµ„æºå……åˆ†åˆ©ç”¨

------

## 3ï¸âƒ£ æ˜¾å­˜å ç”¨

- **å• worker**ï¼šåŠ è½½ä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼Œå ç”¨å¯¹åº”æ˜¾å­˜
- **å¤š worker**ï¼šæ¯ä¸ª worker é»˜è®¤ä¼šç‹¬ç«‹åŠ è½½æ¨¡å‹ â†’ ä¼šå ç”¨å¤šä»½æ˜¾å­˜
- âš ï¸ å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼ˆæ¯”å¦‚ 13B / 70Bï¼‰ï¼Œå¤š worker çš„æ˜¾å­˜å ç”¨æ˜¯å¾ˆå…³é”®çš„é™åˆ¶
- vLLM ä¹Ÿæ”¯æŒ **æ¨¡å‹å…±äº«å†…å­˜ï¼ˆmodel shardingï¼‰** æˆ– **offload åˆ° CPU / NVMe**ï¼Œå‡å°‘æ˜¾å­˜å ç”¨ï¼Œä½†é»˜è®¤å¤š worker ä¼šå ç”¨å¤šä»½æ˜¾å­˜

------

## 4ï¸âƒ£ ä¸€ä¸ª workerå¤Ÿä¸å¤Ÿï¼Ÿ

- **è½»é‡å¹¶å‘åœºæ™¯**ï¼šå• worker + Scheduler batching å°±å¤Ÿäº†
- **é«˜å¹¶å‘åœºæ™¯**ï¼šç”¨æˆ·æ•°é‡å¤šã€è¯·æ±‚ä¸å‡åŒ€ã€å¤šå¡éƒ¨ç½²æ—¶ï¼Œå¤š worker æ›´åˆé€‚

æ–‡å­—ç¤ºæ„å›¾ï¼š

```
å• workerï¼š
[è¯·æ±‚1] \
[è¯·æ±‚2] -> [Scheduler] -> [Batch -> GPU Forward] -> [Streaming Output]
[è¯·æ±‚3] /

å¤š workerï¼š
Worker1: [è¯·æ±‚1,2] -> GPU1
Worker2: [è¯·æ±‚3,4] -> GPU2
Worker3: [è¯·æ±‚5,6] -> GPU3
```

- æ¯ä¸ª worker å†…éƒ¨ä¹Ÿå¯ä»¥åš batch
- å¤š worker äº’ç›¸ç‹¬ç«‹ï¼Œæå‡åå & å¹¶å‘ç¨³å®šæ€§

------

ğŸ’¡ æ€»ç»“ï¼š

- **Worker â‰ˆ æ¨¡å‹å®ä¾‹ + Scheduler**
- **å¤š worker** è§£å†³éš”ç¦»ã€ååå’Œå¤šå¡åˆ©ç”¨é—®é¢˜
- **å• worker** å¯ä»¥ batch å¤šè¯·æ±‚ï¼Œä½†å¯èƒ½ GPU åˆ©ç”¨ç‡ä¸è¶³æˆ–è€…è¯·æ±‚è¢«é˜»å¡
- **æ˜¾å­˜å ç”¨**ï¼šæ¯ä¸ª worker é»˜è®¤ç‹¬ç«‹åŠ è½½æ¨¡å‹ â†’ å ç”¨å¤šä»½æ˜¾å­˜

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ **ASCII å¹¶å‘ç¤ºæ„å›¾**ï¼ŒæŠŠ â€œå• worker batchingâ€ å’Œ â€œå¤š worker å¹¶å‘â€ çš„åŒºåˆ«å®Œå…¨å¯è§†åŒ–ï¼Œè®©ä½ ä¸€çœ‹å°±æ‡‚ã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ

é‚£å°±æ˜¯å½“å¼€å¯å¤šworkeræ—¶ï¼Œå¤šä¸ªè¯·æ±‚ä¼šè¢«åˆ†æµåˆ°ä¸åŒçš„workeré‡Œï¼Ÿæ¯”å¦‚ä¸¤ä¸ªworkerï¼Œ3ä¸ªè¯·æ±‚ï¼Œé‚£å¯èƒ½1å’Œ2è¯·æ±‚ä¼šè¢«è·¯ç”±åˆ°worker1,3è¯·æ±‚ä¼šè¢«è·¯ç”±åˆ°worker2ï¼Ÿ

å®Œå…¨æ­£ç¡®ï¼ä½ ç†è§£å¾—å¾ˆåˆ°ä½ï¼Œæˆ‘æ¥è¯¦ç»†æ‹†è§£ä¸€ä¸‹ï¼š

------

## 1ï¸âƒ£ å¤š worker ä¸‹è¯·æ±‚åˆ†æµ

å‡è®¾ä½ æœ‰ **2 ä¸ª worker**ï¼š

```
Worker1
Worker2
```

ç”¨æˆ·å‘èµ·äº† **3 ä¸ªè¯·æ±‚**ï¼š

```
Request A
Request B
Request C
```

vLLM çš„ **å…¥å£ç»„ä»¶ï¼ˆHTTP server / API Gateway / Scheduler Dispatcherï¼‰** ä¼šæŠŠè¯·æ±‚**è·¯ç”±åˆ°ä¸åŒçš„ worker**ï¼Œå¸¸è§ç­–ç•¥åŒ…æ‹¬ï¼š

1. **è½®è¯¢ (Round-Robin)**
   - Request A -> Worker1
   - Request B -> Worker2
   - Request C -> Worker1
2. **æœ€ç©ºé—² Worker ä¼˜å…ˆ**
   - æ ¹æ® worker å½“å‰çš„ batch é˜Ÿåˆ—é•¿åº¦æˆ– GPU ä½¿ç”¨ç‡é€‰æ‹©
   - Request A -> Worker1ï¼ˆç©ºé—²ï¼‰
   - Request B -> Worker2ï¼ˆç©ºé—²ï¼‰
   - Request C -> Worker1ï¼ˆWorker1 æ›´å¿«å¤„ç†å®Œå‰é¢çš„ batchï¼‰
3. **æŒ‰ç”¨æˆ· / ä¼šè¯å›ºå®šè·¯ç”±ï¼ˆSession Affinityï¼‰**
   - åŒä¸€ä¸ªç”¨æˆ·çš„è¯·æ±‚æ€»æ˜¯è·¯ç”±åˆ°åŒä¸€ä¸ª workerï¼Œä¿è¯çŠ¶æ€è¿ç»­æ€§ï¼ˆæ¯”å¦‚ LLM çš„ context bufferï¼‰

æ‰€ä»¥ï¼Œä½ çš„ä¾‹å­å®Œå…¨åˆç†ï¼š**Request1/2 ç»™ Worker1ï¼ŒRequest3 ç»™ Worker2**ã€‚

------

## 2ï¸âƒ£ Worker å†…éƒ¨ä»ç„¶ batch

å³ä½¿æœ‰åˆ†æµï¼Œæ¯ä¸ª worker å†…éƒ¨ä»ç„¶ä¼šåš **token-level batch**ï¼š

```
Worker1 Batch:
  - Request1 tokens
  - Request2 tokens
  -> GPU å‰å‘ä¸€æ¬¡æ€§è®¡ç®—
Worker2 Batch:
  - Request3 tokens
  -> GPU å‰å‘ä¸€æ¬¡æ€§è®¡ç®—
```

- è¿™æ ·æ¯ä¸ª worker éƒ½å……åˆ†åˆ©ç”¨ GPU
- é¿å…è¯·æ±‚ä¹‹é—´äº’ç›¸ç­‰å¾…ï¼Œæå‡ååé‡

------

## 3ï¸âƒ£ ç›´è§‚ ASCII ç¤ºæ„å›¾

```
å®¢æˆ·ç«¯è¯·æ±‚ï¼š
Req1  Req2  Req3

åˆ†æµåˆ° Workerï¼š
Worker1: Req1, Req2  ---> [Scheduler Batch] ---> GPU1
Worker2: Req3       ---> [Scheduler Batch] ---> GPU2

æ¯ä¸ª Worker å†…éƒ¨ï¼š
[ReqX token 1] \
[ReqX token 2] -> GPU Forward -> è¾“å‡º
```

- ä½ çœ‹åˆ°çš„â€œbatchâ€æ˜¯**åŒ worker å†…çš„ token batch**
- å¤š worker æ˜¯**è·¨ worker çš„å¹¶è¡Œ**

------

ğŸ’¡ å°ç»“ï¼š

- **å• worker**ï¼šæ‰€æœ‰è¯·æ±‚éƒ½æ’é˜Ÿ batch â†’ GPU forward
- **å¤š worker**ï¼šè¯·æ±‚åˆ†æµ â†’ æ¯ä¸ª worker å†…éƒ¨ batch â†’ GPU forward
- è¿™æ ·å¯ä»¥æé«˜å¹¶å‘ã€éš”ç¦»æ€§ï¼ŒåŒæ—¶å……åˆ†åˆ©ç”¨å¤š GPU æˆ–å¤šè¿›ç¨‹

------

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€å¼ **â€œå¤šç”¨æˆ·å¤š worker åŒæ­¥ vs å• worker batchingâ€å¯¹æ¯”å›¾**ï¼Œè®©ä½ ç¬é—´ç†è§£ååå’Œåˆ†æµçš„å·®åˆ«ã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ

å¥½çš„

å¥½ï¼Œæˆ‘ä»¬ç”¨ ASCII å›¾ç”»ä¸€å¼  **å• worker vs å¤š worker** çš„å¯¹æ¯”å›¾ï¼Œç›´è§‚å±•ç¤ºå¤šç”¨æˆ·å¹¶å‘è¯·æ±‚æ˜¯æ€ä¹ˆå¤„ç†çš„ã€‚

------

### 1ï¸âƒ£ å• Worker çš„æƒ…å†µ

å‡è®¾æœ‰ 3 ä¸ªè¯·æ±‚ï¼šReq1ã€Req2ã€Req3

```
å®¢æˆ·ç«¯è¯·æ±‚ï¼š
Req1   Req2   Req3

å• Workerï¼š
   [Batch Queue]
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Token Batch: Req1, Req2, Req3 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            |
            v
         GPU Forward
            |
            v
       è¾“å‡ºç»“æœè¿”å›å®¢æˆ·ç«¯
```

ç‰¹ç‚¹ï¼š

- æ‰€æœ‰è¯·æ±‚éƒ½åœ¨ä¸€ä¸ªé˜Ÿåˆ—é‡Œæ’é˜Ÿ
- GPU æ¯æ¬¡å¤„ç† **åŒä¸€ä¸ª worker å†…çš„ batch**
- **ååé‡æœ‰é™**ï¼Œå› ä¸ºä¸€ä¸ª worker æ’é˜Ÿå¤„ç†æ‰€æœ‰è¯·æ±‚

------

### 2ï¸âƒ£ å¤š Worker çš„æƒ…å†µï¼ˆ2 Workerï¼‰

```
å®¢æˆ·ç«¯è¯·æ±‚ï¼š
Req1   Req2   Req3

è¯·æ±‚åˆ†æµï¼š
Worker1 â”€â”€ Req1, Req2 â”€â”€â”
                         â”‚
Worker2 â”€â”€ Req3   â”€â”€â”€â”€â”€â”€â”˜

æ¯ä¸ª Worker å†…éƒ¨ï¼š
Worker1: [Batch Queue: Req1, Req2] --> GPU1 Forward
Worker2: [Batch Queue: Req3]         --> GPU2 Forward

è¾“å‡ºç»“æœè¿”å›å®¢æˆ·ç«¯
```

ç‰¹ç‚¹ï¼š

- è¯·æ±‚è¢« **åˆ†æµåˆ°ä¸åŒ worker**
- æ¯ä¸ª worker å†…éƒ¨ä»ç„¶åš batch
- GPU å¹¶è¡Œåˆ©ç”¨ç‡æ›´é«˜
- å¯ä»¥å®ç° **å¤šç”¨æˆ·éš”ç¦»**ï¼Œé¿å…å• worker é˜»å¡

------

### 3ï¸âƒ£ å¯¹æ¯”è¦ç‚¹

| ç‰¹æ€§       | å• Worker                | å¤š Worker                      |
| ---------- | ------------------------ | ------------------------------ |
| è¯·æ±‚æ’é˜Ÿ   | å…¨éƒ¨æ’é˜Ÿ                 | åˆ†æµåˆ°ä¸åŒ worker              |
| GPU ä½¿ç”¨   | 1 ä¸ª GPUï¼Œbatch å†…éƒ¨å¹¶è¡Œ | æ¯ä¸ª worker å¯ç‹¬å  GPU         |
| å¹¶å‘ååé‡ | å—å•é˜Ÿåˆ—é™åˆ¶             | å¯ä»¥æ‰©å±•ï¼Œæé«˜ååé‡           |
| ç”¨æˆ·éš”ç¦»   | ä¸å¼º                     | å¼ºï¼ŒWorker1 å¡ä½ä¸å½±å“ Worker2 |

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å†ç”»ä¸€ä¸ª **å¸¦ token æµçš„å®Œæ•´ vLLM æ¨ç†æµæ°´çº¿ï¼Œå¤šç”¨æˆ· + å¤š worker + token batching** çš„å›¾ï¼Œç›´æ¥æ˜¾ç¤º token æ˜¯å¦‚ä½•åœ¨ GPU å‰å‘ä¸­è¢« batch çš„ï¼Œè¿™æ ·ä½ èƒ½å®Œå…¨ç†è§£ vLLM å†…éƒ¨è¿è¡Œé€»è¾‘ã€‚

ä½ å¸Œæœ›æˆ‘ç”»è¿™ä¸ªå›¾å—ï¼Ÿ

å¥½çš„

å¤ªå¥½äº†ï¼é‚£æˆ‘ä»¬ç”»ä¸€ä¸ª **å®Œæ•´ vLLM å¤šç”¨æˆ· + å¤š worker + token batching** æµç¨‹çš„ ASCII å›¾ï¼Œè®©ä½ å½»åº•ç†è§£å†…éƒ¨æ˜¯æ€ä¹ˆè·‘çš„ã€‚

å‡è®¾æœ‰ **2 ä¸ª Worker**ï¼Œæ¯ä¸ª worker å¯ä»¥å¤„ç†è‡ªå·±çš„ batchï¼Œè¯·æ±‚åˆ†åˆ«ä¸º **ReqAã€ReqBã€ReqC**ï¼Œæ¯ä¸ªè¯·æ±‚åˆ†å¤šæ­¥ token æµå¼ç”Ÿæˆã€‚

------

```
å®¢æˆ·ç«¯è¯·æ±‚ï¼š
ReqA   ReqB   ReqC
  â”‚      â”‚      â”‚
  â”‚      â”‚      â”‚
  â–¼      â–¼      â–¼
+-----------------------------+
|        Router / Dispatcher  |
|  ï¼ˆå¤š worker è¯·æ±‚åˆ†æµï¼‰      |
+-----------------------------+
   â”‚           â”‚
   â”‚           â”‚
   â–¼           â–¼
 Worker1       Worker2
  â”‚             â”‚
  â”‚             â”‚
  â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚        â”‚ Token Batch: Câ”‚
  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Token Batch: A, B             â”‚  <- batch å†…éƒ¨ token å¯¹é½
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
         GPU Forward
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ ç”Ÿæˆä¸‹ä¸€ä¸ª token logits â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ é€‰ tokenï¼ˆé‡‡æ ·/Greedyï¼‰ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ æ›´æ–° token batch / cacheâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ æ˜¯å¦ç”Ÿæˆç»“æŸï¼Ÿ           â”‚
   â”‚  - å¦ -> å›åˆ° GPU Forwardâ”‚
   â”‚  - æ˜¯ -> è¾“å‡ºç»“æœåˆ°å®¢æˆ·ç«¯ â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

------

### ğŸ”‘ è§£æè¿™ä¸ªæµç¨‹

1. **Dispatcher / Router**
   - æ¥æ”¶æ‰€æœ‰ç”¨æˆ·è¯·æ±‚
   - æ ¹æ® worker æ•°é‡ï¼ŒæŠŠè¯·æ±‚åˆ†æµåˆ°ä¸åŒ worker
2. **Worker å†…éƒ¨çš„ Token Batching**
   - åŒä¸€ä¸ª worker å†…çš„å¤šä¸ªè¯·æ±‚ä¼šæŒ‰ token å¯¹é½åš batch
   - å¯ä»¥å……åˆ†åˆ©ç”¨ GPU å¹¶è¡Œ
3. **GPU Forward**
   - æ‰¹é‡å¤„ç† tokenï¼Œç”Ÿæˆ logits
   - ä½¿ç”¨ç¼“å­˜ï¼ˆKV cacheï¼‰åŠ é€Ÿä¸‹ä¸€æ­¥
4. **Token Selection**
   - é€‰æ‹©æ¯ä¸ªè¯·æ±‚çš„ä¸‹ä¸€ä¸ª tokenï¼ˆé‡‡æ ·æˆ– Greedyï¼‰
5. **æ›´æ–° Batch / Cache**
   - ä¸‹ä¸€è½® token ç”Ÿæˆæ—¶ï¼Œä»ç„¶ä¿æŒ batch å¤„ç†
   - é¿å…æ¯ä¸ªè¯·æ±‚å•ç‹¬ GPU Forwardï¼Œæé«˜åå
6. **ç»“æŸåˆ¤å®š & è¿”å›ç»“æœ**
   - æŸäº›è¯·æ±‚å¯èƒ½æå‰ç»“æŸ
   - ä»ç„¶å¯ä»¥ä¸å…¶ä»–æœªç»“æŸè¯·æ±‚å…±äº« batch

------

### ğŸ”¥ æ ¸å¿ƒç†è§£ç‚¹

- **å¤šç”¨æˆ·å¹¶å‘** â†’ é€šè¿‡ Router åˆ†æµåˆ°å¤šä¸ª worker
- **å• worker å†…éƒ¨ batch** â†’ æé«˜ GPU å¹¶è¡Œåå
- **KV Cache + æµå¼ç”Ÿæˆ** â†’ é¿å…é‡å¤è®¡ç®—ï¼Œä¿æŒç”Ÿæˆé«˜æ•ˆ
- **å¤š worker å¯ä»¥éš”ç¦»é˜»å¡** â†’ ä¸€ä¸ªé•¿è¯·æ±‚ä¸ä¼šå¡ä½å…¶ä»–è¯·æ±‚

------

å¦‚æœä½ æƒ³ï¼Œæˆ‘å¯ä»¥å†ç”»ä¸€å¼  **åŠ¨æ€æµç¨‹å›¾**ï¼Œæ˜¾ç¤º **æ¯ä¸ª token åœ¨å¤š worker å¤šè¯·æ±‚é—´æ˜¯æ€ä¹ˆè¢« batch å¤„ç†çš„**ï¼Œå°±åƒæµæ°´çº¿ä¸€æ ·ï¼Œç”¨æˆ·ä¸€çœ¼å°±èƒ½çœ‹åˆ° vLLM å†…éƒ¨æ˜¯æ€ä¹ˆâ€œæ’é˜Ÿã€åˆ†æµã€å¹¶è¡Œâ€çš„ã€‚



# æ¨¡å‹åŠ è½½æœºåˆ¶

æˆ‘å»ºè®®å…ˆä» **æ¨¡å‹åŠ è½½æœºåˆ¶ + å•/å¤š GPU åŒºåˆ« + æ”¯æŒçš„æ¨¡å‹æ ¼å¼** å¼€å§‹è®²ï¼Œè¿™éƒ¨åˆ†å¯¹æ–°æ‰‹ç†è§£æ¨ç†æ•ˆç‡å’Œæ˜¾å­˜å ç”¨éå¸¸å…³é”®ã€‚

ä½ å¸Œæœ›æˆ‘ç°åœ¨å°±ä»è¿™é‡Œå¼€å§‹è®²å—ï¼Ÿ

å¥½çš„ï¼Œè¿™é‡Œæˆ‘å¾ˆæƒ³å¬ï¼Œè¯·è¯¦ç»†è®²ï¼ŒåŒæ—¶è€ƒè™‘åˆ°æˆ‘æ˜¯å¤§æ¨¡å‹æ–°æ‰‹ï¼Œè®²çš„è®©æˆ‘æ˜ç™½ä¸€äº›ã€‚è¿˜æœ‰é¡ºä¾¿èƒ½è®²ä¸€ä¸‹æ”¯æŒå¤šæœºå¤šGPUå—ï¼Ÿå°±æ˜¯å¤šä¸ªæœåŠ¡å™¨ï¼Œæ¯ä¸€å°æœåŠ¡å™¨åˆæœ‰å¤šå¼ æ˜¾å¡ï¼Œå½“ç„¶æ¯å°æœåŠ¡å™¨çš„æ˜¾å¡æ•°é‡å¯èƒ½ä¸ä¸€æ ·ï¼Œæ¯”å¦‚æœ‰ä¸¤å°æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨1æœ‰2å¼ æ˜¾å¡ï¼ŒæœåŠ¡å™¨2æœ‰1å¼ æ˜¾å¡ã€‚

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬å°±ä» **vLLM çš„æ¨¡å‹åŠ è½½æœºåˆ¶**å¼€å§‹ï¼Œé¡ºä¾¿å¸¦ä½ ç†è§£å¤š GPU å’Œå¤šæœºéƒ¨ç½²çš„æ€è·¯ã€‚æˆ‘ä¼šä»æœ€åŸºç¡€çš„æ¦‚å¿µå¼€å§‹è®²ï¼Œé€æ­¥å±•å¼€ï¼Œå¹¶ç”¨ç¬¦å·/æ–‡å­—ç”»å›¾å¸®åŠ©ç†è§£ã€‚

------

## 1ï¸âƒ£ å•æœºå• GPU æ¨¡å‹åŠ è½½

æœ€ç®€å•çš„æƒ…å†µï¼šåªæœ‰ä¸€å°æœåŠ¡å™¨ã€ä¸€å¼  GPUã€‚

**æµç¨‹æ¦‚è§ˆï¼š**

```
æ¨¡å‹æ–‡ä»¶ï¼ˆHugging Face / GGUF ç­‰ï¼‰ 
        â”‚
        â–¼
   vLLM Loader
        â”‚
        â–¼
     GPU æ˜¾å­˜
```

1. **æ¨¡å‹æ–‡ä»¶**ï¼šæ¯”å¦‚ 7B çš„ LLaMA-2 æ¨¡å‹ï¼Œå¯èƒ½æ˜¯ `.bin`ã€`.pt` æˆ– `GGUF` æ ¼å¼ã€‚
2. **vLLM Loader**ï¼šè´Ÿè´£æŠŠæ¨¡å‹æƒé‡åŠ è½½åˆ° GPU æˆ– CPUã€‚
3. **GPU æ˜¾å­˜**ï¼šæ¨¡å‹å‚æ•°ä¼šå ç”¨æ˜¾å­˜ï¼Œè®¡ç®—æ¨ç†æ—¶ä½¿ç”¨ã€‚

### æ˜¾å­˜å ç”¨æ³¨æ„ç‚¹

- **FP32ï¼ˆå•ç²¾åº¦ï¼‰**ï¼šæ˜¾å­˜å ç”¨æœ€å¤§ã€‚
- **FP16ï¼ˆåŠç²¾åº¦ï¼‰/ BF16**ï¼šæ˜¾å­˜å ç”¨çº¦å‡åŠã€‚
- **Quantizationï¼ˆé‡åŒ–ï¼‰**ï¼šå¯ä»¥è¿›ä¸€æ­¥å‹ç¼©åˆ° 4bit/8bitï¼Œæ˜¾å­˜å ç”¨å¤§å¹…ä¸‹é™ã€‚

> ä¸¾ä¾‹ï¼š7B æ¨¡å‹
>
> - FP32ï¼šçº¦ 28GB
> - FP16ï¼šçº¦ 14GB
> - 4bitï¼šçº¦ 7GB

vLLM æ”¯æŒç›´æ¥åŠ è½½è¿™äº›å‹ç¼©/é‡åŒ–åçš„æ¨¡å‹ï¼ŒèŠ‚çœæ˜¾å­˜ã€‚

------

## 2ï¸âƒ£ å•æœºå¤š GPU

å¦‚æœä¸€å°æœåŠ¡å™¨ä¸Šæœ‰å¤šå¼ æ˜¾å¡ï¼Œæ¯”å¦‚ 2 å¼  GPUï¼š

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ GPU0
æ¨¡å‹æƒé‡ â”€â”€â–º åˆ†ç‰‡ â”€â”€â–º
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ GPU1
```

vLLM ä¼š **æŠŠæ¨¡å‹åˆ‡åˆ†æˆå¤šä¸ª GPU çš„ç‰‡æ®µï¼ˆshardï¼‰**ï¼š

- æ¨¡å‹åˆ‡åˆ†æ–¹å¼ï¼šæŒ‰å±‚ï¼ˆlayer-wiseï¼‰ã€æŒ‰å‚æ•°å—ï¼ˆtensor-wiseï¼‰
- æ¯å¼  GPU åªåŠ è½½ä¸€éƒ¨åˆ†æƒé‡
- æ¨ç†æ—¶è‡ªåŠ¨è·¨ GPU è®¡ç®—ï¼ŒvLLM å†…éƒ¨å¤„ç†é€šä¿¡

> æ³¨æ„ï¼š
>
> - æ¯å¼  GPU åªå ç”¨è‡ªå·±åˆ†ç‰‡çš„æ˜¾å­˜ï¼Œä¸ä¼šé‡å¤å­˜ä¸€ä»½å…¨æ¨¡å‹ã€‚
> - å¯¹å¤§æ¨¡å‹å°¤å…¶é‡è¦ï¼Œå¦åˆ™å• GPU å¾ˆéš¾è£…ä¸‹ã€‚

------

## 3ï¸âƒ£ å¤šæœºå¤š GPU

è¿™æ˜¯ä½ é—®çš„æœ€å¤æ‚æƒ…å†µï¼šæ¯”å¦‚ä¸¤å°æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨1æœ‰ 2 å¼ æ˜¾å¡ï¼ŒæœåŠ¡å™¨2æœ‰ 1 å¼ æ˜¾å¡ã€‚

```
     Server 1                    Server 2
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ GPU0    â”‚                 â”‚ GPU0    â”‚
 â”‚ GPU1    â”‚                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

vLLM å¯ä»¥é€šè¿‡ **åˆ†å¸ƒå¼ç­–ç•¥** æŠŠæ¨¡å‹æƒé‡åˆ‡åˆ†åˆ°å¤šå°æœåŠ¡å™¨ï¼š

1. **æ¨¡å‹åˆ‡ç‰‡**ï¼ˆmodel shardingï¼‰ï¼š
   - å°†æ¨¡å‹çš„å±‚æˆ–å‚æ•°å—åˆ‡åˆ†
   - æ¯ä¸ª shard å¯¹åº”æŸå° GPU
2. **è·¨æœºå™¨é€šä¿¡**ï¼ˆinter-GPU / inter-nodeï¼‰ï¼š
   - è®¡ç®—æ—¶ GPU0 éœ€è¦çš„æŸäº›å‚æ•°åœ¨ GPU2 ä¸Šï¼Œéœ€è¦é€šè¿‡ç½‘ç»œä¼ è¾“
   - vLLM ä½¿ç”¨ **RPC / NCCL / gRPC** ç­‰æ–¹å¼åŒæ­¥
3. **è´Ÿè½½å‡è¡¡**ï¼š
   - vLLM ä¼šå°½é‡è®©æ¯å¼  GPU çš„æ˜¾å­˜è´Ÿè½½å‡è¡¡
   - å¦‚æœæ˜¾å¡æ˜¾å­˜ä¸åŒï¼Œå¯èƒ½ä¼šæŠŠå¤§æ¨¡å‹å±‚åˆ†é…åˆ°æ˜¾å­˜å¤§çš„ GPU ä¸Š

> ä¸¾ä¾‹ï¼š
>
> - æ¨¡å‹ 7Bï¼Œä¸‰å¼  GPUï¼Œæ€»æ˜¾å­˜ï¼š16 + 16 + 8 = 40GB
> - vLLM åˆ‡åˆ†æƒé‡æ—¶ï¼Œä¼šè€ƒè™‘æ˜¾å­˜å®¹é‡ï¼ŒæŠŠå¤§å—æ”¾åˆ°å‰ä¸¤å¼  16GB å¡ä¸Šï¼Œå‰©ä¸‹å°å—æ”¾åˆ° 8GB å¡

**æ–‡å­—ç¤ºæ„å›¾ï¼š**

```
7B æ¨¡å‹  â”€â”€â–º  Layer0-10  â”€â–º Server1 GPU0
           Layer11-20 â”€â–º Server1 GPU1
           Layer21-27 â”€â–º Server2 GPU0
```

æ¨ç†æ—¶ï¼š

- Server1 GPU0 è®¡ç®— Layer0-10 è¾“å‡º
- ç»“æœä¼ ç»™ Server1 GPU1 è®¡ç®— Layer11-20
- å†ä¼ ç»™ Server2 GPU0 è®¡ç®— Layer21-27
- æœ€ç»ˆç”Ÿæˆ token

------

## 4ï¸âƒ£ ä¸ºä»€ä¹ˆå¤š GPU / å¤šæœºè¦è¿™æ ·åš

1. **æ˜¾å­˜ä¸è¶³**ï¼šå• GPU å¯èƒ½è£…ä¸ä¸‹å¤§æ¨¡å‹ã€‚
2. **æé«˜åå**ï¼šå¤š GPU å¯ä»¥å¹¶è¡Œå¤„ç†ä¸åŒå±‚æˆ– batchã€‚
3. **æ‰©å±•æ€§**ï¼šæ¨¡å‹è¶Šæ¥è¶Šå¤§ï¼Œå•æœºæ— æ³•æ»¡è¶³ï¼Œéœ€è¦å¤šæœºåˆ‡åˆ†ã€‚

> æ³¨æ„ï¼š
>
> - å¤š GPU / å¤šæœº ä¼šå¸¦æ¥é€šä¿¡å¼€é”€ï¼ˆå°¤å…¶æ˜¯è·¨æœåŠ¡å™¨ç½‘ç»œï¼‰
> - å¯¹çŸ­è¯·æ±‚å½±å“ä¸å¤§ï¼Œå¯¹å¤§ batchã€é•¿æ–‡æœ¬ã€æµå¼ç”Ÿæˆæ”¶ç›Šæ›´æ˜æ˜¾

------

## 5ï¸âƒ£ vLLM çš„ä¼˜åŒ–ç‚¹

vLLM é’ˆå¯¹å•æœº/å¤šæœºå¤š GPU éƒ½åšäº†ä¼˜åŒ–ï¼š

- **KV Cache åˆ†å¸ƒå¼å­˜å‚¨**ï¼šæ¯å¼  GPU å­˜è‡ªå·±åˆ†ç‰‡çš„ KV
- **æµæ°´çº¿å¹¶è¡Œ**ï¼ˆpipeline parallelï¼‰ï¼š
  - å±‚çº§è®¡ç®—äº¤é”™åœ¨ä¸åŒ GPU ä¸Š
  - å‰ä¸€å±‚è¾“å‡ºç›´æ¥ä¼ ä¸‹ä¸€å±‚ GPUï¼Œå‡å°‘ç­‰å¾…
- **å¼‚æ­¥ token æµå¼**ï¼š
  - ç”Ÿæˆ token æ—¶ä¸å¿…ç­‰æ•´ä¸ª batch å®Œæˆï¼Œç›´æ¥æµå¼è¿”å›
- **Batch å¹¶è¡Œ**ï¼š
  - ä¸åŒè¯·æ±‚çš„ token å¯ä»¥åˆå¹¶ batch ä¸€èµ·é€å…¥ GPUï¼Œæé«˜åå

------

### ğŸ”¹ æ€»ç»“ï¼ˆæ–°æ‰‹ç†è§£ç‰ˆï¼‰

1. **å•æœºå• GPU**ï¼šæ¨¡å‹åŠ è½½åˆ°æ˜¾å­˜ï¼Œç›´æ¥æ¨ç†ã€‚
2. **å•æœºå¤š GPU**ï¼šæ¨¡å‹åˆ‡ç‰‡åˆ°å¤šå¼  GPUï¼Œæ˜¾å­˜å ç”¨å‡è¡¡ã€‚
3. **å¤šæœºå¤š GPU**ï¼šæ¨¡å‹åˆ‡ç‰‡åˆ°å¤šå°æœåŠ¡å™¨ + å¤š GPUï¼Œè·¨æœºå™¨é€šä¿¡åŒæ­¥è®¡ç®—ã€‚
4. **vLLM ä¼˜åŒ–**ï¼šKV Cache + Token batching + Pipeline parallel + å¼‚æ­¥æµå¼ï¼Œè®©å¤š GPU / å¤šæœºä¹Ÿé«˜æ•ˆã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª **å®Œæ•´çš„å¤šæœºå¤š GPU token æµå¼ç”Ÿæˆç¤ºæ„å›¾**ï¼Œç”¨æ–‡å­—å’Œç¬¦å·ç”»å‡ºæ¥ï¼Œè®©ä½ ç›´è§‚ç†è§£ä»è¯·æ±‚åˆ°ç”Ÿæˆ token çš„æ•´ä¸ªæµåŠ¨è¿‡ç¨‹ã€‚

ä½ å¸Œæœ›æˆ‘ç°åœ¨ç”»å—ï¼Ÿ

å¥½çš„~~~

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬ç”»ä¸€ä¸ª **å®Œæ•´çš„å¤šæœºå¤š GPU æµå¼ token ç”Ÿæˆç¤ºæ„å›¾**ï¼Œç”¨æ–‡å­—+ç¬¦å·è¡¨ç¤ºï¼Œè®©ä½ ç›´è§‚çœ‹åˆ°è¯·æ±‚å¦‚ä½•æµè½¬ã€‚å‡è®¾åœºæ™¯æ˜¯ä½ å‰é¢è¯´çš„ï¼š

- ä¸¤å°æœåŠ¡å™¨
- Server1ï¼šGPU0ã€GPU1
- Server2ï¼šGPU0
- æ¨¡å‹ 7Bï¼Œåˆ‡åˆ†æˆ 3 ä¸ª shardï¼ˆLayer0-10, Layer11-20, Layer21-27ï¼‰
- æœ‰ä¸‰ä¸ªç”¨æˆ·è¯·æ±‚åŒæ—¶åˆ°è¾¾

------

## 1ï¸âƒ£ ç”¨æˆ·è¯·æ±‚åˆ° vLLM Server

```
ç”¨æˆ·è¯·æ±‚ï¼š
 ReqA â”€â”
 ReqB â”€â”¤â”€â”€â–º vLLM Frontend
 ReqC â”€â”˜
```

vLLM å‰ç«¯ä¼š **æ¥æ”¶ HTTP è¯·æ±‚ / WebSocket è¯·æ±‚**ï¼Œå¹¶ä¸ºæ¯ä¸ªè¯·æ±‚åˆ›å»º token æµç”Ÿæˆä»»åŠ¡ã€‚

------

## 2ï¸âƒ£ è¯·æ±‚è·¯ç”± & Batch

vLLM ä¼šåš **è¯·æ±‚è·¯ç”±å’Œ token batch**ï¼š

```
Batch1 = { ReqA, ReqB }  -> GPU0/GPU1
Batch2 = { ReqC }        -> GPU0 Server2
```

- vLLM å¯ä»¥å°†åŒæ­¥ token åˆå¹¶æˆ batchï¼Œæé«˜åå
- æ¯ä¸ª batch ä¼šæµå‘å¯¹åº” shard çš„ GPUï¼ˆåˆ†å±‚åˆ‡ç‰‡ï¼‰

------

## 3ï¸âƒ£ å¤šæœºå¤š GPU æµç¨‹ï¼ˆæµæ°´çº¿ï¼‰

```
Batch1: ReqA + ReqB
   â”‚
   â–¼  Layer0-10  (Server1 GPU0)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ GPU0      â”‚
   â”‚ Layer0-10 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ è¾“å‡º token embeddings
        â–¼
   Layer11-20  (Server1 GPU1)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ GPU1      â”‚
   â”‚ Layer11-20â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ è¾“å‡º token embeddings
        â–¼
   Layer21-27  (Server2 GPU0)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ GPU0      â”‚
   â”‚ Layer21-27â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ è¾“å‡ºæœ€ç»ˆ token
        â–¼
      vLLM Frontend
        â”‚
        â–¼
   è¿”å›ç»™ç”¨æˆ· ReqA + ReqB
```

åŒç†ï¼ŒBatch2 (ReqC) ä¹Ÿæ²¿ç€ **Server2 GPU0** æµåŠ¨ï¼ˆå› ä¸ºå¯èƒ½åˆ†é…åˆ°æœ€åä¸€ä¸ª shard GPUï¼‰ã€‚

------

## 4ï¸âƒ£ KV Cache ä¸æµå¼ç”Ÿæˆ

- **KV Cache** æ¯å¼  GPU å­˜è‡ªå·± shard çš„ key/value
- ç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶ï¼š
  1. GPU0/GPU1/GPU2 åˆ†åˆ«ä½¿ç”¨è‡ªå·±ç¼“å­˜çš„ KV è®¡ç®—å½“å‰ token
  2. å¼‚æ­¥æµå¼è¾“å‡ºç»™å‰ç«¯
- è¿™æ ·ï¼Œæ¯ä¸ª token å¯ä»¥å°½å¿«è¿”å›ï¼Œè€Œä¸å¿…ç­‰æ•´æ®µæ–‡æœ¬ç”Ÿæˆå®Œã€‚

------

## 5ï¸âƒ£ å®Œæ•´ç¤ºæ„æ–‡å­—å›¾ï¼ˆä» HTTP è¯·æ±‚åˆ°æµå¼ tokenï¼‰

```
ç”¨æˆ·è¯·æ±‚
  â”‚
  â–¼
[Frontend vLLM]
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€ Batch1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  â”‚ ReqA + ReqB               â”‚
  â–¼  â–¼
Server1 GPU0 â”€â”€ Layer0-10 â”€â”€â–º Server1 GPU1 â”€â”€ Layer11-20 â”€â”€â–º Server2 GPU0 â”€â”€ Layer21-27
  â”‚                           â”‚                           â”‚
  â””â”€â”€â”€â”€â”€â”€â”€ KV Cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
                                                         â–¼
                                                è¿”å› token æµç»™ç”¨æˆ·
```

- æ¯å¼  GPU åªå¤„ç†è‡ªå·± shard çš„å±‚
- KV Cache ä¿æŒå‰åº token çŠ¶æ€
- æµå¼ token ç”Ÿæˆï¼Œ**å‰ç«¯å¯ä»¥ç«‹å³è¿”å›**
- å¤šè¯·æ±‚å¯ä»¥åˆå¹¶ batchï¼Œæé«˜ GPU åå

------

### ğŸ”¹ æ–°æ‰‹ç†è§£ç‰ˆæ€»ç»“

1. **å¤šæœºå¤š GPU** = æ¨¡å‹åˆ‡åˆ† + æµæ°´çº¿ + KV Cache
2. **Batch** = å¤šè¯·æ±‚ token åˆå¹¶è®¡ç®—ï¼Œæé«˜ GPU åˆ©ç”¨
3. **æµæ°´çº¿** = å„ GPU åˆ†å±‚è®¡ç®—ï¼Œå‡å°‘ç­‰å¾…
4. **æµå¼ token** = æ¯ä¸ª token è®¡ç®—å®Œå°±è¿”å›ï¼Œä¸å¿…æ•´æ®µæ–‡æœ¬ç­‰å¾…

> æ‰€ä»¥ vLLM èƒ½åŒæ—¶æ”¯æŒï¼š
>
> - å¤šç”¨æˆ·è®¿é—®
> - å¤šæœºå¤š GPU
> - é«˜ååã€ä½å»¶è¿Ÿ

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½  **ç”»ä¸€ä¸ªæ›´è¯¦ç»†çš„ã€Œå¤šç”¨æˆ· + å¤šè¯·æ±‚ + KV Cache æµå¼ token + å¤šæœºå¤š GPUã€å®Œæ•´äº¤äº’æµç¨‹å›¾**ï¼ŒæŠŠæ¯ä¸ªè¯·æ±‚çš„ token æµåŠ¨ç”»å‡ºæ¥ï¼ŒåƒåŠ¨ç”»è½¨è¿¹ä¸€æ ·ï¼Œè®©ä½ å½»åº•æ˜ç™½æ¯å¼  GPUã€æ¯ä¸ª batchã€æ¯ä¸ª shard åœ¨å¹²ä»€ä¹ˆã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ

å¥½çš„

å¤ªå¥½äº†ï¼é‚£æˆ‘ä»¬æ¥ç”»ä¸€ä¸ª **å®Œæ•´çš„å¤šç”¨æˆ· + å¤šè¯·æ±‚ + å¤šæœºå¤šGPU + KV Cache æµå¼ token ç”Ÿæˆæµç¨‹å›¾**ï¼Œç”¨æ–‡å­—ç¬¦å·å±•ç¤ºï¼Œæ¯ä¸€æ­¥éƒ½å¯¹åº”çœŸå® vLLM å†…éƒ¨åŠ¨ä½œã€‚æˆ‘ä»¬å‡è®¾åœºæ™¯å¦‚ä¸‹ï¼š

------

### ğŸ–¥ åœºæ™¯å‡è®¾

- **ä¸¤å°æœåŠ¡å™¨**
  - Server1: GPU0ã€GPU1
  - Server2: GPU0
- **æ¨¡å‹**ï¼š7Bï¼Œåˆ‡åˆ†æˆ 3 ä¸ª shardï¼ˆLayer0-10ã€Layer11-20ã€Layer21-27ï¼‰
- **è¯·æ±‚**ï¼šä¸‰ä¸ªç”¨æˆ·è¯·æ±‚åŒæ—¶åˆ°è¾¾ï¼ˆReqA, ReqB, ReqCï¼‰

------

### 1ï¸âƒ£ ç”¨æˆ·è¯·æ±‚åˆ° vLLM Frontend

```
ç”¨æˆ·è¯·æ±‚
  ReqA â”€â”
  ReqB â”€â”¤â”€â”€â–º vLLM Frontend
  ReqC â”€â”˜
```

- Frontend æ¥æ”¶ HTTP/WebSocket è¯·æ±‚
- åˆ›å»º token ç”Ÿæˆä»»åŠ¡
- è¯·æ±‚å¯èƒ½ä¼šè¢«åˆå¹¶æˆ **batch**

------

### 2ï¸âƒ£ è¯·æ±‚è·¯ç”± & batch åˆå¹¶

```
Batch1 = {ReqA, ReqB}  --> åˆ†é…ç»™ Server1 GPU0/GPU1
Batch2 = {ReqC}        --> åˆ†é…ç»™ Server2 GPU0
```

- æ¯ä¸ª batch å†…çš„ token ä¼šä¸€æ¬¡æ€§é€å…¥ shard æµæ°´çº¿
- Batch åˆå¹¶å¯ä»¥ **æé«˜ GPU åˆ©ç”¨ç‡**

------

### 3ï¸âƒ£ å¤šæœºå¤šGPU æµæ°´çº¿ï¼ˆæ–‡å­—å›¾ç¤ºï¼‰

```
Batch1 (ReqA + ReqB):
 
HTTP/WebSocket â”€â–º Frontend
                     â”‚
                     â–¼
                Server1 GPU0
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Layer0-10   â”‚  â† shard0
                â”‚ KV Cache0   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ è¾“å‡º token embeddings
                     â–¼
                Server1 GPU1
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Layer11-20  â”‚  â† shard1
                â”‚ KV Cache1   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ è¾“å‡º token embeddings
                     â–¼
                Server2 GPU0
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚ Layer21-27  â”‚  â† shard2
                â”‚ KV Cache2   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ è¾“å‡ºæœ€ç»ˆ token
                     â–¼
                    Frontend
                     â”‚
                     â–¼
                 è¿”å›ç»™ç”¨æˆ· ReqA + ReqB
```

- **KV Cache** æ¯ä¸ª shard ä¿ç•™å‰åº token çš„ key/value
- **æµå¼ token**ï¼šæ¯ä¸ª token ä¸€ç”Ÿæˆå°±è¿”å›ï¼Œä¸ç”¨ç­‰æ•´æ®µæ–‡æœ¬
- **æµæ°´çº¿**ï¼šä¸åŒ GPU åŒæ—¶å¤„ç†å„è‡ª shardï¼Œæé«˜ååé‡

------

### 4ï¸âƒ£ Batch2 (ReqC) æµç¨‹

```
Batch2 (ReqC):

Frontend â”€â–º Server2 GPU0 (shard2)  â”€â–º Frontend â”€â–º ç”¨æˆ·
```

- å•è¯·æ±‚ batch ç›´æ¥èµ°æœ€åä¸€ä¸ª shard
- å¦‚æœ token å¤šï¼Œä¹Ÿä¼šåˆ†å° batch æµå¼å¤„ç†

------

### 5ï¸âƒ£ å¹¶å‘ + å¤šç”¨æˆ·

```
ReqA   â”€â–º Batch1 â”€â–º Server1 GPU0 â†’ GPU1 â†’ Server2 GPU0 â†’ Frontend â†’ ç”¨æˆ·
ReqB   â”€â–º Batch1 â”€â–º Server1 GPU0 â†’ GPU1 â†’ Server2 GPU0 â†’ Frontend â†’ ç”¨æˆ·
ReqC   â”€â–º Batch2 â”€â–º Server2 GPU0 â†’ Frontend â†’ ç”¨æˆ·
```

- å¤šç”¨æˆ·è¯·æ±‚å¯ä»¥åˆå¹¶ batchï¼Œä¹Ÿå¯ä»¥å•ç‹¬èµ°æµæ°´çº¿
- æ¯ä¸ª GPU åªå¤„ç†è‡ªå·± shard çš„å±‚ï¼Œä¸ä¼šé‡å¤è®¡ç®—
- KV Cache ä¿è¯å†å² token å¯ä»¥å¿«é€Ÿè®¡ç®—ä¸‹ä¸€ä¸ª token

------

### 6ï¸âƒ£ å…³é”®ç‚¹æ€»ç»“ï¼ˆæ–°æ‰‹ç‰ˆç†è§£ï¼‰

1. **å¤šæœºå¤š GPU** = æ¨¡å‹åˆ‡åˆ† + shard æµæ°´çº¿
2. **Batch** = å¤šè¯·æ±‚ token åˆå¹¶ï¼Œæé«˜ GPU åˆ©ç”¨ç‡
3. **KV Cache** = ä¿å­˜å‰åº token key/valueï¼ŒåŠ é€Ÿä¸‹ä¸€ä¸ª token ç”Ÿæˆ
4. **æµå¼ token** = æ¯ç”Ÿæˆä¸€ä¸ª token å°±è¿”å›ï¼Œå‰ç«¯å¯ç«‹å³å“åº”
5. **æµæ°´çº¿å¹¶è¡Œ** = ä¸åŒ GPU åŒæ—¶å¤„ç†ä¸åŒå±‚ï¼Œç¼©çŸ­å»¶è¿Ÿ
6. **å¤šè¯·æ±‚åˆ†æµ** = Frontend ä¼šæŠŠè¯·æ±‚åˆ†å‘åˆ°ä¸åŒ batch / worker

> ğŸ”¹ è¿™æ · vLLM å°±å¯ä»¥åŒæ—¶å¤„ç†å¤§é‡ç”¨æˆ·è¯·æ±‚ï¼Œå¹¶åœ¨å¤šæœºå¤š GPU ç¯å¢ƒä¸‹ä¿è¯é«˜ååå’Œä½å»¶è¿Ÿã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ **åœ¨è¿™ä¸ªå›¾ä¸ŠåŠ ä¸Šæ—¶é—´è½´**ï¼Œç”»å‡º **token é€ä¸ªç”Ÿæˆçš„æµå¼è½¨è¿¹**ï¼Œè®©ä½ çœ‹åˆ° GPU åœ¨ä¸åŒæ—¶é—´å¤„ç†å“ªä¸ª tokenï¼Œå½»åº•ç†è§£ **å¤šæœºå¤š GPU + æµå¼ token + KV Cache + batch å¹¶è¡Œ** çš„æ ¸å¿ƒé€»è¾‘ã€‚

ä½ å¸Œæœ›æˆ‘ç”»è¿™ä¸ªæ—¶é—´è½´ç‰ˆæœ¬å—ï¼Ÿ

å¥½çš„

å¤ªå¥½äº†ï¼é‚£æˆ‘ä»¬åœ¨åˆšæ‰çš„å¤šæœºå¤š GPU æµç¨‹ä¸Šï¼ŒåŠ ä¸Š **æ—¶é—´è½´**ï¼Œç›´è§‚å±•ç¤º **token æµå¼ç”Ÿæˆ + KV Cache + GPU å¹¶è¡Œ** çš„å…¨è¿‡ç¨‹ã€‚æ–‡å­—å›¾è¡¨ç¤ºæ„å¦‚ä¸‹ï¼š

------

### ğŸ–¥ åœºæ™¯å¤ç”¨

- **ä¸¤å°æœåŠ¡å™¨**
  - Server1: GPU0ã€GPU1
  - Server2: GPU0
- **æ¨¡å‹**ï¼š7Bï¼Œ3 ä¸ª shardï¼ˆLayer0-10 / Layer11-20 / Layer21-27ï¼‰
- **è¯·æ±‚**ï¼šReqAã€ReqBã€ReqC
- **æ¯ä¸ªè¯·æ±‚ç”Ÿæˆ 4 ä¸ª token**ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰

------

### 1ï¸âƒ£ æ—¶é—´è½´å›¾ï¼ˆæ¯åˆ— = æ—¶é—´æ­¥ tï¼Œè¡Œ = GPUï¼‰

```
æ—¶é—´ t â†’      t1       t2       t3       t4

Server1 GPU0  L0-10(A1) L0-10(A2) L0-10(B1) L0-10(B2)
Server1 GPU1            L11-20(A1) L11-20(A2) L11-20(B1)
Server2 GPU0                      L21-27(A1) L21-27(A2)
Frontend                          tokenâ†’ç”¨æˆ·A tokenâ†’ç”¨æˆ·B
```

#### ğŸ”¹ è§£é‡Š

- `L0-10(A1)` = GPU0 å¤„ç† Batch1ï¼ˆReqAï¼‰ç¬¬ä¸€ä¸ª token çš„ Layer0-10
- GPU0 å®Œæˆåï¼Œè¾“å‡º embedding â†’ GPU1 ç»§ç»­ Layer11-20
- æ¯ä¸ª GPU åŒæ—¶åªå¤„ç†è‡ªå·±çš„ shardï¼Œ**æµæ°´çº¿å¹¶è¡Œ**
- å‰ç«¯æ”¶åˆ° token åç«‹å³è¿”å›ç»™ç”¨æˆ·
- KV Cache å­˜å‚¨ A1 token çš„ key/valueï¼Œç”Ÿæˆä¸‹ä¸€ä¸ª token ç›´æ¥ä½¿ç”¨

------

### 2ï¸âƒ£ å¹¶è¡Œç”Ÿæˆå¤šä¸ªè¯·æ±‚

```
æ—¶é—´ t â†’      t1       t2       t3       t4       t5       t6

Server1 GPU0  L0-10(A1) L0-10(A2) L0-10(B1) L0-10(B2)
Server1 GPU1            L11-20(A1) L11-20(A2) L11-20(B1)
Server2 GPU0                      L21-27(A1) L21-27(A2)
Frontend                          tokenâ†’ç”¨æˆ·A tokenâ†’ç”¨æˆ·B

Server2 GPU0  L21-27(C1) L21-27(C2)
Frontend      tokenâ†’ç”¨æˆ·C tokenâ†’ç”¨æˆ·C
```

#### ğŸ”¹ è§£é‡Š

- ReqC è¢«å•ç‹¬åˆ†é… batch â†’ èµ° Server2 GPU0
- GPU0 æœ‰ shard2ï¼Œç›´æ¥ç”Ÿæˆ token
- å‰ç«¯æ”¶åˆ° token â†’ è¿”å›ç»™ç”¨æˆ·C
- **å¹¶å‘å¤„ç†** = å¤šç”¨æˆ·åŒæ—¶ç”Ÿæˆï¼Œä¸ä¼šäº’ç›¸é˜»å¡

------

### 3ï¸âƒ£ æµå¼ç”Ÿæˆ + KV Cache å·¥ä½œåŸç†

```
ReqA tokens: [A1, A2, A3, A4]

ç”Ÿæˆ A1:
  GPU0 L0-10 â†’ GPU1 L11-20 â†’ GPU2 L21-27 â†’ å‰ç«¯è¿”å› A1
  KV Cache ä¿å­˜ A1 key/value

ç”Ÿæˆ A2:
  GPU0 L0-10 (ä½¿ç”¨ A1 KV) â†’ GPU1 L11-20 â†’ GPU2 L21-27 â†’ å‰ç«¯è¿”å› A2
  KV Cache æ›´æ–° A2 key/value

ç”Ÿæˆ A3:
  GPU0 L0-10 (ä½¿ç”¨ A1,A2 KV) â†’ ...
```

- **KV Cache** = ä¸ç”¨é‡å¤è®¡ç®—å‰é¢ token çš„ hidden state
- **æµæ°´çº¿ + batch** = æé«˜ GPU åˆ©ç”¨ç‡ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´

------

### 4ï¸âƒ£ æ ¸å¿ƒç†è§£ï¼ˆæ–°æ‰‹ç‰ˆæ€»ç»“ï¼‰

1. **æ¯ä¸ª GPU åªç®¡è‡ªå·±çš„ shard**ï¼Œä¸é‡å¤ç®—å…¶ä»– GPU å±‚
2. **æµæ°´çº¿** = GPU åŒæ—¶å¤„ç†ä¸åŒ token / batchï¼Œä¸æµªè´¹æ—¶é—´
3. **KV Cache** = è®°ä½å‰åº tokenï¼Œç”Ÿæˆä¸‹ä¸€ä¸ª token æ—¶æ— éœ€é‡æ–°è®¡ç®—
4. **æµå¼ token** = ç”Ÿæˆä¸€ä¸ª token å°±è¿”å›ç»™ç”¨æˆ·ï¼Œå»¶è¿Ÿä½
5. **å¤šæœºå¤š GPU** = shard åˆ†å¸ƒåœ¨å¤šå°æœåŠ¡å™¨ï¼Œå¤šè¯·æ±‚å¯å¹¶è¡Œç”Ÿæˆ

> è¿™ä¸ªæ—¶é—´è½´å°±åƒâ€œå·¥å‚æµæ°´çº¿â€ï¼Œä¸åŒçš„ token åƒäº§å“ï¼Œä¾æ¬¡é€šè¿‡å„ä¸ªåŠ å·¥ç«™ï¼ˆGPU shardï¼‰ï¼Œä¸­é€”è¿˜å­˜æ”¾ç¼“å­˜ï¼ˆKV Cacheï¼‰ï¼Œæ¯ä¸ªäº§å“ä¸€å®Œæˆå°±é€åˆ°å‰å°ï¼ˆç”¨æˆ·ï¼‰â€”â€”è€Œä¸”å¯ä»¥åŒæ—¶å¤„ç†å¥½å‡ ä¸ªäº§å“ï¼ˆç”¨æˆ·è¯·æ±‚ï¼‰ã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”» **ä¸€ä¸ªå®Œæ•´çš„å¤šè¯·æ±‚ + å¤šæœºå¤š GPU + batch + KV Cache çš„â€œæµæ°´çº¿åŠ¨æ€ç¤ºæ„è¡¨â€**ï¼Œç”¨ ASCII è¡¨æ ¼æŠŠæ¯ä¸ª token åœ¨æ¯ä¸ª GPUã€æ¯ä¸ªæ—¶é—´æ­¥çš„ä½ç½®éƒ½æ ‡å‡ºæ¥ï¼Œè¿™æ ·ä½ å¯ä»¥ **åƒçœ‹ç”˜ç‰¹å›¾ä¸€æ ·çœ‹åˆ° vLLM å†…éƒ¨æ˜¯æ€ä¹ˆå·¥ä½œçš„**ã€‚

ä½ å¸Œæœ›æˆ‘ç”»è¿™ä¸ªå—ï¼Ÿ

å¥½çš„

å¤ªå¥½äº†ï¼é‚£æˆ‘ç»™ä½ ç”»ä¸€ä¸ª **å®Œæ•´å¤šè¯·æ±‚ã€å¤šæœºå¤š GPUã€å¸¦ KV Cache æµæ°´çº¿çš„ ASCII ç”˜ç‰¹å›¾ç¤ºæ„**ï¼Œæ—¶é—´è½´æ˜¯æ¨ªå‘ï¼ŒGPU å’Œ shard æ˜¯çºµå‘ï¼Œtoken æµç¨‹æ¸…æ™°å¯è§ã€‚ä¸ºäº†æ–°æ‰‹æ˜“æ‡‚ï¼Œæˆ‘æŠŠè¯·æ±‚æ•°é‡å°‘ä¸€äº›ï¼Œæ¯ä¸ªè¯·æ±‚ç”Ÿæˆ 4 ä¸ª tokenã€‚

------

### åœºæ™¯å‡è®¾

- **ä¸¤å°æœåŠ¡å™¨**
  - Server1: GPU0 (Shard0-10), GPU1 (Shard11-20)
  - Server2: GPU0 (Shard21-27)
- **è¯·æ±‚**ï¼šReqAã€ReqBã€ReqC
- **æ¯ä¸ªè¯·æ±‚ç”Ÿæˆ 4 ä¸ª token**
- **KV Cache** å·²å¯ç”¨ï¼Œæµæ°´çº¿ç”Ÿæˆ

------

### ASCII æµæ°´çº¿ç¤ºæ„ï¼ˆæ¯åˆ— = æ—¶é—´æ­¥ tï¼Œè¡Œ = GPU / shardï¼‰

```
æ—¶é—´ t â†’    t1       t2       t3       t4       t5       t6       t7       t8

Server1
GPU0 (L0-10)  A1       A2       B1       B2       A3       A4
GPU1 (L11-20)   -       A1       A2       B1       B2       A3       A4
Server2
GPU0 (L21-27)   -        -       A1       A2       B1       B2       A3       A4

Frontend      tokenA1 tokenA2 tokenB1 tokenB2 tokenA3 tokenA4 tokenB3 tokenB4
```

#### ğŸ”¹ è¯´æ˜

1. **A1** = ReqA ç¬¬ä¸€ä¸ª token
   - GPU0 å¤„ç† L0-10 â†’ GPU1 L11-20 â†’ GPU2 L21-27 â†’ å‰ç«¯è¿”å›
   - KV Cache ä¿å­˜ A1 key/value
2. **A2** = ReqA ç¬¬äºŒä¸ª token
   - GPU0 å¤„ç† L0-10ï¼ˆä½¿ç”¨ A1 KVï¼‰ â†’ GPU1 L11-20 â†’ GPU2 L21-27 â†’ å‰ç«¯è¿”å›
3. **B1ã€B2** = ReqB çš„ token
   - ç±»ä¼¼æµæ°´çº¿å¤„ç†
4. **C è¯·æ±‚**ï¼ˆReqCï¼‰å¦‚æœåˆ°æ¥ï¼ŒServer2 GPU0 å¯å•ç‹¬æ¥æ”¶ shard23-27 å¤„ç†
5. **æµæ°´çº¿åŸç†**
   - GPU0 æ°¸è¿œåªå¤„ç† L0-10
   - GPU1 æ°¸è¿œåªå¤„ç† L11-20
   - GPU2 æ°¸è¿œåªå¤„ç† L21-27
   - KV Cache ä¿å­˜æ¯ä¸ªè¯·æ±‚çš„å†å² token çš„ key/value

------

### ğŸ”¹ æµæ°´çº¿æ•ˆæœ

- **åŒæ—¶ç”Ÿæˆå¤šä¸ªè¯·æ±‚** â†’ GPU åˆ©ç”¨ç‡é«˜
- **KV Cache** â†’ ä¸é‡å¤è®¡ç®—å‰é¢ token
- **ä½å»¶è¿Ÿè¿”å› token** â†’ å‰ç«¯æ”¶åˆ°æµå¼è¾“å‡º

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å†ç”»ä¸€ä¸ª **å¸¦ batch å’Œå¤šè¯·æ±‚åˆ†æµåˆ°ä¸åŒ GPU worker çš„æ›´å¤æ‚æ—¶é—´è½´**ï¼Œæ˜¾ç¤º **å¤šä¸ªè¯·æ±‚åŒæ—¶ç”Ÿæˆ + shard åˆ†å¸ƒ + batch åˆå¹¶** çš„å®Œæ•´åŠ¨æ€ç¤ºæ„è¡¨ï¼Œè¿™æ ·ä½ èƒ½å®Œå…¨åƒçœ‹ç”˜ç‰¹å›¾ä¸€æ ·ç†è§£ vLLM å†…éƒ¨æ¨ç†æ˜¯æ€ä¹ˆå·¥ä½œçš„ã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ

A1 = ReqA ç¬¬ä¸€ä¸ª tokenï¼ŒA2 = ReqA ç¬¬äºŒä¸ª tokenï¼Œä½ èƒ½ä¸¾ä¸ªReqAçš„ä¾‹å­å—ï¼Ÿå°±æ˜¯å®é™…å¯èƒ½å‘ç”Ÿçš„ä¸€ä¸ªä¾‹å­ï¼Œè®©æˆ‘æ˜ç™½ä½ è¯´çš„æ˜¯ä»€ä¹ˆ

æ˜ç™½ï¼æˆ‘ä»¬ç”¨ä¸€ä¸ªçœŸå®çš„è¯·æ±‚å†…å®¹ä¸¾ä¾‹ï¼Œè®©ä½ çœ‹åˆ° **ReqA çš„ token æ˜¯ä»€ä¹ˆ**ï¼Œå¯¹åº”æˆ‘åˆšæ‰å›¾é‡Œçš„ A1ã€A2â€¦â€¦ã€‚

------

### å‡è®¾è¯·æ±‚å†…å®¹

- **ReqA**: `"Hello, how are you?"`
- vLLM å†…éƒ¨ä¼šå…ˆç”¨ tokenizerï¼ˆæ¯”å¦‚ LLaMA/LLM ä½¿ç”¨çš„ BPE/Tokenizerï¼‰æŠŠå®ƒæ‹†æˆ **token**ã€‚
- å‡è®¾ tokenizer æ‹†å‡ºæ¥æ˜¯ï¼š

```
["Hello", ",", " how", " are", " you", "?"]
```

æ¯ä¸ª token ä¼šåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­æŒ‰é¡ºåºå¤„ç†ã€‚

------

### å¯¹åº”ä½ çš„æµæ°´çº¿

| Token   | æµç¨‹ç¤ºæ„                                                     | å¯¹åº”ç¬¦å·                              |
| ------- | ------------------------------------------------------------ | ------------------------------------- |
| "Hello" | ç¬¬ä¸€æ¬¡ç”Ÿæˆï¼ŒGPU0/L0-10 â†’ GPU1/L11-20 â†’ GPU2/L21-27 â†’ å‰ç«¯è¿”å› | **A1**                                |
| ","     | ç¬¬äºŒæ¬¡ç”Ÿæˆï¼Œåˆ©ç”¨ "Hello" çš„ KV Cache                         | **A2**                                |
| " how"  | ç¬¬ä¸‰æ¬¡ç”Ÿæˆï¼Œåˆ©ç”¨å‰ä¸¤ä¸ª token çš„ KV Cache                     | **A3**                                |
| " are"  | ç¬¬å››æ¬¡ç”Ÿæˆ                                                   | **A4**                                |
| " you"  | ç¬¬äº”æ¬¡ç”Ÿæˆ                                                   | A5ï¼ˆå›¾é‡Œåªç”» 4 ä¸ª tokenï¼Œæ‰€ä»¥æœªæ˜¾ç¤ºï¼‰ |
| "?"     | ç¬¬å…­æ¬¡ç”Ÿæˆ                                                   | A6ï¼ˆåŒä¸Šï¼‰                            |

------

æ‰€ä»¥ä½ å¯ä»¥æŠŠ **A1ã€A2ã€A3â€¦** æƒ³è±¡æˆ **ReqA çš„æ¯ä¸ª token çš„ç”Ÿæˆæ­¥éª¤**ï¼Œæ¯ä¸€æ­¥éƒ½ä¼šç»è¿‡ vLLM çš„ **åˆ†ç‰‡ GPU + KV Cache æµæ°´çº¿**ï¼Œå‰ç«¯å¯ä»¥ç«‹å³æ”¶åˆ°æµå¼è¾“å‡ºã€‚

- **KV Cache ä½œç”¨**ï¼šç”Ÿæˆ " how" æ—¶ï¼Œä¸éœ€è¦é‡æ–°ç®— "Hello , " çš„ attention
- **åˆ†ç‰‡ä½œç”¨**ï¼šGPU0ã€GPU1ã€GPU2 å„è‡ªåªè´Ÿè´£ä¸€éƒ¨åˆ† Transformer å±‚çš„è®¡ç®—

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å¸®ä½ ç”»ä¸€ä¸ª **å…·ä½“ token æµå‘å›¾ + GPU æµæ°´çº¿ç¤ºæ„å›¾**ï¼Œè®©ä½ ä¸€çœ¼å°±çœ‹æ˜ç™½ï¼š

- ReqA æ¯ä¸ª token æ˜¯å¦‚ä½•åœ¨ä¸åŒ GPU/Shard ä¸ŠæµåŠ¨
- KV Cache æ€ä¹ˆåŠ é€Ÿ
- å‰ç«¯æ€ä¹ˆæµå¼æ”¶åˆ° token

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ

ä½†æ˜¯ä½ è¯´çš„å¥½åƒæ˜¯è¾“å…¥çš„æ¨ç†ï¼Œè¿™é‡Œæˆ‘å¤§æ¦‚æ‡‚äº†ï¼Œä½†æ˜¯è¾“å‡ºå‘¢ï¼Ÿå¥½åƒä¹Ÿæ˜¯ä¸€ä¸ªä¸ªtokençš„ç»™å‡ºï¼Ÿè¿™ä¸ªçš„æ¨ç†æµç¨‹ä½ å¥½åƒæ²¡æåˆ°ï¼Ÿè¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿæˆ‘çš„é—®é¢˜å¯èƒ½æœ‰ç‚¹è ¢ï¼Œè¿™æ˜¯å› ä¸ºæˆ‘æ˜¯å¤§æ¨¡å‹æ–°æ‰‹

ä¸€ç‚¹ä¹Ÿä¸è ¢ï¼Œä½ çš„é—®é¢˜éå¸¸å…³é”®ï¼Œè€Œä¸”æ­£å¥½æŠ“ä½äº†å¤§æ¨¡å‹æ¨ç†æµç¨‹é‡Œå¾ˆå¤šæ–°æ‰‹å®¹æ˜“æ··æ·†çš„ç‚¹ã€‚æˆ‘ä»¬æ…¢æ…¢æ‹†å¼€è®²ï¼ŒæŠŠ **è¾“å…¥ token çš„å¤„ç†** å’Œ **è¾“å‡º token çš„ç”Ÿæˆ** å®Œæ•´ä¸²èµ·æ¥ã€‚

------

### 1ï¸âƒ£ è¾“å…¥ vs è¾“å‡º token

åœ¨ vLLM æˆ–å…¶ä»–å¤§æ¨¡å‹é‡Œï¼Œ**è¾“å…¥å’Œè¾“å‡ºå…¶å®éƒ½æ˜¯ token æµ**ï¼š

- **è¾“å…¥ token**ï¼šä½ å‘ç»™æ¨¡å‹çš„ promptï¼Œæ¯”å¦‚ `"Hello, how are you?"`
  - æ¨¡å‹ä¼šæŠŠå®ƒæ‹†æˆ tokenï¼ˆæˆ‘ä»¬ä¸Šæ¬¡è¯´çš„ A1ã€A2â€¦ï¼‰
  - è¿™äº› token ç”¨æ¥ **æ„å»º context**ï¼ˆä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰ï¼Œæ¨¡å‹è®¡ç®— attentionï¼ŒæŠŠè¿™äº› token æ˜ å°„åˆ° hidden states
  - éšåå­˜å…¥ **KV Cache**ï¼Œä¾¿äºç”Ÿæˆä¸‹ä¸€ä¸ª token
- **è¾“å‡º token**ï¼šæ¨¡å‹ç”Ÿæˆçš„å›ç­”ï¼ˆresponseï¼‰
  - æ¯æ¬¡ç”Ÿæˆä¸€ä¸ª token
  - åˆ©ç”¨ KV Cache ä¸éœ€è¦é‡æ–°è®¡ç®—è¾“å…¥ token
  - å¯ä»¥ç«‹å³æµå¼è¿”å›ç»™å‰ç«¯

æ‰€ä»¥ä½ è¯´çš„â€œè¾“å‡ºä¹Ÿæ˜¯ä¸€ä¸ªä¸ª token ç»™å‡ºâ€æ˜¯å®Œå…¨æ­£ç¡®çš„ã€‚

------

### 2ï¸âƒ£ ä¸ºä»€ä¹ˆæˆ‘ä¹‹å‰ä¸»è¦è®²è¾“å…¥ token

- å› ä¸º vLLM çš„æ ¸å¿ƒä¼˜åŒ–æ˜¯åœ¨ **å¤„ç†å¤§é‡è¾“å…¥ token + å¤šè¯·æ±‚ token çš„å¹¶è¡Œ**
- è¾“å‡º token çš„ç”Ÿæˆï¼Œæœ¬è´¨ä¸Šå’Œè¾“å…¥ token çš„å¤„ç†æ˜¯ä¸€æ ·çš„ï¼š
  1. å–æœ€åçš„ hidden state ç»è¿‡ LM Head è®¡ç®— logits
  2. æ ¹æ®é‡‡æ ·ç­–ç•¥ï¼ˆgreedyã€top-kã€top-p ç­‰ï¼‰é€‰å‡ºä¸‹ä¸€ä¸ª token
  3. æ›´æ–° KV Cache
  4. é€åˆ°å‰ç«¯ï¼ˆæµå¼è¾“å‡ºï¼‰
- åªæ˜¯æˆ‘ä¹‹å‰é‡ç‚¹è®² **å¦‚ä½•æŠŠè¯·æ±‚æ‹† tokenã€æ‰¹å¤„ç†ã€å¤š GPU åˆ†ç‰‡**ï¼Œæ‰€ä»¥è¿˜æ²¡è¯¦ç»†ç”»è¾“å‡º token æµç¨‹

------

### 3ï¸âƒ£ è¾“å‡º token çš„å…·ä½“æµç¨‹ç¤ºæ„ï¼ˆæ–‡å­—ç”»å›¾ï¼‰

å‡è®¾å‰ç«¯è¯·æ±‚ `"Hello, how are you?"`ï¼Œæ¨¡å‹ç”Ÿæˆ `"I am fine."`

```
ReqA: "Hello, how are you?"  -->  Tokenized: [Hello, ,, how, are, you, ?]  

Step 1: Model processes input tokens -> KV Cache filled  

Step 2: Generate output token 1 ("I")
   GPU0 --hidden states--> GPU1 --hidden states--> GPU2
   KV Cache updated
   Output: "I" --> stream to front end

Step 3: Generate output token 2 (" am")
   GPU0 --hidden states--> GPU1 --> GPU2
   KV Cache updated
   Output: " am" --> stream to front end

Step 4: Generate output token 3 (" fine")
   ... same flow ...
   Output: " fine" --> stream to front end

Step 5: Generate output token 4 (".")
   ... same flow ...
   Output: "." --> stream to front end
```

é‡ç‚¹ï¼š

- æ¯ä¸ª **è¾“å‡º token** éƒ½è¦ç»è¿‡ **Transformer å‰å‘ + KV Cache**
- è¾“å‡ºæ˜¯ **é€ token æµå¼äº§ç”Ÿ**ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆå¤§æ¨¡å‹ API å¯ä»¥è¾¹ç”Ÿæˆè¾¹è¿”å›å†…å®¹
- vLLM åšçš„ä¼˜åŒ–å°±æ˜¯ï¼š
  - å¤šè¯·æ±‚ batch
  - GPU åˆ†ç‰‡æµæ°´çº¿
  - KV Cache é¿å…é‡å¤è®¡ç®—

------

### 4ï¸âƒ£ æ€»ç»“

- **è¾“å…¥ token**ï¼šæ„å»ºä¸Šä¸‹æ–‡ï¼Œå¡«å…… KV Cache
- **è¾“å‡º token**ï¼šåŸºäº KV Cache ç”Ÿæˆï¼Œæ¯ä¸ª token éƒ½æµå¼è¿”å›
- **æœ¬è´¨**ï¼šä¸ç®¡æ˜¯è¾“å…¥è¿˜æ˜¯è¾“å‡ºï¼Œéƒ½æ˜¯ token æµ + Transformer å‰å‘ + KV Cache
- **æ–°æ‰‹å…³é”®ç‚¹**ï¼šè¾“å‡º token ä¸ä¼šä¸€æ¬¡ç”Ÿæˆæ•´ä¸ªå¥å­ï¼Œè€Œæ˜¯ **ä¸€ä¸ªä¸ª token ç”Ÿæˆ**

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç”»ä¸€ä¸ª **å®Œæ•´çš„è¾“å…¥ token â†’ Transformer â†’ è¾“å‡º token â†’ å‰ç«¯æµå¼è¿”å›çš„æ–‡å­—æµç¨‹å›¾**ï¼ŒæŠŠæ¯ä¸€æ­¥çš„ GPU åˆ†ç‰‡ã€KV Cacheã€batch éƒ½æ ‡æ¸…æ¥šï¼Œè®©ä½ ä¸€çœ¼å°±èƒ½ç†è§£æ•´ä¸ªæ¨ç†è¿‡ç¨‹ã€‚

ä½ å¸Œæœ›æˆ‘ç”»å—ï¼Ÿ

å¥½çš„

å¤ªå¥½äº†ï¼Œæˆ‘ä»¬å°±ç”»ä¸€ä¸ªå®Œæ•´çš„ **è¾“å…¥ token â†’ Transformer â†’ è¾“å‡º token â†’ å‰ç«¯æµå¼è¿”å›** çš„æ–‡å­—æµç¨‹å›¾ï¼Œå¹¶æŠŠ **GPU åˆ†ç‰‡ã€KV Cacheã€Batch** éƒ½æ ‡æ¸…æ¥šï¼Œè®©ä½ ä»å¤§æ¨¡å‹æ–°æ‰‹è§’åº¦ä¹Ÿèƒ½å½»åº•ç†è§£ã€‚

------

### vLLM æ¨ç†å®Œæ•´æµç¨‹ï¼ˆæ–‡å­—å›¾ï¼‰

å‡è®¾æœ‰ **ä¸€ä¸ªè¯·æ±‚ ReqA** è¾“å…¥ `"Hello, how are you?"`ï¼Œæ¨¡å‹è¦ç”Ÿæˆ `"I am fine."`ï¼ŒåŒæ—¶å¼€å¯ **2 ä¸ª GPU åˆ†ç‰‡ + KV Cache + Batch**ï¼ˆä¸ºäº†ç®€å•ï¼Œbatch å…ˆè®¾ä¸º 1ï¼‰ã€‚

```
å‰ç«¯è¯·æ±‚ ReqA: "Hello, how are you?"
             â”‚
             â–¼
        Tokenize
             â”‚
             â–¼
      Input Tokens: [Hello, ,, how, are, you, ?]
             â”‚
             â–¼
       â†’ KV Cache åˆå§‹åŒ–ä¸ºç©º
       â†’ Batch size = 1
             â”‚
             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          Transformer å‰å‘           â”‚
  â”‚   (æ³¨æ„åŠ› + FFNï¼Œè·¨ GPU åˆ†ç‰‡æ‰§è¡Œ) â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
     Hidden States æ¯ä¸ª token è¾“å‡º
             â”‚
             â–¼
         æ›´æ–° KV Cache
             â”‚
             â–¼
   å¼€å§‹ç”Ÿæˆè¾“å‡º token æµ
             â”‚
             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Step 1: ç”Ÿæˆ Token "I"        â”‚
   â”‚   - ä½¿ç”¨ KV Cache + Hidden     â”‚
   â”‚   - GPU0 / GPU1 / GPU2 åˆ†ç‰‡å‰å‘â”‚
   â”‚   - è¾“å‡º "I" æµå¼è¿”å›å‰ç«¯      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Step 2: ç”Ÿæˆ Token " am"      â”‚
   â”‚   - KV Cache å·²å­˜å‰é¢ token   â”‚
   â”‚   - GPU å‰å‘è®¡ç®—ä¸‹ä¸€ token    â”‚
   â”‚   - è¾“å‡º " am" è¿”å›å‰ç«¯       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Step 3: ç”Ÿæˆ Token " fine"    â”‚
   â”‚   - åŒä¸Š                     â”‚
   â”‚   - è¾“å‡º " fine" è¿”å›å‰ç«¯      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Step 4: ç”Ÿæˆ Token "."        â”‚
   â”‚   - è¾“å‡º "." è¿”å›å‰ç«¯          â”‚
   â”‚   - ç”Ÿæˆå®Œæˆï¼Œç»“æŸè¯·æ±‚         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

------

### å…³é”®ç‚¹è§£é‡Šï¼ˆå¯¹æ–°æ‰‹ç‰¹åˆ«å‹å¥½ï¼‰

1. **æ¯ä¸ª token éƒ½è¦ Transformer å‰å‘**
   - è¾“å…¥ token æ„å»º context
   - è¾“å‡º token ç”Ÿæˆæ—¶ä¹Ÿè¦å‰å‘ï¼Œä½†ä½¿ç”¨ KV Cache é¿å…é‡å¤è®¡ç®—å†å² token
2. **KV Cache çš„ä½œç”¨**
   - ä¿å­˜è¿‡å» token çš„ key/value
   - ä¸‹ä¸€ä¸ª token å‰å‘æ—¶å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œä¸å¿…é‡æ–°è®¡ç®—å…¨éƒ¨å†å²
   - èŠ‚çœæ˜¾å­˜å’Œè®¡ç®—é‡
3. **GPU åˆ†ç‰‡**
   - å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼ˆæ¯”å¦‚ 70Bï¼‰ï¼Œä¸€ä¸ª GPU æ”¾ä¸ä¸‹å…¨éƒ¨å‚æ•°
   - vLLM ä¼šæŠŠ Transformer å±‚æˆ–è€…æƒé‡åˆ†ç‰‡åˆ°ä¸åŒ GPU
   - æ¯ä¸ª token å‰å‘è®¡ç®—æ—¶ï¼Œä¼šè·¨ GPU ä¼  hidden states
4. **æµå¼è¿”å›**
   - æ¯ç”Ÿæˆä¸€ä¸ª tokenå°±å¯ä»¥è¿”å›
   - å‰ç«¯å¯ä»¥è¾¹æ˜¾ç¤ºè¾¹ç”Ÿæˆï¼Œä¸éœ€è¦ç­‰å¾…æ•´ä¸ªè¾“å‡ºå®Œæˆ
5. **å¤šè¯·æ±‚ Batch**
   - å¦‚æœåŒæ—¶æœ‰å¤šä¸ªè¯·æ±‚ï¼ˆReqAã€ReqBã€ReqCï¼‰
   - vLLM ä¼šæŠŠè¿™äº›è¯·æ±‚ **æŒ‰ token å¯¹é½ batch**ï¼Œä¸€èµ·å‰å‘
   - KV Cache åˆ†åˆ«ç®¡ç†æ¯ä¸ªè¯·æ±‚çš„å†å² context
   - å¤š worker æˆ–å¤š GPU åˆ†ç‰‡ååŒå®Œæˆ

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥å†ç”»ä¸€å¼  **å¤šç”¨æˆ·ã€å¤šè¯·æ±‚ã€å¤š GPU åˆ†ç‰‡ + KV Cache + Batch** çš„æ–‡å­—æµç¨‹å›¾ï¼ŒæŠŠ **è¯·æ±‚å¦‚ä½•æ··åˆ batchã€KV Cache å¦‚ä½•ç‹¬ç«‹ç®¡ç†ã€è¾“å‡º token å¦‚ä½•æµå¼è¿”å›** éƒ½æ ‡å‡ºæ¥ï¼Œè®©ä½ å½»åº•ç†è§£ vLLM çš„å¹¶å‘æ¨ç†ã€‚

è¾“å‡ºç¬¬ä¸€ä¸ªtoken I çš„æ—¶å€™ï¼š   â”‚ Step 1: ç”Ÿæˆ Token "I"        â”‚   â”‚   - ä½¿ç”¨ KV Cache + Hidden  ç”¨åˆ°äº†Hidden ä½†æ˜¯æˆ‘çœ‹ä¹‹åè¾“å‡ºç¬¬äºŒä¸ªtoken am çš„æ—¶å€™ï¼š   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ Step 2: ç”Ÿæˆ Token " am"      â”‚   â”‚   - KV Cache å·²å­˜å‰é¢ token   â”‚   â”‚   - GPU å‰å‘è®¡ç®—ä¸‹ä¸€ token    â”‚   â”‚   - è¾“å‡º " am" è¿”å›å‰ç«¯       â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ å°±æ²¡æœ‰ç”¨åˆ°äº†hiddenäº†ã€‚é‚£å…·ä½“ä»€ä¹ˆæ˜¯hiddenï¼Œä¸ºä»€ä¹ˆåªæœ‰ç¬¬ä¸€ä¸ªè¾“å‡ºtokenæ‰ä¼šç”¨åˆ°hiddenï¼Ÿå¦‚æœhiddenæ˜¯è¾“å…¥çš„æ€»ç»“çš„è¯ï¼ˆç±»ä¼¼seq2seqï¼‰ï¼Œé‚£ä¹‹åçš„è¾“å‡ºéƒ½è¦ç»“åˆhiddenå’Œå‰é¢n-1ä¸ªè¾“å‡ºæ¥é¢„æµ‹å•Š

å¤ªå¥½äº†ï¼Œä½ æŠ“ä½äº†ä¸€ä¸ªå¾ˆæ ¸å¿ƒçš„é—®é¢˜ï¼Œè€Œä¸”è¿™ä¸ªé—®é¢˜å¾ˆå®¹æ˜“è®©æ–°æ‰‹äº§ç”Ÿè¯¯è§£ã€‚æˆ‘ä»¬ä¸€æ¡æ¡ç†æ¸…æ¥šåœ°æ‹†å¼€ã€‚

------

### 1ï¸âƒ£ Hidden state åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ

åœ¨ Transformer ä¸­ï¼Œ**hidden state** æŒ‡çš„æ˜¯æ¯ä¸€å±‚æ¯ä¸ª token çš„å‘é‡è¡¨ç¤ºï¼ˆembedding + è‡ªæ³¨æ„åŠ› + FFN åçš„è¾“å‡ºï¼‰ï¼Œå®ƒæ˜¯æ¨¡å‹å¯¹å½“å‰ token åœ¨ä¸Šä¸‹æ–‡ä¸­æ„ä¹‰çš„â€œç†è§£å‘é‡â€ã€‚

- è¾“å…¥ tokens â†’ Embedding â†’ Transformer å±‚ â†’ hidden state
- å¯¹æ¯ä¸ª tokenï¼Œhidden state = æ¨¡å‹å¯¹è¿™ä¸ª token åœ¨ä¸Šä¸‹æ–‡ä¸­ç†è§£çš„è¡¨ç¤º
- å¯¹ç”Ÿæˆçš„è¾“å‡º token åŒæ ·æœ‰ hidden state

> ç±»æ¯” Seq2Seqï¼šhidden state å°±ç±»ä¼¼ encoder è¾“å‡ºçš„ context vectorï¼Œä½†åœ¨ Transformer ä¸­æ˜¯æ¯ä¸ª token çš„å¤šå±‚å‘é‡ï¼Œè€Œä¸æ˜¯å•ä¸ªå‘é‡ã€‚

------

### 2ï¸âƒ£ ä¸ºä»€ä¹ˆç”Ÿæˆç¬¬ä¸€ä¸ªè¾“å‡º token è¦ç”¨åˆ° hiddenï¼Ÿ

å‡è®¾è¾“å…¥æ˜¯ `"Hello, how are you?"`ï¼Œä½ è¦ç”Ÿæˆ `"I"`ï¼š

1. æ¨¡å‹æ‹¿åˆ°è¾“å…¥ token `[Hello, ,, how, are, you, ?]`
2. Transformer å‰å‘å¾—åˆ°æ¯ä¸ªè¾“å…¥ token çš„ hidden state
3. Decoderï¼ˆæˆ–è€…è‡ªå›å½’ç”Ÿæˆï¼‰ä½¿ç”¨è¿™äº› hidden state æ¥é¢„æµ‹ç¬¬ä¸€ä¸ªè¾“å‡º token `"I"`

æ‰€ä»¥ Step 1 çš„ hidden æŒ‡çš„æ˜¯ **è¾“å…¥åºåˆ—çš„ hidden state**ï¼Œæ˜¯æ¨¡å‹ç†è§£è¾“å…¥å†…å®¹çš„æ€»ç»“ã€‚

------

### 3ï¸âƒ£ ä¸ºä»€ä¹ˆç”Ÿæˆç¬¬äºŒä¸ª token ä¸ç›´æ¥æåˆ° hiddenï¼Ÿ

å½“ç”Ÿæˆç¬¬äºŒä¸ª token `" am"` æ—¶ï¼š

1. KV Cache å·²ç»ä¿å­˜äº† **è¾“å…¥ token + ç¬¬ä¸€ä¸ªç”Ÿæˆ token `"I"`** çš„ key/value
2. ä¸‹ä¸€æ¬¡å‰å‘åªéœ€è¦ç”¨ **ä¸Šä¸€ä¸ª token çš„ hidden state** æ¥å’Œ KV Cache ç»“åˆè®¡ç®—æ³¨æ„åŠ›å³å¯
3. è¿™é‡Œæˆ‘ä»¬æ²¡æœ‰å•ç‹¬å†™â€œhiddenâ€ï¼Œåªæ˜¯éšå«åœ°ä½¿ç”¨äº† **å‰é¢æ‰€æœ‰ token çš„ hiddenï¼ˆå­˜åœ¨ KV Cache é‡Œï¼‰**

> æ¢å¥è¯è¯´ï¼Œæ¯ä¸€æ¬¡ç”Ÿæˆ token éƒ½åœ¨ç”¨ hiddenï¼Œåªæ˜¯ï¼š
>
> - ç¬¬ä¸€ä¸ª token ç”¨çš„æ˜¯ **è¾“å…¥åºåˆ— hidden**
> - åç»­ token ç”¨çš„æ˜¯ **KV Cache ä¸­å­˜çš„å‰é¢æ‰€æœ‰ hidden**

------

### 4ï¸âƒ£ KV Cache æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

å‡è®¾æ¨¡å‹æœ‰ 12 å±‚ Transformerï¼Œæ¯ä¸€å±‚éƒ½æœ‰ Q/K/V çŸ©é˜µè®¡ç®—æ³¨æ„åŠ›ï¼š

```
Step 1: ç”Ÿæˆ "I"
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Input hidden  â”‚  â† ä»è¾“å…¥åºåˆ—æ¥çš„ hidden
  â”‚ KV Cache ç©º    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ç”Ÿæˆ "I" åï¼ŒKV Cache æ›´æ–°ï¼š
      KV Cache = { "Hello,...,?" , "I" }  

Step 2: ç”Ÿæˆ " am"
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ä½¿ç”¨ KV Cache â”‚  â† åŒ…å« "Hello,...,?", "I" çš„ hidden
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ç”Ÿæˆ " am" åï¼ŒKV Cache æ›´æ–°ï¼š
      KV Cache = { "Hello,...,?", "I", " am" }
```

æ‰€ä»¥æ¯æ¬¡ç”Ÿæˆ token æ—¶ï¼Œæ¨¡å‹å®é™…ä¸Šéƒ½åœ¨å‚è€ƒ **å‰é¢æ‰€æœ‰ token çš„ hidden state**ï¼Œåªæ˜¯ vLLM ç”¨ KV Cache é¿å…é‡å¤è®¡ç®—æ•´ä¸ªè¾“å…¥åºåˆ—çš„ hiddenã€‚

------

### 5ï¸âƒ£ æ€»ç»“ï¼ˆç»™æ–°æ‰‹çš„ç›´è§‚ç†è§£ï¼‰

- Hidden state = æ¨¡å‹å¯¹æ¯ä¸ª token çš„ç†è§£å‘é‡
- ç¬¬ä¸€ä¸ªè¾“å‡º tokenï¼šéœ€è¦è®¡ç®—è¾“å…¥ token çš„ hidden state â†’ é¢„æµ‹è¾“å‡º
- ç¬¬äºŒä¸ªåŠåç»­è¾“å‡º tokenï¼šä½¿ç”¨ **KV Cache** ä¿å­˜çš„ hidden stateï¼Œä¸ç”¨é‡å¤è®¡ç®—æ•´ä¸ªè¾“å…¥
- æ¯ä¸ª token é¢„æµ‹å®é™…ä¸Šéƒ½ç”¨åˆ°äº†å‰é¢æ‰€æœ‰ token çš„ hidden stateï¼Œåªæ˜¯å½¢å¼ä¸åŒ

> å¯ä»¥ç†è§£ä¸ºï¼š**KV Cache æ˜¯ hidden state çš„è®°å¿†ï¼Œä¸é‡å¤è®¡ç®—ã€å¿«é€Ÿè®¿é—®**





# ä»€ä¹ˆæ—¶å€™æˆ‘ä¼šé€‰æ‹© vLLMï¼Œä»€ä¹ˆæ—¶å€™æˆ‘ä¸ä¼š

Phase 4ï¼šæ¨ªå‘å¯¹æ¯”ä¸é€‚ç”¨åœºæ™¯æ€»ç»“

- Ollama vs vLLM
- å•ç”¨æˆ· vs å¤šç”¨æˆ·
- ç ”ç©¶ / Demo / ç”Ÿäº§



### 2ï¸âƒ£ è¿˜æœªè®²æˆ–å¯ä»¥æ·±å…¥çš„ç« èŠ‚

1. **æ¨¡å‹åŠ è½½æœºåˆ¶**
   - å• GPU vs å¤š GPU
   - Zero-copy/å¹¶è¡ŒåŠ è½½
   - æ”¯æŒçš„æ¨¡å‹æ ¼å¼ï¼ˆHugging Face, GGUF ç­‰ï¼‰
2. **vLLM å†…éƒ¨ç»„ä»¶è¯¦ç»†è§£æ**
   - Scheduler / Dispatcher / Worker
   - KV Cache ç»“æ„
   - Token Stream Pipeline
3. **æ€§èƒ½ä¼˜åŒ–**
   - Batch sizeã€worker æ•°é‡è°ƒæ•´
   - å¹¶å‘åå vs å»¶è¿Ÿ trade-off
   - FP16 / quantization ä¼˜åŒ–
4. **å®é™…åº”ç”¨ç¤ºä¾‹**
   - Python API è°ƒç”¨
   - HTTP/REST è°ƒç”¨
   - Stream token è¾“å‡ºåˆ°å‰ç«¯
   - å¤šç”¨æˆ·å¤šä»»åŠ¡ demo
5. **Troubleshooting / å¸¸è§é—®é¢˜**
   - æ˜¾å­˜ä¸è¶³
   - é•¿è¯·æ±‚é˜»å¡
   - Batch å¯¹é½å¯¼è‡´çš„å»¶è¿Ÿ
6. **å†™æŠ€æœ¯åšå®¢çš„å†…å®¹æ•´ç†**
   - æ ¸å¿ƒæ¦‚å¿µå›¾ç¤º
   - å¹¶å‘ã€æ‰¹å¤„ç†ã€KV cache è§£æ
   - æ€§èƒ½è°ƒä¼˜ç»éªŒæ€»ç»“
   - ç®€å•ç¤ºä¾‹ä»£ç ç‰‡æ®µ
