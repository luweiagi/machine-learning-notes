# ä¿å­˜åŠ è½½æ¨¡å‹æ–‡ä»¶

* [è¿”å›ä¸Šå±‚ç›®å½•](../pytorch.md)
* [ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹](#ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹)
  * [ä¿å­˜æ•´ä¸ªæ¨¡å‹](#ä¿å­˜æ•´ä¸ªæ¨¡å‹)
  * [åŠ è½½æ•´ä¸ªæ¨¡å‹](#åŠ è½½æ•´ä¸ªæ¨¡å‹)
* [ä»…ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰](#ä»…ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰)
  * [ä¿å­˜æ¨¡å‹å‚æ•°](#ä¿å­˜æ¨¡å‹å‚æ•°)
  * [åŠ è½½æ¨¡å‹å‚æ•°](#åŠ è½½æ¨¡å‹å‚æ•°)
* [ä½¿ç”¨jitä¿å­˜ä¸ºTorchScriptæ ¼å¼](#ä½¿ç”¨jitä¿å­˜ä¸ºTorchScriptæ ¼å¼)

pytorchå®˜ç½‘ä¿å­˜åŠ è½½æ¨¡å‹æ–‡ä»¶çš„æ•™ç¨‹ï¼š

[https://pytorch.org/tutorials/beginner/saving_loading_models.html](https://pytorch.org/tutorials/beginner/saving_loading_models.html)

# ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹

## ä¿å­˜æ•´ä¸ªæ¨¡å‹

```python
import torch
import torch.nn as nn

net = nn.Sequential(nn.Linear(128, 16), nn.ReLU(), nn.Linear(16, 1))
print(net)

# ä¿å­˜æ•´ä¸ªæ¨¡å‹ï¼ŒåŒ…å«æ¨¡å‹ç»“æ„å’Œå‚æ•°
torch.save(net, 'sample_model.pt')
```

è¾“å‡ºï¼š

```shell
Sequential(
  (0): Linear(in_features=128, out_features=16, bias=True)
  (1): ReLU()
  (2): Linear(in_features=16, out_features=1, bias=True)
)
```

## åŠ è½½æ•´ä¸ªæ¨¡å‹

```python
import torch
import torch.nn as nn

# åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼ŒåŒ…å«æ¨¡å‹ç»“æ„å’Œå‚æ•°
loaded_model = torch.load('sample_model.pt')
print(loaded_model)
```

è¾“å‡ºï¼š

```shell
Sequential(
  (0): Linear(in_features=128, out_features=16, bias=True)
  (1): ReLU()
  (2): Linear(in_features=16, out_features=1, bias=True)
)
```

# ä»…ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰

## ä¿å­˜æ¨¡å‹å‚æ•°

```python
import torch
import torch.nn as nn
 
model = nn.Sequential(nn.Linear(128, 16), nn.ReLU(), nn.Linear(16, 1))
 
# ä¿å­˜æ•´ä¸ªæ¨¡å‹
torch.save(model.state_dict(), 'sample_model.pt')
```

## åŠ è½½æ¨¡å‹å‚æ•°

```python
import torch
import torch.nn as nn
 
# ä¸‹è½½æ¨¡å‹å‚æ•° å¹¶æ”¾åˆ°æ¨¡å‹ä¸­
loaded_model = nn.Sequential(nn.Linear(128, 16), nn.ReLU(), nn.Linear(16, 1))
loaded_model.load_state_dict(torch.load('sample_model.pt'))
print(loaded_model)
```

è¾“å‡ºï¼š

```shell
Sequential(
  (0): Linear(in_features=128, out_features=16, bias=True)
  (1): ReLU()
  (2): Linear(in_features=16, out_features=1, bias=True)
)
```

ä½ ä¼šå¥½å¥‡å¦‚æœåªæ˜¯å•çº¯çš„`torch.load('sample_model.pt')`ä¼šæ˜¯ä»€ä¹ˆï¼Ÿ

é‚£å°±æ‰“å°ä¸€ä¸‹çœ‹çœ‹ï¼š

```python
import torch
loaded_model = torch.load('sample_model.pt')
print(loaded_model)
```



```shell
OrderedDict([('0.weight', tensor([[-0.0798,  0.0245,  0.0880,  ..., -0.0812,  0.0253, -0.0277],
        [ 0.0382,  0.0644,  0.0483,  ...,  0.0039, -0.0329, -0.0226],
        [ 0.0399,  0.0307, -0.0601,  ...,  0.0154,  0.0748, -0.0678],
        ...,
        [ 0.0279, -0.0479,  0.0126,  ...,  0.0778,  0.0654,  0.0521],
        [ 0.0613,  0.0283,  0.0219,  ..., -0.0807,  0.0087, -0.0058],
        [ 0.0824,  0.0022,  0.0803,  ..., -0.0146,  0.0389, -0.0284]])), ('0.bias', tensor([ 0.0554,  0.0607, -0.0356,  0.0661, -0.0491, -0.0182, -0.0611,  0.0212,
         0.0386,  0.0012, -0.0663, -0.0005,  0.0487, -0.0223, -0.0781,  0.0154])), ('2.weight', tensor([[-0.0585, -0.0603,  0.0626, -0.2448,  0.0612,  0.0704, -0.0561, -0.0535,
         -0.2328, -0.2104, -0.2206, -0.1715,  0.2299, -0.2423, -0.0247, -0.0756]])), ('2.bias', tensor([0.0428]))])
```

å¯ä»¥å‘ç°ï¼Œæ˜¯å­—å…¸ï¼ˆstate_dictï¼‰ï¼ŒåŒ…æ‹¬äº†æ¯ä¸€å±‚çš„åå­—å’Œå‚æ•°ï¼Œä½†æ˜¯ä¸æ˜¯æ•´ä¸ªæ¨¡å‹ç»“æ„ã€‚

> æ˜¯ä»€ä¹ˆstate_dictï¼šPyTorchä¸­çš„state_dictæ˜¯ä¸€ä¸ªpythonå­—å…¸å¯¹è±¡ï¼Œå°†æ¯ä¸ªå±‚æ˜ å°„åˆ°å…¶å‚æ•°Tensorã€‚state_dictå¯¹è±¡å­˜å‚¨æ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°ï¼Œå³æƒé‡å’Œåå·®ï¼Œå¹¶ä¸”å¯ä»¥éå¸¸å®¹æ˜“åœ°åºåˆ—åŒ–å’Œä¿å­˜ã€‚

# åŠ è½½æ¨¡å‹å‚æ•°æ—¶deveceçš„é€‰æ‹©

```python
model.load_state_dict(torch.load(path, map_location=device))
```

å¿…é¡»è¦åŠ map_location=deviceå—ï¼Œä¸åŠ æ˜¯ä¸æ˜¯å°±é”™äº†ï¼Ÿæ¯”å¦‚æˆ‘æœ¬æ¥æ˜¯åœ¨gpuä¸Šè®­ç»ƒï¼Œç„¶åè®­ç»ƒä¸­æ–­ï¼Œç„¶åè¦é‡æ–°åŠ è½½æ¨¡å‹å‚æ•°

è¿™ä¸ªé—®é¢˜é—®å¾—éå¸¸å…³é”®ï¼æˆ‘æ¥åˆ†æƒ…å†µè¯¦ç»†è®²ï¼š

å¦‚æœä½ å†™ï¼š

```python
model.load_state_dict(torch.load(path))
```

**é»˜è®¤è¡Œä¸º**æ˜¯ï¼š

- `torch.load(path)` ä¼š**å°è¯•**æŠŠä¿å­˜æ—¶çš„è®¾å¤‡ï¼ˆæ¯”å¦‚ GPUï¼‰ä¸€èµ·æ¢å¤å‡ºæ¥ã€‚
- å¦‚æœä½ ç°åœ¨**æ­£åœ¨ç”¨çš„è®¾å¤‡**å’Œ**ä¿å­˜æ—¶çš„è®¾å¤‡**ä¸€æ ·ï¼Œæ¯”å¦‚ä½ ä¿å­˜æ—¶åœ¨ GPUï¼Œç°åœ¨åŠ è½½æ—¶ä¹Ÿæ˜¯åœ¨ GPUï¼Œå°±æ²¡é—®é¢˜ã€‚

ä½†æ˜¯å¦‚æœï¼š

- ä½ ä¿å­˜æ—¶æ˜¯åœ¨ GPU (`cuda`) ä¸Šï¼Œ
- ç°åœ¨åŠ è½½æ˜¯åœ¨ CPUï¼ˆæ¯”å¦‚æœ‰æ—¶å€™åªæƒ³åœ¨ CPU è°ƒè¯•ä¸€ä¸‹ï¼‰ï¼Œ

å°±ä¼šç›´æ¥æŠ¥é”™ï¼š

> `RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False.`
>  ï¼ˆå¤§æ„å°±æ˜¯ï¼šæ‰¾ä¸åˆ° CUDA è®¾å¤‡ï¼Œä½†ä½ ä¿å­˜çš„æ˜¯ CUDA çš„ Tensorï¼‰

æ‰€ä»¥ï¼Œæ€»ç»“ä¸€ä¸‹ï¼š

| åœºæ™¯                                     | è¦ä¸è¦åŠ  `map_location`              |
| ---------------------------------------- | ------------------------------------ |
| ä¿å­˜æ—¶å’ŒåŠ è½½æ—¶è®¾å¤‡ä¸€è‡´ï¼Œæ¯”å¦‚éƒ½æ˜¯ GPU     | **å¯ä»¥ä¸åŠ **                         |
| ä¿å­˜æ—¶åœ¨ GPUï¼ŒåŠ è½½æ—¶åœ¨ CPU               | **å¿…é¡»åŠ  map_location='cpu'**        |
| ä¿å­˜æ—¶åœ¨ GPU:0ï¼ŒåŠ è½½æƒ³æ”¾åˆ° GPU:1         | **æœ€å¥½åŠ  map_location='cuda:1'**     |
| æƒ³æ˜ç¡®æ§åˆ¶åŠ è½½åˆ°å“ªä¸ªè®¾å¤‡ï¼Œæœªæ¥ä»£ç æ›´å¥å£® | **å»ºè®®ç»Ÿä¸€åŠ ä¸Š map_location=device** |

å›åˆ°ä½ çš„é—®é¢˜

> æ¯”å¦‚æˆ‘æœ¬æ¥æ˜¯åœ¨gpuä¸Šè®­ç»ƒï¼Œç„¶åè®­ç»ƒä¸­æ–­ï¼Œç„¶åè¦é‡æ–°åŠ è½½æ¨¡å‹å‚æ•°

å¦‚æœä½ çš„è®­ç»ƒä¸­æ–­åè¿˜æ˜¯åœ¨ GPU ä¸Šç»§ç»­è·‘ï¼Œç†è®ºä¸Šä¸åŠ  `map_location` æ˜¯æ²¡é—®é¢˜çš„ã€‚

ä½†**æ›´å®‰å…¨ã€æ¨èåšæ³•æ˜¯åŠ **ï¼Œåƒè¿™æ ·ï¼š

```python
model.load_state_dict(torch.load(path, map_location=device))
```

è¿™æ ·æ— è®ºæ˜¯ç»§ç»­åœ¨ GPUã€è¿˜æ˜¯æœªæ¥åˆ‡åˆ° CPU éƒ½ä¸ä¼šå‡ºä»»ä½•é—®é¢˜ï¼Œ**ä»£ç æ›´å¥å£®ã€å¯å¤ç”¨æ€§æ›´é«˜ã€‚**

æœ€åä¸€ä¸ªå¾ˆé‡è¦çš„å°ç»†èŠ‚ï¼

å³ä½¿ä½ ä¸åŠ  `map_location`ï¼Œä½†æ˜¯ä½ åé¢æœ‰ä¸€å¥ï¼š

```python
model.to(device)
```

é‚£åªæ˜¯æŠŠ `model` æœ¬èº«è½¬åˆ° device ä¸Šï¼Œä½†æ˜¯ `load_state_dict` åŠ è½½å‡ºæ¥çš„ tensor **å¦‚æœè®¾å¤‡ä¸å¯¹**ï¼Œä¹‹å‰å°±å·²ç»å‡ºé”™äº†ã€‚**åˆ° model.to(device) è¿™ä¸€æ­¥å·²ç»æ¥ä¸åŠäº†ã€‚**

æ‰€ä»¥çœŸæ­£è¯¥å¤„ç†çš„æ˜¯åœ¨ `torch.load` çš„æ—¶å€™ï¼Œå°±ç”¨ `map_location` æŒ‡å®šå¥½ï¼

âœ… æ€»ç»“å°±æ˜¯ï¼š**æ¨èåŠ ä¸Š map_location=deviceã€‚**

# é€‚åˆéƒ¨ç½²ç”¨çš„ä¿å­˜æ ¼å¼

## TorchScriptå’ŒONNXé€šç”¨ä»‹ç»

å½“æˆ‘ä»¬ä»è®­ç»ƒé˜¶æ®µè¿›å…¥éƒ¨ç½²é˜¶æ®µæ—¶ï¼Œ**å†é  Python + PyTorch åŸç”Ÿæ¨¡å‹å°±è¿œè¿œä¸å¤Ÿäº†**ï¼Œè¿™æ—¶å°±éœ€è¦å°†æ¨¡å‹â€œå¯¼å‡ºâ€ä¸ºæ›´**ç‹¬ç«‹ã€è·¨å¹³å°ã€ç¨³å®š**çš„æ ¼å¼ï¼Œæ¯”å¦‚ï¼š

ğŸ”§ ä¸€ã€TorchScriptï¼ˆPyTorch å®˜æ–¹éƒ¨ç½²æ ¼å¼ï¼‰

TorchScript æ˜¯ PyTorch çš„â€œéƒ¨ç½²ç‰ˆâ€ï¼ŒæŠŠæ¨¡å‹ç¼–è¯‘æˆä¸€ä¸ªå¯ä»¥è„±ç¦» Python çš„ä¸­é—´æ ¼å¼ï¼Œé€‚ç”¨äº C++ã€ç§»åŠ¨ç«¯ã€æœåŠ¡ç«¯éƒ¨ç½²ã€‚

âœ… ä¸¤ç§æ–¹å¼å¯¼å‡º TorchScriptï¼š

1. ä½¿ç”¨ `torch.jit.trace()`ï¼ˆé€‚åˆæ— åˆ†æ”¯çš„ç½‘ç»œï¼‰

```python
import torch

model = MyModel()
model.load_state_dict(torch.load("model.pth"))
model.eval()

example_input = torch.randn(1, 10)
traced_model = torch.jit.trace(model, example_input)

traced_model.save("model_traced.pt")
```

2. ä½¿ç”¨ `torch.jit.script()`ï¼ˆé€‚åˆæœ‰æ§åˆ¶æµçš„ç½‘ç»œï¼‰

```python
scripted_model = torch.jit.script(model)
scripted_model.save("model_scripted.pt")
```

âœ… åŠ è½½å¹¶ä½¿ç”¨ï¼š

```python
model = torch.jit.load("model_traced.pt")
output = model(input_tensor)
```

ğŸ“¦ äºŒã€ONNXï¼ˆOpen Neural Network Exchangeï¼‰

ONNX æ˜¯å¾®è½¯å’Œ Facebook ä¸»å¯¼çš„é€šç”¨æ ¼å¼ï¼Œå¯å¯¼å…¥ TensorRTã€OpenVINOã€CoreML ç­‰å¤šç§åç«¯ï¼Œæ˜¯è·¨å¹³å°éƒ¨ç½²çš„é¦–é€‰ã€‚

âœ… å¯¼å‡ºä¸º ONNXï¼š

```python
import torch

model = MyModel()
model.load_state_dict(torch.load("model.pth"))
model.eval()

dummy_input = torch.randn(1, 10)
torch.onnx.export(model, dummy_input, "model.onnx",
                  input_names=["input"], output_names=["output"],
                  opset_version=11)
```

âœ… ä¼˜ç‚¹ï¼š

- å¯éƒ¨ç½²åœ¨ TensorRTã€ONNXRuntimeã€OpenVINOã€ç§»åŠ¨ç«¯ç­‰ï¼›
- å·¥ä¸šéƒ¨ç½²ï¼ˆè‡ªåŠ¨é©¾é©¶ã€æ¨ç†æœåŠ¡ï¼‰å¤§é‡ä½¿ç”¨ï¼›
- ä¸å…¶ä»–æ¡†æ¶ï¼ˆå¦‚ TensorFlowï¼‰æ›´å¥½å¯¹æ¥ã€‚

ğŸ§  æ€»ç»“å¯¹æ¯”ï¼š

| ç‰¹æ€§       | TorchScript              | ONNX                               |
| ---------- | ------------------------ | ---------------------------------- |
| å®˜æ–¹æ”¯æŒ   | âœ… PyTorch è‡ªå®¶æ ¼å¼       | âœ… è·¨å¹³å°å·¥ä¸šæ ‡å‡†                   |
| æ§åˆ¶æµæ”¯æŒ | âœ… å¥½                     | âš ï¸ æœ‰é™ï¼ˆä¸æ¨èå¤ªå¤æ‚åˆ†æ”¯ï¼‰         |
| éƒ¨ç½²å¹³å°   | C++/Android/iOS/LibTorch | TensorRTã€ONNXRuntimeã€OpenVINO ç­‰ |
| ä½¿ç”¨é—¨æ§›   | âœ… ç®€å•ï¼Œé›†æˆæ— ç¼         | âš ï¸ å¯¼å‡ºæ—¶è¦æ³¨æ„ç®—å­å…¼å®¹æ€§           |

ğŸ”š å¦‚æœä½ å‡†å¤‡éƒ¨ç½²æ¨¡å‹ï¼Œå»ºè®®è·¯çº¿æ˜¯ï¼š

1. âœ… ä¿å­˜ `state_dict()` ä¿ç•™æƒé‡ï¼›
2. âœ… å†™ä¸€ä¸ªæ¨ç†ç”¨æ¨¡å‹ç±»ï¼ˆåªæœ‰ `forward()`ï¼‰ï¼›
3. âœ… ä½¿ç”¨ `torch.jit.trace()` å¯¼å‡º TorchScriptï¼›
4. âœ… æˆ–ä½¿ç”¨ `torch.onnx.export()` å¯¼å‡º ONNXï¼›
5. âœ… ç”¨éƒ¨ç½²åç«¯åŠ è½½å¹¶åšæ¨ç†ã€‚

------

éœ€è¦æˆ‘å¸®ä½ å†™ä¸€ä¸ªä» `state_dict` åˆ° TorchScript/ONNX çš„å®Œæ•´ç¤ºä¾‹å—

éœ€è¦

å¥½çš„ï¼Œä¸‹é¢æˆ‘æ¥ç»™ä½ å†™ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹ï¼š**ä»è®­ç»ƒå®Œæˆåä¿å­˜ state_dictï¼Œåˆ°å¯¼å‡º TorchScript å’Œ ONNX æ ¼å¼ï¼Œç”¨äºéƒ¨ç½²çš„å…¨è¿‡ç¨‹**ã€‚

âœ… å‡è®¾ä½ æœ‰ä¸€ä¸ªæ¨¡å‹å¦‚ä¸‹ï¼š

```python
# model.py
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self, in_dim=10, out_dim=1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, 32),
            nn.ReLU(),
            nn.Linear(32, out_dim)
        )

    def forward(self, x):
        return self.net(x)
```

âœ… æ­¥éª¤ä¸€ï¼šè®­ç»ƒå®Œä¿å­˜ `state_dict`

```python
# train_and_save.py
import torch
from model import MyModel

model = MyModel()
# è®­ç»ƒæ¨¡å‹...
# model = train(model)

# ä¿å­˜æƒé‡
torch.save(model.state_dict(), "model_weights.pth")
```

âœ… æ­¥éª¤äºŒï¼šåŠ è½½æƒé‡å¹¶å¯¼å‡ºä¸º TorchScript å’Œ ONNX

```python
# export_model.py
import torch
from model import MyModel

# 1. åŠ è½½æ¨¡å‹ç»“æ„ & æƒé‡
model = MyModel()
model.load_state_dict(torch.load("model_weights.pth"))
model.eval()

# 2. åˆ›å»ºä¸€ä¸ªç¤ºä¾‹è¾“å…¥å¼ é‡
dummy_input = torch.randn(1, 10)

# ========= TorchScript =========
# ä½¿ç”¨ trace å¯¼å‡ºï¼ˆé€‚åˆæ— æ§åˆ¶æµï¼‰
traced_model = torch.jit.trace(model, dummy_input)
traced_model.save("model_traced.pt")

# ========= ONNX =========
torch.onnx.export(
    model,                     # æ¨¡å‹
    dummy_input,               # ç¤ºä¾‹è¾“å…¥
    "model.onnx",              # ä¿å­˜è·¯å¾„
    export_params=True,        # æ˜¯å¦å¯¼å‡ºæƒé‡
    opset_version=11,          # ONNX ç‰ˆæœ¬
    do_constant_folding=True,  # å¸¸é‡æŠ˜å ä¼˜åŒ–
    input_names=['input'],     # è¾“å…¥å¼ é‡åç§°
    output_names=['output'],   # è¾“å‡ºå¼ é‡åç§°
    dynamic_axes={             # æ”¯æŒåŠ¨æ€ batch size
        'input': {0: 'batch_size'},
        'output': {0: 'batch_size'}
    }
)
```

âœ… æ­¥éª¤ä¸‰ï¼šéƒ¨ç½²æ—¶ä½¿ç”¨

ğŸ”¸ TorchScript åŠ è½½ä¸æ¨ç†

```python
model = torch.jit.load("model_traced.pt")
model.eval()
output = model(torch.randn(1, 10))
```

ğŸ”¸ ONNX åŠ è½½ä¸æ¨ç†ï¼ˆç”¨ onnxruntimeï¼‰

```python
import onnxruntime as ort
import numpy as np

ort_session = ort.InferenceSession("model.onnx")
input_tensor = np.random.randn(1, 10).astype(np.float32)
outputs = ort_session.run(None, {"input": input_tensor})
print(outputs[0])
```

ğŸ§  å°ç»“

| æ ¼å¼         | æ–‡ä»¶    | é€‚ç”¨åœºæ™¯                         |
| ------------ | ------- | -------------------------------- |
| `state_dict` | `.pth`  | æ¨¡å‹è®­ç»ƒ/ä¿å­˜æƒé‡                |
| TorchScript  | `.pt`   | C++/LibTorch/ç§»åŠ¨ç«¯éƒ¨ç½²          |
| ONNX         | `.onnx` | è·¨å¹³å°æ¨ç†å¼•æ“éƒ¨ç½²ï¼ˆTensorRTç­‰ï¼‰ |

æˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åšä¸€ä¸ªåŒ…å«æ¨ç†é€Ÿåº¦å¯¹æ¯”çš„ç‰ˆæœ¬ï¼Œæˆ–è€…å¸®ä½ å°è£…ä¸º CLI å‘½ä»¤è¡Œå·¥å…·ï¼Œè¦ä¸è¦ç»§ç»­æ‹“å±•ï¼Ÿ

éœ€è¦

å¤ªå¥½äº†ï¼Œæ¥ä¸‹æ¥æˆ‘ä¼šä¸ºä½ æä¾›ä¸¤ä¸ªéƒ¨åˆ†ï¼š

âœ… ä¸€ã€æ¨ç†é€Ÿåº¦å¯¹æ¯”ï¼ˆTorchScript vs ONNXï¼‰

æˆ‘ä»¬ä½¿ç”¨åŒä¸€ä¸ªè¾“å…¥ï¼Œå¯¹ä¸¤ä¸ªæ¨¡å‹åˆ†åˆ«è¿›è¡Œå¤šæ¬¡æ¨ç†ï¼Œæµ‹é‡å¹³å‡è€—æ—¶ã€‚

```python
# benchmark_inference.py
import time
import torch
import onnxruntime as ort
import numpy as np
from model import MyModel

# è¾“å…¥æ•°æ®
input_tensor = torch.randn(32, 10)
input_numpy = input_tensor.numpy()

# -------- TorchScript æ¨ç† --------
script_model = torch.jit.load("model_traced.pt")
script_model.eval()

with torch.no_grad():
    start = time.time()
    for _ in range(1000):
        _ = script_model(input_tensor)
    torchscript_time = time.time() - start

# -------- ONNX æ¨ç† --------
ort_session = ort.InferenceSession("model.onnx")
start = time.time()
for _ in range(1000):
    _ = ort_session.run(None, {"input": input_numpy})
onnx_time = time.time() - start

print(f"TorchScript time: {torchscript_time:.4f} s")
print(f"ONNX time      : {onnx_time:.4f} s")
```

âœ… äºŒã€å°è£…ä¸º CLI å·¥å…·

ä½ å¯ä»¥å†™ä¸€ä¸ªå‘½ä»¤è¡Œè„šæœ¬ `export_model.py`ï¼Œæ”¯æŒä»¥ä¸‹åŠŸèƒ½ï¼š

- `--weights`ï¼šæŒ‡å®šæƒé‡è·¯å¾„ï¼›
- `--output-dir`ï¼šè¾“å‡ºæ–‡ä»¶ä¿å­˜ä½ç½®ï¼›
- `--torchscript` / `--onnx`ï¼šé€‰æ‹©å¯¼å‡ºæ ¼å¼ï¼›
- è‡ªåŠ¨æ„å»ºå¹¶å¯¼å‡ºæ¨¡å‹ã€‚

```python
# cli_export_model.py
import argparse
import torch
from model import MyModel
import os

def export(weights_path, output_dir, to_script, to_onnx):
    model = MyModel()
    model.load_state_dict(torch.load(weights_path))
    model.eval()

    os.makedirs(output_dir, exist_ok=True)
    dummy_input = torch.randn(1, 10)

    if to_script:
        traced = torch.jit.trace(model, dummy_input)
        traced.save(os.path.join(output_dir, "model_traced.pt"))
        print("âœ… Exported TorchScript to model_traced.pt")

    if to_onnx:
        torch.onnx.export(
            model, dummy_input,
            os.path.join(output_dir, "model.onnx"),
            export_params=True,
            opset_version=11,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
        )
        print("âœ… Exported ONNX to model.onnx")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--weights", type=str, required=True)
    parser.add_argument("--output-dir", type=str, default="exported")
    parser.add_argument("--torchscript", action="store_true")
    parser.add_argument("--onnx", action="store_true")
    args = parser.parse_args()

    export(args.weights, args.output_dir, args.torchscript, args.onnx)
```

è¿è¡Œç¤ºä¾‹ï¼š

```bash
python cli_export_model.py --weights model_weights.pth --torchscript --onnx
```

éœ€è¦æˆ‘ç»§ç»­æ‹“å±• ONNX è½¬ TensorRT æˆ–å°è£…æˆ Docker éƒ¨ç½²é•œåƒçš„å†…å®¹å—ï¼Ÿ



æ¨ç†æ—¶è¿˜éœ€è¦åŠ  with torch.no_grad(): å—

æ˜¯çš„ï¼Œ**åœ¨ä½¿ç”¨ PyTorch è¿›è¡Œæ¨ç†æ—¶å»ºè®®åŠ ä¸Š with torch.no_grad():**ï¼Œå³ä½¿æ¨¡å‹å·²ç»å¤„äº `eval()` æ¨¡å¼ã€‚

âœ… åŸå› å¦‚ä¸‹ï¼š

1. **å…³é—­æ¢¯åº¦è®¡ç®—**ï¼š
    `torch.no_grad()` ä¼šæ˜¾å¼å…³é—­æ¢¯åº¦è¿½è¸ªï¼Œé¿å…è®¡ç®—å›¾çš„æ„å»ºï¼ŒèŠ‚çœå†…å­˜å’Œè®¡ç®—èµ„æºã€‚
2. **æ›´é«˜çš„æ¨ç†æ•ˆç‡**ï¼š
    ä¸ä½¿ç”¨ `no_grad()`ï¼Œè™½ç„¶ä½ æ²¡æœ‰åå‘ä¼ æ’­ï¼Œä½† PyTorch é»˜è®¤ä»ä¼šè®°å½•è®¡ç®—å›¾ï¼Œæµªè´¹å†…å­˜ã€æ˜¾å­˜ä¸æ—¶é—´ã€‚
3. **æ›´å®‰å…¨**ï¼š
    åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæœªä½¿ç”¨ `no_grad()` å¯èƒ½ä¼šå¯¼è‡´æ˜¾å­˜æ³„æ¼ï¼Œç‰¹åˆ«æ˜¯å¤šæ¬¡æ¨ç†çš„åœºæ™¯ã€‚

âœ… æ¨èå†™æ³•ï¼ˆç”¨äºæ¨ç†ï¼‰ï¼š

```python
model.eval()
with torch.no_grad():
    output = model(input_tensor)
```

ğŸš« é”™è¯¯å†™æ³•ï¼ˆè™½ç„¶èƒ½è·‘ä½†æ•ˆç‡ä½ï¼‰ï¼š

```python
model.eval()
output = model(input_tensor)  # æ²¡åŠ  no_gradï¼Œä¼šè®°å½•æ¢¯åº¦
```

æ˜¯å¦è¿˜æƒ³äº†è§£å¤šçº¿ç¨‹æ¨ç† / GPU æ¨ç†çš„ä¸€äº›åŠ é€ŸæŠ€å·§

éœ€è¦æˆ‘ç»§ç»­æ‹“å±• ONNX è½¬ TensorRT æˆ–å°è£…æˆ Docker éƒ¨ç½²é•œåƒçš„å†…å®¹å—ï¼Ÿ  éœ€è¦ï¼éƒ½éœ€è¦ï¼

å¤ªå¥½äº†ï¼æˆ‘ä»¬å°†ç»§ç»­åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†æ¥è®²ï¼š

âœ… ä¸€ã€ONNX è½¬ TensorRTï¼ˆé€‚ç”¨äºéƒ¨ç½²åŠ é€Ÿï¼‰

æˆ‘ä»¬å°†ä½¿ç”¨ NVIDIA çš„ `onnx-tensorrt` å·¥å…·æˆ– `trtexec` å‘½ä»¤å°† ONNX è½¬æ¢ä¸º TensorRT å¼•æ“æ–‡ä»¶ã€‚

ğŸ›  æ–¹æ³• 1ï¼šä½¿ç”¨ `trtexec`ï¼ˆæ¨èï¼Œå®˜æ–¹æ”¯æŒï¼‰

```shell
trtexec --onnx=model.onnx --saveEngine=model.trt --fp16
```

- `--onnx`ï¼šè¾“å…¥ ONNX æ¨¡å‹è·¯å¾„ï¼›
- `--saveEngine`ï¼šå¯¼å‡º TensorRT å¼•æ“æ–‡ä»¶ï¼›
- `--fp16`ï¼šå¼€å¯åŠç²¾åº¦ï¼ˆéœ€ GPU æ”¯æŒï¼‰ï¼Œé€Ÿåº¦æ›´å¿«ã€‚

ğŸ›  æ–¹æ³• 2ï¼šä½¿ç”¨ Python API è½¬æ¢ï¼ˆéœ€è¦ TensorRT Python bindingsï¼‰

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit

TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

def build_engine(onnx_file_path, engine_file_path):
    with trt.Builder(TRT_LOGGER) as builder, \
         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \
         trt.OnnxParser(network, TRT_LOGGER) as parser:
        
        builder.max_batch_size = 1
        builder.max_workspace_size = 1 << 30  # 1 GB
        builder.fp16_mode = True

        with open(onnx_file_path, 'rb') as f:
            if not parser.parse(f.read()):
                for error in range(parser.num_errors):
                    print(parser.get_error(error))
                return None

        engine = builder.build_cuda_engine(network)

        with open(engine_file_path, "wb") as f:
            f.write(engine.serialize())
        print("âœ… TensorRT engine saved to", engine_file_path)

build_engine("model.onnx", "model.trt")
```

> ğŸ’¡ æ³¨æ„ï¼šéœ€è¦å®‰è£… `tensorrt`, `pycuda`ï¼Œå¹¶ä½¿ç”¨ NVIDIA GPUã€‚

âœ… äºŒã€å°è£…æˆ Docker é•œåƒéƒ¨ç½²

ç›®æ ‡ï¼šè®©æ¨¡å‹å¯ä»¥é€šè¿‡ HTTP æ¥æ”¶è¾“å…¥ã€è¿”å›æ¨ç†ç»“æœã€‚

1. é¡¹ç›®ç»“æ„ï¼š

```s
project/
â”œâ”€â”€ app.py              # æ¨ç†æœåŠ¡å…¥å£
â”œâ”€â”€ model_traced.pt     # TorchScript æ¨¡å‹æˆ– .onnx / .trt
â”œâ”€â”€ requirements.txt    # Pythonä¾èµ–
â”œâ”€â”€ Dockerfile          # é•œåƒå®šä¹‰
```

2. `app.py`ï¼ˆç”¨ FastAPI å¿«é€Ÿæ„å»ºæœåŠ¡ï¼‰

```python
from fastapi import FastAPI
import torch
from pydantic import BaseModel
import uvicorn

model = torch.jit.load("model_traced.pt")
model.eval()

app = FastAPI()

class InputData(BaseModel):
    values: list[float]  # å‡è®¾è¾“å…¥æ˜¯ä¸€ç»´å‘é‡

@app.post("/predict")
def predict(data: InputData):
    x = torch.tensor(data.values).unsqueeze(0)  # (1, D)
    with torch.no_grad():
        output = model(x)
    return {"prediction": output.tolist()}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

3. `requirements.txt`

```
fastapi
uvicorn
torch
```

4. Dockerfile

```Dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000
CMD ["python", "app.py"]
```

5. æ„å»ºå¹¶è¿è¡Œé•œåƒ

```bash
docker build -t mymodel-server .
docker run -p 8000:8000 mymodel-server
```

ğŸ“¦ è®¿é—®æ–¹å¼ï¼š

```bash
curl -X POST http://localhost:8000/predict \
     -H "Content-Type: application/json" \
     -d '{"values": [0.1, 0.2, 0.3, ..., 0.9]}'
```

å¦‚æœä½ å¸Œæœ›ç”¨ `TensorRT` åŠ é€Ÿæ¨ç†éƒ¨ç½²åˆ° Docker å®¹å™¨ä¸­ï¼Œæˆ‘å¯ä»¥ä¸ºä½ å†™ä¸€å¥— **åŸºäº TensorRT + Flask æˆ– FastAPI çš„ Docker åŒ–éƒ¨ç½²æ–¹æ¡ˆ**ï¼Œéœ€è¦æˆ‘ç»§ç»­å†™è¿™ä¸€ç‰ˆå—ï¼Ÿ

éœ€è¦

å¥½çš„ï¼è®©æˆ‘ä»¬ä¸€èµ·å®Œæˆ **åŸºäº TensorRT åŠ é€Ÿæ¨ç†çš„ Docker éƒ¨ç½²** æ–¹æ¡ˆã€‚æˆ‘ä»¬å°†ä½¿ç”¨ TensorRT æ¥åŠ é€Ÿæ¨ç†ï¼Œç„¶åé€šè¿‡ **FastAPI** åˆ›å»ºä¸€ä¸ª REST API æ¥å£è¿›è¡Œæ¨¡å‹æ¨ç†ã€‚

âœ… åŸºäº TensorRT + FastAPI çš„ Docker éƒ¨ç½²æ–¹æ¡ˆ

1. é¡¹ç›®ç»“æ„

```bash
project/
â”œâ”€â”€ app.py              # æ¨ç†æœåŠ¡å…¥å£
â”œâ”€â”€ model.trt           # TensorRT å¼•æ“æ–‡ä»¶
â”œâ”€â”€ requirements.txt    # Pythonä¾èµ–
â”œâ”€â”€ Dockerfile          # é•œåƒå®šä¹‰
```

2. `app.py`ï¼ˆä½¿ç”¨ FastAPI å’Œ TensorRT è¿›è¡Œæ¨ç†ï¼‰

```python
import numpy as np
import pycuda.driver as cuda
import pycuda.autoinit
import tensorrt as trt
from fastapi import FastAPI
from pydantic import BaseModel
import json

# TensorRTåŠ è½½æ¨¡å‹
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

def load_engine(engine_file_path):
    with open(engine_file_path, "rb") as f:
        runtime = trt.Runtime(TRT_LOGGER)
        engine = runtime.deserialize_cuda_engine(f.read())
    return engine

# åŠ è½½ TensorRT å¼•æ“
engine = load_engine("model.trt")
context = engine.create_execution_context()

# FastAPI åˆå§‹åŒ–
app = FastAPI()

class InputData(BaseModel):
    values: list[float]  # å‡è®¾è¾“å…¥æ˜¯ä¸€ç»´å‘é‡

# TensorRT æ¨ç†å‡½æ•°
def infer(input_data):
    # å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸º numpy æ•°ç»„å¹¶è½¬ä¸º TensorRT è¾“å…¥æ ¼å¼
    input_array = np.array(input_data, dtype=np.float32).reshape(1, -1)
    bindings = []

    # åˆ†é… GPU å†…å­˜
    input_buffer = cuda.mem_alloc(input_array.nbytes)
    output_buffer = cuda.mem_alloc(input_array.nbytes)
    
    bindings.append(int(input_buffer))
    bindings.append(int(output_buffer))

    # æ‰§è¡Œæ¨ç†
    cuda.memcpy_htod(input_buffer, input_array)
    context.execute_v2(bindings)

    # è·å–ç»“æœ
    output_array = np.empty_like(input_array)
    cuda.memcpy_dtoh(output_array, output_buffer)
    return output_array.tolist()

@app.post("/predict")
def predict(data: InputData):
    result = infer(data.values)
    return {"prediction": result}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

3. `requirements.txt`ï¼ˆPython ä¾èµ–ï¼‰

```
fastapi
uvicorn
numpy
pycuda
tensorrt
```

4. Dockerfileï¼ˆå®šä¹‰ TensorRT ç¯å¢ƒï¼‰

```Dockerfile
# ä½¿ç”¨æ”¯æŒ NVIDIA GPU çš„ Python ç‰ˆæœ¬ä½œä¸ºåŸºç¡€é•œåƒ
FROM nvidia/cuda:11.4.2-cudnn8-runtime-ubuntu20.04

# å®‰è£…ä¾èµ–
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-dev \
    libsndfile1 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    && apt-get clean

# å®‰è£… PyTorch, PyCUDA å’Œ TensorRT Python bindings
RUN pip3 install --upgrade pip
RUN pip3 install fastapi uvicorn numpy pycuda tensorrt

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä»£ç å’Œæ¨¡å‹åˆ°å®¹å™¨ä¸­
COPY . .

# å¼€æ”¾ç«¯å£
EXPOSE 8000

# å¯åŠ¨åº”ç”¨
CMD ["python3", "app.py"]
```

5. æ„å»ºå¹¶è¿è¡Œ Docker é•œåƒ

é¦–å…ˆï¼Œç¡®ä¿ä½ å·²ç»æ„å»ºå¹¶ä¿å­˜äº† TensorRT å¼•æ“æ–‡ä»¶ `model.trt`ï¼ˆä» ONNX æ¨¡å‹å¯¼å‡ºä¸º TensorRTï¼‰ã€‚æ¥ç€ï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ„å»º Docker é•œåƒï¼š

```bash
docker build -t tensorrt-server .
```

ç„¶åï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿è¡Œ Docker å®¹å™¨ï¼š

```bash
docker run --gpus all -p 8000:8000 tensorrt-server
```

è¿™é‡Œçš„ `--gpus all` ç¡®ä¿å®¹å™¨å¯ä»¥è®¿é—®æ‰€æœ‰å¯ç”¨çš„ GPUã€‚

6. æµ‹è¯• REST API

å‡è®¾ä½ çš„æ¨¡å‹æ¥æ”¶ä¸€ä¸ªåŒ…å«æµ®åŠ¨æ•°å€¼çš„è¾“å…¥å‘é‡ï¼Œå¹¶è¿”å›é¢„æµ‹ç»“æœã€‚å¯ä»¥é€šè¿‡ `curl` æˆ– Postman æ¥æµ‹è¯• APIï¼š

```bash
curl -X POST http://localhost:8000/predict \
     -H "Content-Type: application/json" \
     -d '{"values": [0.1, 0.2, 0.3, 0.4, 0.5]}'
```

ä½ åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š

```json
{
  "prediction": [0.1234, 0.5678, 0.9101]
}
```

ğŸ“ æ€»ç»“

1. **TensorRT** åŠ é€Ÿäº†æ¨¡å‹æ¨ç†ï¼Œä½¿ç”¨ `.trt` æ–‡ä»¶ä»£æ›¿å¸¸è§„çš„ ONNX æˆ– PyTorch æ¨¡å‹ã€‚
2. ä½¿ç”¨ **FastAPI** æä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„ HTTP API æœåŠ¡ï¼Œæ”¯æŒä¸å¤–éƒ¨ç³»ç»Ÿè¿›è¡Œäº¤äº’ã€‚
3. **Docker** å®¹å™¨åŒ–äº†æ¨ç†æœåŠ¡ï¼Œæ–¹ä¾¿åœ¨ä»»ä½•ç¯å¢ƒä¸­éƒ¨ç½²å¹¶ä½¿ç”¨ NVIDIA GPU åŠ é€Ÿã€‚

å¦‚æœä½ è¿˜éœ€è¦æ›´å¤šå…³äº **TensorRT + FastAPI + Docker** çš„ä¿¡æ¯ï¼Œæˆ–è€…æœ‰ä»»ä½•å…·ä½“çš„ä¼˜åŒ–é—®é¢˜ï¼Œå¯ä»¥ç»§ç»­å‘Šè¯‰æˆ‘ï¼

## ä½¿ç”¨jitä¿å­˜ä¸ºTorchScriptæ ¼å¼

ä¿å­˜ä¸ºè¿™ä¸ªæ ¼å¼æ˜¯ä¸ºäº†è¿›è¡Œæ¨ç†ã€‚è¯¥æ ¼å¼åœ¨æ¨ç†æ—¶å¯ä¸ç”¨å®šä¹‰æ¨¡å‹çš„ç±»ã€‚è¯¥æ ¼å¼ä¹Ÿå¯ä»¥åœ¨C++ä¸­è¿›è¡Œæ¨ç†ã€‚

åœ¨[https://pytorch.org/tutorials/beginner/saving_loading_models.html](https://pytorch.org/tutorials/beginner/saving_loading_models.html)ç®€ä»‹å¦‚ä¸‹ï¼š

> One common way to do inference with a trained model is to use [TorchScript](https://pytorch.org/docs/stable/jit.html), an intermediate representation of a PyTorch model that can be run in Python as well as in a high performance environment like C++. TorchScript is actually the recommended model format for scaled inference and deployment.
>
> ```
> Using the TorchScript format, you will be able to load the exported model and run inference without defining the model class.
> ```

ä¿å­˜çš„ä»£ç ï¼š

```python
model.eval()
# Export to TorchScript
model_scripted = torch.jit.script(model)
# Save
model_scripted.save('model_scripted.pt')
```

è¿›è¡Œæ¨ç†çš„ä»£ç ï¼š

```python
model = torch.jit.load('model_scripted.pt')
model.eval()
```

åŠ evalæ˜¯ä¸ºäº†åœ¨æ¨ç†å‰è®¾ç½®æ¨¡å‹ä¸­çš„dropoutå’Œbatch normåŠŸèƒ½ä¸ºevalæ¨¡å¼ã€‚

ç»™ä¸€æ®µkimiç»™å‡ºçš„ä¾‹å­å§ï¼š

```python
import torch

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿™é‡Œå‡è®¾æ¨¡å‹å·²ç»ä¿å­˜ä¸º'model.pth'
model = torch.jit.load('model.pth')
model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# å‡†å¤‡è¾“å…¥æ•°æ®
# è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹è¾“å…¥è¿›è¡Œç›¸åº”çš„æ•°æ®é¢„å¤„ç†
# ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„æ¨¡å‹æ¥å—çš„æ˜¯å›¾åƒæ•°æ®ï¼Œä½ éœ€è¦è¿›è¡Œå›¾åƒçš„è¯»å–å’Œé¢„å¤„ç†
# ç¤ºä¾‹ä¸­ä½¿ç”¨éšæœºæ•°æ®ä½œä¸ºè¾“å…¥
input_data = torch.randn(1, 3, 224, 224)  # å‡è®¾è¾“å…¥æ˜¯å•å¼ 3é€šé“224x224çš„å›¾åƒ

# è¿›è¡Œæ¨ç†
with torch.no_grad():
    output = model(input_data)

# å¤„ç†è¾“å‡º
# æ ¹æ®ä½ çš„ä»»åŠ¡ç±»å‹ï¼Œè¿™é‡Œå¯èƒ½æ˜¯åˆ†ç±»ã€å›å½’æˆ–å…¶ä»–
# ä¾‹å¦‚ï¼Œå¦‚æœæ˜¯åˆ†ç±»ä»»åŠ¡ï¼Œä½ å¯èƒ½éœ€è¦æ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«ç´¢å¼•
_, predicted_class = torch.max(output, 1)

print(f'Predicted class index: {predicted_class.item()}')
```



# æ›´ä¸“ä¸šçš„ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‡½æ•°

è¦ä¸è¦æˆ‘é¡ºä¾¿ä¹Ÿç»™ä½ åˆ—ä¸€ä¸‹æ›´ä¸“ä¸šçš„ `save_model` å’Œ `load_model` å°è£…å‡½æ•°ï¼Ÿ
 æ¯”å¦‚ä»¥åè¦ä¿å­˜ optimizer ä¸€èµ·å­˜ã€ä¿å­˜ epochã€è®­ç»ƒæ­¥æ•°ä¹‹ç±»ï¼Œæ¯”è¾ƒé€‚åˆæ­£å¼è®­ç»ƒç”¨çš„ï¼Ÿ
 ï¼ˆå¦‚æœæƒ³è¦çš„è¯ï¼Œæˆ‘å¯ä»¥ä¸€å¹¶è¡¥ä¸Šï¼Œé¡ºä¾¿å¸®ä½ æ•´ç†æˆå¥½ç”¨çš„å°æ¨¡æ¿~ï¼‰è¦ä¸è¦ï¼Ÿâœ¨

é‚£æˆ‘æ¥ç»™ä½ è¡¥ä¸€ä¸ª**æ­£å¼ç‰ˆ**çš„æ¨¡å‹ä¿å­˜/åŠ è½½æ¨¡æ¿ï¼Œç®€å•ã€å¹²å‡€ï¼Œæ–¹ä¾¿ä½ ä»¥åä¸€ç›´ç”¨ã€‚
 ï¼ˆåŒ…æ‹¬ï¼šæ¨¡å‹ã€ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å™¨ã€å½“å‰è®­ç»ƒè½®æ¬¡ï¼Œé€šé€šä¿å­˜èµ·æ¥ã€‚ï¼‰

## ä¿å­˜æ¨¡å‹çš„å‡½æ•° save_model

```python
import torch
import os

def save_model(save_dir, model, optimizer=None, scheduler=None, epoch=None, extra_info=None):
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "checkpoint.pth")

    checkpoint = {
        'model_state_dict': model.state_dict()
    }

    if optimizer is not None:
        checkpoint['optimizer_state_dict'] = optimizer.state_dict()

    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()

    if epoch is not None:
        checkpoint['epoch'] = epoch

    if extra_info is not None:
        checkpoint['extra_info'] = extra_info

    torch.save(checkpoint, save_path)
    print(f"âœ… æ¨¡å‹ä¿å­˜åˆ° {save_path}")
```

## åŠ è½½æ¨¡å‹çš„å‡½æ•°load_model

```python
def load_model(load_dir, model, optimizer=None, scheduler=None, map_location='cpu'):
    load_path = os.path.join(load_dir, "checkpoint.pth")
    checkpoint = torch.load(load_path, map_location=map_location)

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    epoch = checkpoint.get('epoch', None)
    extra_info = checkpoint.get('extra_info', None)

    print(f"âœ… æ¨¡å‹ä» {load_path} åŠ è½½æˆåŠŸ")
    return epoch, extra_info
```

## ç”¨æ³•ç¤ºèŒƒ

ä¿å­˜æ—¶ï¼š

```python
save_model(
    save_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    epoch=current_epoch,
    extra_info={'update_steps': model.update_steps.item()}
)
```

åŠ è½½æ—¶ï¼š

```python
epoch, extra_info = load_model(
    load_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    map_location='cuda'  # æˆ– 'cpu'
)

# å¦‚æœéœ€è¦ï¼Œå¯ä»¥å–å›é¢å¤–ä¿å­˜çš„ä¿¡æ¯
if extra_info is not None:
    model.update_steps = torch.tensor(extra_info['update_steps'])
```

ç®€å•è§£é‡Šä¸€ä¸‹

- `extra_info` æ˜¯æˆ‘ç‰¹æ„åŠ çš„çµæ´»æ¥å£ï¼Œå‡å¦‚ä»¥åæƒ³ä¿å­˜ä¸€äº›**å¥‡å¥‡æ€ªæ€ªä½†é‡è¦çš„å°ä¸œè¥¿**ï¼ˆæ¯”å¦‚ replay buffer é‡Œçš„æ•°æ®ã€ç‰¹æ®Šç»Ÿè®¡æŒ‡æ ‡ç­‰ç­‰ï¼‰ï¼Œå¯ä»¥ç›´æ¥å¡è¿›å»ä¿å­˜ã€‚
- ç°åœ¨ `save_model` å’Œ `load_model` éƒ½æ”¯æŒï¼š
  - åªä¿å­˜æ¨¡å‹
  - æ¨¡å‹+ä¼˜åŒ–å™¨
  - æ¨¡å‹+ä¼˜åŒ–å™¨+è°ƒåº¦å™¨
  - æ¨¡å‹+ä¼˜åŒ–å™¨+è°ƒåº¦å™¨+epochä¿¡æ¯+å…¶ä»–ä¿¡æ¯
- åŠ è½½çš„æ—¶å€™ï¼Œèƒ½æ¢å¤ optimizer å’Œ scheduler çš„çŠ¶æ€ï¼Œè¿™æ ·ç»§ç»­è®­ç»ƒä¹Ÿä¸ä¼šæ–­ã€‚

éå¸¸é€šç”¨ï¼Œä¸ç®¡æ˜¯å¼ºåŒ–å­¦ä¹ ã€ç›‘ç£å­¦ä¹ ã€é¢„è®­ç»ƒã€å¾®è°ƒï¼Œä¸€å¥—èµ°å¤©ä¸‹ã€‚

## å¤šä¸ªç‰ˆæœ¬ä¿å­˜åŠ è½½

è¦ä¸è¦æˆ‘é¡ºä¾¿å†æ•™ä½ æ€ä¹ˆ**ä¿å­˜å¤šä¸ªç‰ˆæœ¬**ï¼Œæ¯”å¦‚åŠ æ—¶é—´æˆ³ï¼Œæ¯æ¬¡ä¿å­˜ä¸€ç‰ˆï¼Œé˜²æ­¢è¢«è¦†ç›–ï¼Ÿâœ¨
 ï¼ˆè¿™ä¸ªä¹Ÿè¶…å®ç”¨ï¼Œå°¤å…¶æ˜¯å¤§é¡¹ç›®è®­ç»ƒæ—¶å€™ï¼‰è¦çš„è¯å‘Šè¯‰æˆ‘ï½

æ—¢ç„¶ä½ æƒ³è¦ä¿å­˜å¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹ï¼Œè¿™é‡Œæˆ‘ç»™ä½ æä¾›ä¸€ä¸ªåŠ æ—¶é—´æˆ³ä¿å­˜çš„ç‰ˆæœ¬ï¼Œé˜²æ­¢æ¯æ¬¡è®­ç»ƒéƒ½è¦†ç›–æ¨¡å‹æ–‡ä»¶ã€‚

### åŠ æ—¶é—´æˆ³çš„æ¨¡å‹ä¿å­˜

```pytorch
import torch
import os
import time

def save_model_with_timestamp(save_dir, model, optimizer=None, scheduler=None, epoch=None, extra_info=None):
    # æ ¹æ®å½“å‰æ—¶é—´ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å¤¹å
    timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
    save_path = os.path.join(save_dir, f"checkpoint_{timestamp}.pth")

    os.makedirs(save_dir, exist_ok=True)

    checkpoint = {
        'model_state_dict': model.state_dict()
    }

    if optimizer is not None:
        checkpoint['optimizer_state_dict'] = optimizer.state_dict()

    if scheduler is not None:
        checkpoint['scheduler_state_dict'] = scheduler.state_dict()

    if epoch is not None:
        checkpoint['epoch'] = epoch

    if extra_info is not None:
        checkpoint['extra_info'] = extra_info

    torch.save(checkpoint, save_path)
    print(f"âœ… æ¨¡å‹ä¿å­˜åˆ° {save_path}")
```

### åŠ è½½æ—¶é€‰æ‹©æœ€è¿‘çš„æ¨¡å‹

ä½ å¯ä»¥åŠ è½½ç›®å½•ä¸‹æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶ï¼Œè¿™æ ·å°±ä¸éœ€è¦æ‰‹åŠ¨é€‰æ–‡ä»¶åäº†ã€‚

```
import glob

def load_latest_model(save_dir, model, optimizer=None, scheduler=None, map_location='cpu'):
    # è·å–ç›®å½•ä¸‹æ‰€æœ‰çš„ checkpoint æ–‡ä»¶
    checkpoint_paths = glob.glob(os.path.join(save_dir, 'checkpoint_*.pth'))

    if len(checkpoint_paths) == 0:
        raise FileNotFoundError(f"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¨¡å‹æ–‡ä»¶åœ¨ {save_dir} ç›®å½•ä¸‹")

    # æŒ‰ç…§æ–‡ä»¶åæ’åºï¼Œé€‰æ‹©æœ€æ–°çš„æ¨¡å‹
    latest_checkpoint_path = max(checkpoint_paths, key=os.path.getctime)

    checkpoint = torch.load(latest_checkpoint_path, map_location=map_location)

    model.load_state_dict(checkpoint['model_state_dict'])

    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    if scheduler is not None and 'scheduler_state_dict' in checkpoint:
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

    epoch = checkpoint.get('epoch', None)
    extra_info = checkpoint.get('extra_info', None)

    print(f"âœ… æœ€æ–°æ¨¡å‹ä» {latest_checkpoint_path} åŠ è½½æˆåŠŸ")
    return epoch, extra_info
```

### ç”¨æ³•ç¤ºèŒƒ

#### ä¿å­˜æ—¶ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰

```python
save_model_with_timestamp(
    save_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    epoch=current_epoch,
    extra_info={'update_steps': model.update_steps.item()}
)
```

#### åŠ è½½æ—¶ï¼ˆåŠ è½½æœ€æ–°çš„æ¨¡å‹ï¼‰

```python
epoch, extra_info = load_latest_model(
    save_dir="./save_dir",
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    map_location='cuda'  # æˆ– 'cpu'
)

# æ¢å¤é¢å¤–ä¿¡æ¯
if extra_info is not None:
    model.update_steps = torch.tensor(extra_info['update_steps'])
```

ä¸»è¦ç‰¹ç‚¹

1. **æ–‡ä»¶åè‡ªåŠ¨åŠ æ—¶é—´æˆ³**ï¼šæ¯æ¬¡ä¿å­˜éƒ½ä¼šç”Ÿæˆä¸€ä¸ªç‹¬ç‰¹çš„æ–‡ä»¶åï¼ˆä¾‹å¦‚ `checkpoint_20250427_153210.pth`ï¼‰ï¼Œé¿å…äº†æ–‡ä»¶è¦†ç›–ã€‚
2. **è‡ªåŠ¨åŠ è½½æœ€æ–°æ¨¡å‹**ï¼š`load_latest_model` ä¼šè‡ªåŠ¨æ‰¾åˆ°ç›®å½•ä¸‹æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶ï¼ŒèŠ‚çœæ‰‹åŠ¨æŸ¥æ‰¾çš„éº»çƒ¦ã€‚
3. **çµæ´»ä¿å­˜**ï¼šä¸ä»…ä¿å­˜äº†æ¨¡å‹ï¼Œè¿˜å¯ä»¥ä¿å­˜ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€è®­ç»ƒè½®æ¬¡ç­‰ï¼Œå®Œå…¨å¯ä»¥æ¢å¤è®­ç»ƒã€‚

è¿™æ ·åšï¼Œæ¯æ¬¡ä¿å­˜ä¸€ä¸ªæ–°çš„æ¨¡å‹æ–‡ä»¶ï¼Œæ¨¡å‹ç‰ˆæœ¬é€æ¸å¢å¤šæ—¶ï¼Œä½ å¯ä»¥è½»æ¾æ¢å¤åˆ°æœ€è¿‘çš„è®­ç»ƒçŠ¶æ€ã€‚å¦‚æœä»¥åæƒ³è¦å›æº¯åˆ°ä¹‹å‰çš„æŸä¸ªç‰ˆæœ¬ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ–‡ä»¶åæŸ¥çœ‹å“ªä¸ªæœ€åˆé€‚ã€‚

è¿™ä¸‹ä½ å°±èƒ½å®Œå…¨æŒæ§æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å•¦ï¼ğŸ‰

## æ™®é€šç±»çš„ä¿å­˜å’ŒåŠ è½½

æ¯”å¦‚ä¸€ä¸ªè‡ªå·±å®ç°çš„ä¸€ä¸ªlrçš„schedulerï¼Œæˆ‘è¯¥æ€ä¹ˆå†™def state_dict(self)å’Œdef load_state_dict(self, state_dict)ï¼Ÿ

éå¸¸å¥½çš„é—®é¢˜ï¼å¦‚æœä½ å†™ä¸€ä¸ª**è‡ªå®šä¹‰çš„ç±»**ï¼Œæ¯”å¦‚ä¸€ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆschedulerï¼‰ï¼Œæƒ³è¦åƒ PyTorch æ¨¡å—é‚£æ ·æ”¯æŒ `state_dict()` å’Œ `load_state_dict()`ï¼Œä½ åªéœ€è¦å®ç°ä»¥ä¸‹ä¸¤ä¸ªæ–¹æ³•ï¼š

âœ… ä¸€èˆ¬æ¨èçš„å†™æ³•å¦‚ä¸‹ï¼š

```python
from collections import OrderedDict

class MyLRScheduler:
    def __init__(self, base_lr=0.01, decay_rate=0.9):
        self.base_lr = base_lr
        self.decay_rate = decay_rate
        self.step_num = 0

    def step(self):
        self.step_num += 1

    def get_lr(self):
        return self.base_lr * (self.decay_rate ** self.step_num)

    def state_dict(self):
        return OrderedDict({
            "version": 1,
            "params": OrderedDict({
                "base_lr": self.base_lr,
                "decay_rate": self.decay_rate,
            }),
            "state": OrderedDict({
                "step_num": self.step_num,
            })
        })

    def load_state_dict(self, state_dict):
        assert state_dict["version"] == 1, "Unsupported state version"
        self.base_lr = state_dict["params"]["base_lr"]
        self.decay_rate = state_dict["params"]["decay_rate"]
        self.step_num = state_dict["state"]["step_num"]
```

ğŸ§  ä½¿ç”¨æ–¹å¼ç¤ºä¾‹ï¼š

```python
scheduler = MyLRScheduler(0.01, 0.95)
scheduler.step()
scheduler.step()

# ä¿å­˜
ckpt = scheduler.state_dict()

# åŠ è½½
new_scheduler = MyLRScheduler(0.1, 0.5)
new_scheduler.load_state_dict(ckpt)
```

ğŸ§© è®¾è®¡å»ºè®®ï¼š

| é¡¹ç›®         | å»ºè®®è¯´æ˜                                                     |
| ------------ | ------------------------------------------------------------ |
| å¯ä¿å­˜çš„å˜é‡ | ä»…ä¿å­˜èƒ½å½±å“è°ƒåº¦å™¨è¡Œä¸ºçš„å…³é”®å˜é‡ï¼ˆå¦‚è¶…å‚ã€å†…éƒ¨çŠ¶æ€ï¼‰         |
| å‚æ•°ç±»å‹     | æ¨èä½¿ç”¨åŸºæœ¬ç±»å‹ï¼ˆint, float, str, list, dictï¼‰              |
| é”®çš„å‘½å     | ä½¿ç”¨åŒå¼•å· `"key"` é£æ ¼ï¼Œå’Œ PyTorch ä¿æŒä¸€è‡´                 |
| å‘åå…¼å®¹     | å¦‚æœå°†æ¥ä½ å‡çº§ç‰ˆæœ¬ï¼Œå¯ä»¥åŠ ä¸Š `"version"` å­—æ®µæ–¹ä¾¿å…¼å®¹è€çš„ state_dict |

ğŸ§© æ‰©å±•æŠ€å·§ï¼ˆå¦‚æœä½ æœ‰å¤šå‚æ•°ç»„æˆ–è‡ªå®šä¹‰é€»è¾‘ï¼‰

```python
def state_dict(self):
    return {
        "version": 1,
        "params": {
            "base_lr": self.base_lr,
            "decay_rate": self.decay_rate,
        },
        "state": {
            "step_num": self.step_num,
        }
    }
```

è¿™æ ·å°†è¶…å‚æ•°å’ŒçŠ¶æ€å˜é‡åˆ†ç¦»ï¼Œä¾¿äºç»´æŠ¤ã€‚

é‚£å¯¹åº”çš„loadæ€ä¹ˆå†™å‘¢

ä½ å·²ç»æ„å»ºäº†ä¸€ä¸ªç»“æ„æ¸…æ™°çš„ `state_dict`ã€‚é‚£å¯¹åº”çš„ `load_state_dict` åº”è¯¥ä¸¥æ ¼æŒ‰ç…§ç›¸åŒçš„ç»“æ„å»è¯»å–ï¼Œæ¨èå¦‚ä¸‹å†™æ³•ï¼š

âœ… å¯¹åº”çš„ `load_state_dict` ç¤ºä¾‹ï¼š

```python
def load_state_dict(self, state_dict):
    # å¯é€‰ï¼šæ£€æŸ¥ç‰ˆæœ¬å·ï¼Œç¡®ä¿å…¼å®¹æ€§
    version = state_dict.get("version", 1)
    if version != 1:
        raise ValueError(f"Unsupported state_dict version: {version}")

    # åŠ è½½å‚æ•°
    params = state_dict["params"]
    self.base_lr = params["base_lr"]
    self.decay_rate = params["decay_rate"]

    # åŠ è½½çŠ¶æ€
    state = state_dict["state"]
    self.step_num = state["step_num"]
```

ğŸ”’ åŠ å¼ºå¥å£®æ€§ï¼ˆæ¨èåšæ³•ï¼‰

å¦‚æœä½ å¸Œæœ›è¿™ä¸ªè°ƒåº¦å™¨æœªæ¥å¯ä»¥çƒ­åŠ è½½æ—§ç‰ˆæœ¬ï¼Œæˆ–è€…ç”¨äºæ¨¡å‹è¿ç§»ï¼Œå»ºè®®åŠ ä¸Šä¸€äº›é»˜è®¤å€¼æˆ–å®¹é”™ï¼š

```python
def load_state_dict(self, state_dict):
    version = state_dict.get("version", 1)

    if version == 1:
        params = state_dict.get("params", {})
        self.base_lr = params.get("base_lr", self.base_lr)
        self.decay_rate = params.get("decay_rate", self.decay_rate)

        state = state_dict.get("state", {})
        self.step_num = state.get("step_num", 0)
    else:
        raise ValueError(f"Unsupported state_dict version: {version}")
```



# å‚è€ƒèµ„æ–™

* [pytorchæ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½](https://blog.csdn.net/lsb2002/article/details/131969478)

æœ¬æ–‡å‚è€ƒæ­¤èµ„æ–™ã€‚

