# 自然语言处理

* [返回上层目录](../README.md)
* [自然语言处理概论](natural-language-processing-introduction/natural-language-processing-introduction.md)
* [自然语言](natural-language/natural-language.md)
* [语言模型和中文分词](language-model-and-chinese-word-segmentation/language-model-and-chinese-word-segmentation.md)
* [TF-IDF词频-逆文档频率](tf-idf/tf-idf.md)
* [word2vec](word2vec/word2vec.md)
* [Seq2Seq模型和Attention机制](seq2seq-and-attention-mechanism/seq2seq-and-attention-mechanism.md)
* [Self-Attention和Transformer](self-attention-and-transformer/self-attention-and-transformer.md)

===



[**Mixture-of-Denoisers**击败GPT3，刷新50个SOTA！谷歌全面统一NLP范式](https://mp.weixin.qq.com/s/oMUASBSKe3xgGVLuQz7MGg)

[详解预训练模型 ——从词向量到GPT模型](https://blog.csdn.net/haojiangcong/article/details/121565294)


[从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)

[从One-hot, Word embedding到Transformer，一步步教你理解Bert](https://mp.weixin.qq.com/s/rxkHtRPMrEZPHzyq2UD4Fg)

[从Word2Vec到Bert，聊聊词向量的前世今生（一）](https://mp.weixin.qq.com/s/LaikrZUGlzn864ttdeyBLQ)





