# è‡ªå®šä¹‰LearningRateScheduler

- [è¿”å›ä¸Šå±‚ç›®å½•](../learning-rate.md)
- [æºç ä¸æµ‹è¯•è„šæœ¬](#æºç ä¸æµ‹è¯•è„šæœ¬)
  - [æºç ](#æºç )
  - [æµ‹è¯•è„šæœ¬](#æµ‹è¯•è„šæœ¬)
  - [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æºç çš„åŠŸèƒ½ç¤ºä¾‹](#æºç çš„åŠŸèƒ½ç¤ºä¾‹)
  - [ä¿å­˜ä¸æ¢å¤](#ä¿å­˜ä¸æ¢å¤)
  - [å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–](#å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–)
  - [lambdaè‡ªå®šä¹‰è°ƒåº¦ç­–ç•¥](#lambdaè‡ªå®šä¹‰è°ƒåº¦ç­–ç•¥)
  - [å‚æ•°ç»„å¯è§†åŒ–](#å‚æ•°ç»„å¯è§†åŒ–)
  - [å•å…ƒæµ‹è¯•](#å•å…ƒæµ‹è¯•)
- [é—®é¢˜](#)
  - [æœ€å¤§å’Œæœ€å°å­¦ä¹ ç‡æ¯”å€¼çš„é€‰å–](#æœ€å¤§å’Œæœ€å°å­¦ä¹ ç‡æ¯”å€¼çš„é€‰å–)



æœ¬é¡¹ç›®æä¾›äº†ä¸€ä¸ªç»“æ„æ¸…æ™°ã€åŠŸèƒ½å…¨é¢ã€å®ç”¨æ€§å¼ºçš„**PyTorchè‡ªå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦å™¨**ï¼Œé€‚ç”¨äºæ·±åº¦å¼ºåŒ–å­¦ä¹ ç­‰å¯¹è®­ç»ƒè¿‡ç¨‹æœ‰ç²¾ç»†æ§åˆ¶éœ€æ±‚çš„åœºæ™¯ã€‚

è°ƒåº¦å™¨æ”¯æŒå¤šç§å…ˆè¿›åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š

- **Warm-upæœºåˆ¶**ï¼šæ”¯æŒçº¿æ€§æˆ–ä½™å¼¦æ–¹å¼è¿›è¡Œå­¦ä¹ ç‡é¢„çƒ­ï¼›
- **ä½™å¼¦é€€ç« + é‡å¯**ï¼šé€šè¿‡å‘¨æœŸæ€§é‡å¯æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼›
- **å‘¨æœŸå†…æŒ‡æ•°è¡°å‡**ï¼šæ¯ä¸ªè®­ç»ƒå‘¨æœŸå†…å¯è®¾ç½®æŒ‡æ•°å¼è¡°å‡ç­–ç•¥ï¼›
- **Batch Sizeè‡ªé€‚åº”ç¼©æ”¾**ï¼šæ ¹æ®å½“å‰batch sizeåŠ¨æ€ç¼©æ”¾å­¦ä¹ ç‡ï¼Œæ”¯æŒçº¿æ€§ã€å¹³æ–¹æ ¹ã€æ··åˆç­‰ç¼©æ”¾æ–¹å¼ï¼›
- **å¤šå‚æ•°ç»„ï¼ˆparam groupï¼‰æ”¯æŒ**ï¼šå¯ä¸ºæ¯ç»„å‚æ•°è®¾å®šä¸åŒçš„åˆå§‹å­¦ä¹ ç‡åŠè°ƒåº¦ç­–ç•¥ï¼›
- **Lambda è‡ªå®šä¹‰è°ƒåº¦å‡½æ•°**ï¼šæ”¯æŒä»»æ„å‡½æ•°å®šä¹‰è°ƒåº¦ç­–ç•¥ï¼Œçµæ´»æ€§æé«˜ï¼›
- **è®­ç»ƒä¸­æ–­æ¢å¤æœºåˆ¶**ï¼šæ”¯æŒå®Œæ•´çš„å­¦ä¹ ç‡çŠ¶æ€ä¿å­˜ä¸æ¢å¤ï¼Œä¾¿äºæ–­ç‚¹ç»­è®­ï¼›
- **å¯è§†åŒ–å·¥å…·**ï¼šå†…ç½®å­¦ä¹ ç‡æ›²çº¿ç»˜å›¾ã€ä¿å­˜ä¸æ¢å¤æœºåˆ¶ï¼Œä¾¿äºè§‚å¯Ÿå’Œè°ƒè¯•è®­ç»ƒè¿‡ç¨‹ï¼›

æ ¸å¿ƒè°ƒåº¦å™¨é€šè¿‡ä¸€ä¸ªç»Ÿä¸€çš„ç±» `LRScheduler` å®ç°ï¼Œæ¥å£å‹å¥½ï¼Œä¾¿äºé›†æˆåˆ°ä»»æ„PyTorchè®­ç»ƒæµç¨‹ä¸­ï¼Œå¹¶å¯¹ä¸åŒè®­ç»ƒéœ€æ±‚æä¾›å¼ºå¤§ä¸”å¯æ§çš„æ”¯æŒã€‚

å…¶å¯¹å­¦ä¹ ç‡çš„ä½œç”¨å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

![lr-curve](pic/lr-curve.png)

# æºç ä¸æµ‹è¯•è„šæœ¬

## æºç 

åˆå§‹åŒ–å‚æ•°è¯´æ˜ï¼š

| å‚æ•°å             | ç±»å‹      | é»˜è®¤å€¼   | è¯´æ˜                           |
| ------------------ | --------- | -------- | ------------------------------ |
| `optimizer`        | Optimizer | -        | PyTorch ä¼˜åŒ–å™¨                 |
| `warmup_steps`     | int       | 100      | warm-up æ­¥æ•°                   |
| `warmup_scale_min` | float     | 0.05     | warm-up èµ·å§‹æ¯”ä¾‹               |
| `warmup_type`      | str       | "linear" | å¯é€‰ `linear` æˆ– `cosine`      |
| `T_0`              | int       | 200      | ç¬¬ä¸€ä¸ªä½™å¼¦é‡å¯å‘¨æœŸé•¿åº¦         |
| `T_mult`           | int       | 2        | æ¯æ¬¡é‡å¯åå‘¨æœŸé•¿åº¦ä¹˜æ•°         |
| `eta_scal_min`     | float     | 0.2      | æœ€ä½å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹             |
| `eta_scal_max`     | float     | 2.0      | æœ€å¤§å­¦ä¹ ç‡ç¼©æ”¾æ¯”ä¾‹             |
| `gamma`            | float     | 0.97     | æ¯æ¬¡é‡å¯åçš„è¡°å‡å› å­           |
| `base_batch_size`  | int       | 64       | å‚è€ƒ batch sizeï¼Œç”¨äºç¼©æ”¾      |
| `batch_size_mode`  | str       | "sqrt"   | å¯é€‰ `linear`ã€`sqrt`ã€`blend` |

```python
"""
Custom Learning Rate Scheduler for PyTorch
Supports:
- Warm-up (linear or cosine)
- Cosine Annealing with Warm Restarts
- Exponential Decay (per cycle)
- Batch size scaling (linear, sqrt, blend)
- Multi-parameter-group compatibility
- Custom lambda-based scheduling function
- Learning rate visualization and full training checkpointing support (save/load/resume)
"""
import math
from typing import Literal, Optional, Callable
from torch.optim import Optimizer


# å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆæ”¯æŒbatch_sizeè‡ªé€‚åº”ã€å‘¨æœŸé‡å¯ã€warmupã€æŒ‡æ•°è¡°å‡ã€ä¿å­˜æ¢å¤ï¼‰
class LRScheduler:
    def __init__(
            self,
            optimizer: Optimizer,  # torch.optim.Optimizerå¯¹è±¡
            # é¢„çƒ­æœºåˆ¶
            warmup_steps: int = 100,
            warmup_scale_min: float = 0.05,  # warmupé˜¶æ®µçš„æ¯”ä¾‹ä¸€å¼€å§‹å°±æ˜¯è¯¥å€¼ï¼Œåˆ«ä»0å¼€å§‹
            warmup_type: Literal["linear", "cosine"] = "linear",  # é¢„çƒ­æœºåˆ¶æ˜¯çº¿æ€§ä¸Šå‡è¿˜æ˜¯ä½™å¼¦ä¸Šå‡
            # ä½™å¼¦é‡å¯
            T_0: int = 200,  # æ¯T_0æ­¥è¿›è¡Œä¸€æ¬¡é‡å¯ï¼Œä½†å‘¨æœŸæ­¥æ•°ä¼šæŒ‡æ•°çº§å¢é•¿
            T_mult: int = 2,  # ä½™å¼¦é‡å¯çš„å‘¨æœŸå¢é•¿æ¯”ä¾‹
            eta_scal_min: float = 0.2,  # ä½™å¼¦é‡å¯çš„æœ€å°å­¦ä¹ ç‡å¯¹é»˜è®¤å­¦ä¹ ç‡çš„æ¯”å€¼
            eta_scal_max: float = 2.0,  # ä½™å¼¦é‡å¯çš„æœ€å¤§å­¦ä¹ ç‡å¯¹é»˜è®¤å­¦ä¹ ç‡çš„æ¯”å€¼
            # æŒ‡æ•°è¡°å‡
            gamma: float = 0.97,  # æŒ‡æ•°è¡°å‡ç³»æ•°ï¼Œåœ¨æ¯æ¬¡ä½™å¼¦é‡å¯çš„æ—¶å€™æ‰è¡°å‡ï¼Œè€Œä¸æ˜¯æ¯ä¸€æ­¥
            # BatchSizeç¼©æ”¾
            base_batch_size: int = 64,  # å‚è€ƒå­¦ä¹ ç‡çš„åŸºç¡€batch size
            batch_size_mode: Literal["linear", "sqrt", "blend"] = "sqrt",
            #
            custom_lambda: Optional[Callable[[int], float]] = None  # æˆ·å¯ä»¥ä¼ å…¥ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å— global_stepï¼Œè¿”å›ä¸€ä¸ªç”¨äºç¼©æ”¾å­¦ä¹ ç‡çš„å€¼ã€‚
    ):
        self.optimizer = optimizer
        self.base_lrs = [group['lr'] for group in self.optimizer.param_groups]
        # å†·å¯åŠ¨
        self.warmup_steps = warmup_steps
        self.warmup_scale_min = warmup_scale_min
        self.warmup_type = warmup_type
        # ä½™å¼¦é‡å¯
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min_lrs = [base_lr * eta_scal_min for base_lr in self.base_lrs]
        self.eta_max_lrs = [base_lr * eta_scal_max for base_lr in self.base_lrs]
        # æŒ‡æ•°è¡°å‡
        self.gamma = gamma
        # BatchSizeç¼©æ”¾
        self.base_batch_size = base_batch_size
        self.batch_size_mode = batch_size_mode
        #
        self.custom_lambda = custom_lambda  # ä¿å­˜ç”¨æˆ·ä¼ å…¥çš„å‡½æ•°

        self.global_step = 0
        self.T_i = T_0  # å½“å‰å‘¨æœŸé•¿åº¦
        self.T_cur = 0  # å½“å‰å‘¨æœŸå†…çš„stepæ•°
        self.cycle = 0  # å½“å‰æ˜¯ç¬¬å‡ æ¬¡é‡å¯

        self.lr_scale = 1

    def step(self, batch_size=None):
        self.global_step += 1

        scale_batch = self.batch_size_step(batch_size)
        scale_warmup = self.warmup_step()
        scale_cosine = self.cosine_annealing_warm_restarts_step()
        scale_decay = self.exponential_decrease_step()
        scale_custom = self.custom_lambda_step()

        self.lr_scale = scale_batch * scale_warmup * scale_cosine * scale_decay * scale_custom

        for i, param_group in enumerate(self.optimizer.param_groups):
            lr_min, lr_max = self.eta_min_lrs[i], self.eta_max_lrs[i]
            adjusted_lr = lr_min + (lr_max - lr_min) * self.lr_scale
            param_group["lr"] = adjusted_lr

    def warmup_step(self):
        if self.global_step <= self.warmup_steps:
            progress = self.global_step / self.warmup_steps
            if self.warmup_type == "cosine":
                return max(self.warmup_scale_min, 0.5 * (1 - math.cos(progress * math.pi)))
            else:
                return max(self.warmup_scale_min, progress)
        else:
            return 1.0

    def cosine_annealing_warm_restarts_step(self):
        if self.global_step <= self.warmup_steps:
            return 1.0
        else:
            # ä½™å¼¦é‡å¯é˜¶æ®µ
            self.T_cur += 1
            if self.T_cur >= self.T_i:
                self.T_cur = 0
                self.T_i = self.T_i * self.T_mult  # æ³¨æ„ï¼Œè¿™é‡Œä¸èƒ½æ˜¯**
                self.cycle += 1

            cos_inner = math.pi * self.T_cur / self.T_i
            scale = 0.5 * (1 + math.cos(cos_inner))
            return scale

    def exponential_decrease_step(self):
        if self.global_step <= self.warmup_steps:
            return 1.0
        else:
            # æ¯è½®ç»“æŸåbaseå­¦ä¹ ç‡ä¹˜ä»¥gamma
            scale = self.gamma**self.cycle
            return scale

    def batch_size_step(self, batch_size=None):
        """
        æ ¹æ®å½“å‰batch sizeæ›´æ–°å­¦ä¹ ç‡
        :param batch_size: å½“å‰batch size
        """
        if batch_size is None:
            return 1

        batch_size = max(1, batch_size)

        scale_linear = batch_size / self.base_batch_size  # çº¿æ€§å¢åŠ å­¦ä¹ ç‡
        scale_sqrt = math.sqrt(batch_size / self.base_batch_size)  # å¼€æ–¹å¢åŠ å­¦ä¹ ç‡
        scale_blend = 0.5 * scale_linear + 0.5 * scale_sqrt  # æ··åˆ

        return {"linear": scale_linear, "sqrt": scale_sqrt}.get(self.batch_size_mode, scale_blend)

    def custom_lambda_step(self):
        return self.custom_lambda(self.global_step) if self.custom_lambda else 1.0

    def plot(self, max_steps, batch_size=None):
        """
        ç»˜åˆ¶å­¦ä¹ ç‡çš„å˜åŒ–æ›²çº¿ã€‚
        :param max_steps: æœ€å¤§æ­¥éª¤æ•°
        :param batch_size: å½“å‰çš„batch_size
        """
        import matplotlib.pyplot as plt
        steps = list(range(max_steps))
        lr_scales = []
        lr_groups = []

        # clone å½“å‰çŠ¶æ€ï¼Œé˜²æ­¢ç ´åç°æœ‰ scheduler çŠ¶æ€
        backup_state = self.state_dict()

        for _ in range(max_steps):
            self.step(batch_size)
            lr_scales.append(self.lr_scale)
            lr_groups.append([param_group["lr"] for param_group in self.optimizer.param_groups])

        # æ¢å¤åŸå§‹çŠ¶æ€ï¼Œä¿è¯è®­ç»ƒä¸å—å½±å“
        self.load_state_dict(backup_state)

        # --------- ç”»å›¾éƒ¨åˆ† ---------
        fig, axs = plt.subplots(2, 1, figsize=(10, 8))
        # å›¾1ï¼šå®é™…å­¦ä¹ ç‡å˜åŒ–ï¼ˆæ¯ä¸ªparam groupä¸€æ¡çº¿ï¼‰
        for i in range(len(self.optimizer.param_groups)):
            axs[0].plot(steps, [lr[i] for lr in lr_groups], label=f'Group {i}')
        axs[0].set_title('Learning Rate per Param Group')
        axs[0].set_xlabel('Steps')
        axs[0].set_ylabel('Learning Rate')
        axs[0].legend()

        # å›¾2ï¼šlr_scale çš„å˜åŒ–
        axs[1].plot(steps, lr_scales, label='lr_scale', color='purple')
        axs[1].set_title('Global LR Scale Multiplier')
        axs[1].set_xlabel('Steps')
        axs[1].set_ylabel('LR Scale')
        axs[1].legend()

        plt.tight_layout()
        plt.show()

    def state_dict(self):
        return {
            'global_step': self.global_step,
            'T_i': self.T_i,
            'T_cur': self.T_cur,
            'cycle': self.cycle,
        }

    def load_state_dict(self, state_dict):
        self.global_step = state_dict['global_step']
        self.T_i = state_dict['T_i']
        self.T_cur = state_dict['T_cur']
        self.cycle = state_dict['cycle']

    @staticmethod
    def visualize_optimizer_param_groups(model, optimizer):
        # è·å–å¸¦åå­—çš„å‚æ•°ï¼Œç”¨äºåŒ¹é… param group å†…çš„å‚æ•°
        name_map = {p: n for n, p in model.named_parameters()}

        for i, group in enumerate(optimizer.param_groups):
            print(f"\nğŸŸ¢ Param Group {i}:")
            print(f"  â†ª learning rate (lr): {group.get('lr', 'N/A')}")
            print(f"  â†ª epsilon (eps): {group.get('eps', 'N/A')}")
            print(f"  â†ª weight_decay: {group.get('weight_decay', 'N/A')}")
            print(f"  â†ª Parameters:")

            for param in group["params"]:
                name = name_map.get(param, "âš ï¸ unnamed")
                print(f"     - {name:30} | shape: {tuple(param.shape)}")
```

## æµ‹è¯•è„šæœ¬

æŠŠä¸Šé¢çš„ç±»çš„ä»£ç å¤åˆ¶åˆ°ä¸‹é¢çš„æµ‹è¯•è„šæœ¬ä¸­

```python
if __name__ == '__main__':
    import torch

    # æ¨¡å‹å®šä¹‰
    def create_model():
        model_A = torch.nn.Linear(4, 3)
        model_B = torch.nn.Linear(3, 2)
        model_C = torch.nn.Linear(2, 1)
        return torch.nn.Sequential(model_A, model_B, model_C)

    # ä¼˜åŒ–å™¨åˆå§‹åŒ–
    def create_optimizer(model):
        return torch.optim.Adam([
            {"params": model[0].parameters(), "lr": 0.001},
            {"params": model[1].parameters(), "lr": 0.002},
            {"params": model[2].parameters(), "lr": 0.003},
        ])

    # æ¨¡æ‹Ÿè®­ç»ƒ
    def train_and_save(model, optimizer, scheduler, steps=300, save_step=150, ckpt_path="scheduler_ckpt.pth"):
        for step in range(steps):
            scheduler.step(batch_size=128)  # è¿™é‡Œå‡è®¾æ¯æ­¥ batch size æ˜¯ 128
            if step == save_step:
                torch.save({
                    "model": model.state_dict(),
                    "optimizer": optimizer.state_dict(),
                    "scheduler": scheduler.state_dict(),
                }, ckpt_path)
                print(f"âœ… Checkpoint saved at step {step}")
        print(
            f"ğŸ§® Final scheduler state: step={scheduler.global_step}, cycle={scheduler.cycle}, T_cur={scheduler.T_cur}")

    # æ¨¡æ‹Ÿæ¢å¤
    def load_and_resume(ckpt_path="scheduler_ckpt.pth"):
        model = create_model()
        optimizer = create_optimizer(model)
        scheduler = LRScheduler(optimizer, warmup_steps=200, T_0=100, gamma=0.9)

        checkpoint = torch.load(ckpt_path)
        model.load_state_dict(checkpoint["model"])
        optimizer.load_state_dict(checkpoint["optimizer"])
        scheduler.load_state_dict(checkpoint["scheduler"])

        print(f"âœ… Scheduler restored: step={scheduler.global_step}, cycle={scheduler.cycle}, T_cur={scheduler.T_cur}")
        return model, optimizer, scheduler

    def custom_schedule_fn(step: int) -> float:
        # ä¸¾ä¾‹ï¼šè®­ç»ƒæ—©æœŸä¿æŒ1.0ï¼Œä¸­åæœŸçº¿æ€§ä¸‹é™åˆ°0.5
        # return 1.0 if step < 1000 else max(0.5, 1 - (step - 1000) * 1e-4)
        return 1.0

    checkpoint_path = "scheduler_ckpt.pth"

    # ç¬¬ä¸€æ¬¡è®­ç»ƒå¹¶ä¿å­˜
    model = create_model()
    optimizer = create_optimizer(model)
    scheduler = LRScheduler(
        optimizer,
        warmup_steps=200,
        T_0=100,
        gamma=0.9,
        custom_lambda=custom_schedule_fn
    )
    train_and_save(model, optimizer, scheduler, steps=300, save_step=150, ckpt_path=checkpoint_path)

    # æ¢å¤åç»§ç»­è®­ç»ƒ
    model_restored, optimizer_restored, scheduler_restored = load_and_resume(checkpoint_path)

    # æ‰“å°å„ä¸ªå‚æ•°ç»„çš„å­¦ä¹ ç‡ï¼Œç”¨äºæ ¸å®é¡ºåº
    for i, group in enumerate(optimizer.param_groups):
        print(f"Param group {i} learning rate: {group['lr']}")

    # è®¾ç½®åŸºç¡€å­¦ä¹ ç‡
    scheduler = LRScheduler(
        optimizer,
        base_batch_size=64  # é»˜è®¤åŸºç¡€batch size
    )

    # æ¯è½®è®­ç»ƒåæ›´æ–°å­¦ä¹ ç‡
    for _ in range(0):
        current_batch_size = 128
        scheduler.step(current_batch_size)  # ä¼ å…¥å½“å‰å®é™…çš„batch size

    # æ˜¾ç¤ºæ¨¡å‹ç»“æ„
    scheduler.visualize_optimizer_param_groups(model, optimizer)

    # ç”»å‡ºå­¦ä¹ ç‡å˜åŒ–æ›²çº¿
    scheduler.plot(3000)
```

## ä½¿ç”¨ç¤ºä¾‹

```python
model = create_model()
optimizer = create_optimizer(model)

scheduler = LRScheduler(
    optimizer,
    warmup_steps=200,
    T_0=100,
    gamma=0.9,
    base_batch_size=64,
    batch_size_mode='sqrt'
)

for step in range(total_steps):
    ...
    scheduler.step(batch_size=å½“å‰å®é™…batch_size)
    ...
```

# æºç çš„åŠŸèƒ½ç¤ºä¾‹

## ä¿å­˜ä¸æ¢å¤

```python
# ä¿å­˜
torch.save({
    "model": model.state_dict(),
    "optimizer": optimizer.state_dict(),
    "scheduler": scheduler.state_dict(),
}, "checkpoint.pth")

# æ¢å¤
checkpoint = torch.load("checkpoint.pth")
model.load_state_dict(checkpoint["model"])
optimizer.load_state_dict(checkpoint["optimizer"])
scheduler.load_state_dict(checkpoint["scheduler"])
```

**é—®é¢˜**ï¼šä¸€èˆ¬æ¥è¯´ï¼Œä¿å­˜æ¨¡å‹æ—¶ä¹Ÿè¦ä¿å­˜è¿™ä¸ªå­¦ä¹ ç‡è°ƒåº¦å™¨æ˜¯å§ï¼Œé‚£æ˜¯åŠ åœ¨ä¸€èµ·ä¿å­˜åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œå‘¢ï¼Œè¿˜æ˜¯åˆ†å¼€ä¿å­˜ï¼Ÿ

ä¸€èˆ¬æ¥è¯´ï¼Œ**ä¿å­˜æ¨¡å‹æ—¶ä¹Ÿä¼šä¸€å¹¶ä¿å­˜ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨çš„çŠ¶æ€**ï¼Œè¿™æ ·ä½ åœ¨æ¢å¤è®­ç»ƒæ—¶èƒ½ä»å®Œå…¨ä¸€æ ·çš„è®­ç»ƒçŠ¶æ€ç»§ç»­ï¼Œæ— ç¼è¡”æ¥ã€‚

æ¨èçš„åšæ³•ï¼š**ç»Ÿä¸€ä¿å­˜åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œ**

è¿™æ ·ç®¡ç†æ›´æ–¹ä¾¿ï¼Œæ¨èä½ ç”¨ä»¥ä¸‹æ ¼å¼æ‰“åŒ…ä¿å­˜ï¼š

```python
torch.save({
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'scheduler_state_dict': scheduler.state_dict(),
    'epoch': current_epoch,  # å¯é€‰ï¼šä½ è®­ç»ƒäº†å‡ è½®
    'step': global_step,     # å¯é€‰ï¼šä½ è®­ç»ƒäº†å‡ ä¸ª batch
}, 'checkpoint.pth')
```

åŠ è½½çš„æ—¶å€™ï¼š

```python
checkpoint = torch.load('checkpoint.pth')

model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

# å¦‚æœæœ‰ä¿å­˜ epoch/stepï¼Œå¯ä»¥ç»§ç»­ä»é‚£å„¿è®­ç»ƒ
start_epoch = checkpoint.get('epoch', 0)
global_step = checkpoint.get('step', 0)
```

å•ç‹¬ä¿å­˜æœ‰å¿…è¦å—ï¼Ÿ

å•ç‹¬ä¿å­˜schedulerçš„`.pth`æ–‡ä»¶ä¹Ÿæ˜¯å¯ä»¥çš„ï¼Œ**ä½†é€šå¸¸ä¸æ¨è**ï¼Œå› ä¸ºï¼š

- ä½ æ¢å¤è®­ç»ƒæ—¶å¿…é¡»è®°å¾—æŠŠ scheduler å’Œ optimizer ä¸€èµ·æ¢å¤ï¼›
- å¤šä¸ªæ–‡ä»¶ç®¡ç†ä¸æ–¹ä¾¿ï¼Œä¸å¦‚é›†ä¸­åœ¨ä¸€ä¸ª `checkpoint.pth` æ–‡ä»¶ä¸­ï¼›
- é™¤éä½ ç‰¹åˆ«éœ€è¦æ‹†å¼€ä¿å­˜ï¼ˆæ¯”å¦‚åªæ›´æ–° scheduler è€Œä¸åŠ¨æ¨¡å‹ï¼‰ï¼Œæ‰è€ƒè™‘åˆ†å¼€ä¿å­˜ã€‚

## å¯è§†åŒ–å­¦ä¹ ç‡å˜åŒ–

```python
scheduler.plot(max_steps=1000, batch_size=128)
```

![lr-curve](pic/lr-curve.png)

## lambdaè‡ªå®šä¹‰è°ƒåº¦ç­–ç•¥

ç”¨æˆ·å¯ä»¥ä¼ å…¥ä¸€ä¸ªå‡½æ•°ï¼Œæ¥æ”¶`global_step`ï¼Œè¿”å›ä¸€ä¸ªç”¨äºç¼©æ”¾å­¦ä¹ ç‡çš„å€¼ã€‚

```python
def custom_schedule_fn(step: int) -> float:
    # ä¸¾ä¾‹ï¼šè®­ç»ƒæ—©æœŸä¿æŒ1.0ï¼Œä¸­åæœŸçº¿æ€§ä¸‹é™åˆ°0.5
    return 1.0 if step < 1000 else max(0.5, 1 - (step - 1000) * 1e-4)

scheduler = LRScheduler(
    optimizer,
    warmup_steps=200,
    T_0=100,
    gamma=0.9,
    custom_lambda=custom_schedule_fn
)
```

è¿™é¡¹æ‰©å±•è®©`LRScheduler`ï¼š

- æ›´é€šç”¨ï¼Œç”¨æˆ·å¯ä»¥è½»æ¾å®šä¹‰ä»»æ„è°ƒåº¦ç­–ç•¥ï¼›
- ä¸PyTorchçš„`LambdaLR`ä¿æŒä¸€è‡´ï¼Œä½†åŠŸèƒ½æ›´å¼ºï¼›
- ä¸å·²æœ‰çš„warmup + cosine + gamma + batch_sizeç¼©æ”¾å®Œå…¨å…¼å®¹ã€‚

æ³¨æ„ï¼šä¿å­˜ä¸æ¢å¤ `custom_lambda`ï¼š

ä¸€èˆ¬æˆ‘ä»¬**ä¸åºåˆ—åŒ–å‡½æ•°æœ¬èº«**ï¼Œä½ å¯ä»¥åœ¨ `state_dict()` å’Œ `load_state_dict()` ä¸­å¿½ç•¥ `custom_lambda`ï¼Œè®©ç”¨æˆ·åœ¨æ¢å¤åé‡æ–°è®¾ç½®ã€‚

## å‚æ•°ç»„å¯è§†åŒ–

```python
LRScheduler.visualize_optimizer_param_groups(model, optimizer)
```

![visualize-param](pic/visualize-param.png)

## å•å…ƒæµ‹è¯•

åœ¨`__main__`ä¸­åŒ…å«äº†å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹å’Œä¿å­˜/æ¢å¤æµç¨‹ï¼Œé€‚åˆè°ƒè¯•ã€‚

# é—®é¢˜

## æœ€å¤§å’Œæœ€å°å­¦ä¹ ç‡æ¯”å€¼çš„é€‰å–

> ä¾‹å¦‚ eta_scal_min = 0.2 å’Œ eta_scal_max = 2.0 å…¶å®æ˜¯è°ƒèŠ‚èŒƒå›´å› å­ï¼Œä½†å¸¸è§é»˜è®¤å¯èƒ½æ˜¯ 0.1 å’Œ 1.0ï¼Œ

ä¸ºä»€ä¹ˆé»˜è®¤çš„æœ€å¤§æ¯”ä¾‹æ˜¯1å‘¢ï¼Ÿè¿™æ ·ä¼šè®©æ•´ä½“çš„å¹³å‡å­¦ä¹ ç‡å°äºé»˜è®¤å€¼ï¼Œæˆ‘è®¾ç½®ä¸º2æ˜¯å› ä¸ºï¼š

* æ”¾å¤§å­¦ä¹ ç‡ï¼Œè®©å­¦ä¹ ç‡æ¯”è¾ƒå¤§çš„æ—¶å€™èƒ½è·³å‡ºæå°å€¼

* è®©æ•´ä½“çš„å¹³å‡å­¦ä¹ ç‡æ¥è¿‘é»˜è®¤çš„å­¦ä¹ ç‡

ä½ çš„ç†è§£éå¸¸åˆ°ä½ï¼Œè€Œä¸”ä½ ç»™å‡ºçš„ä¸¤ä¸ªç†ç”±éƒ½**éå¸¸åˆç†ä¸”å…·æœ‰å®è·µä»·å€¼**ï¼š

**é»˜è®¤`eta_scal_min=0.1`å’Œ`eta_scal_max=1.0`çš„åŸå› ï¼š**

* **ä¿å®ˆç­–ç•¥**ï¼šé»˜è®¤å€¼è®¾è®¡é€šå¸¸å€¾å‘äºâ€œä¿å®ˆâ€ï¼Œä¸ºäº†ç¨³å®šåœ°è¡°å‡å­¦ä¹ ç‡ï¼Œä¸æ”¾å¤§å®ƒï¼Œé™ä½è¿‡æ‹Ÿåˆé£é™©ã€‚
* **é¿å…éœ‡è¡**ï¼šæœ€å¤§å€¼æ˜¯1.0æ„å‘³ç€ä¸ä¼šæ¯”åˆå§‹å­¦ä¹ ç‡æ›´é«˜ï¼Œèƒ½ä¿è¯åˆå§‹æ—¶ä¸ä¼šâ€œæŠ–å¾—å¤ªå‰å®³â€ã€‚
* **æ•´ä½“å¹³å‡å­¦ä¹ ç‡ä¸‹é™**ï¼šä»è€Œå‡å°æ›´æ–°æ­¥é•¿ï¼Œæœ‰åˆ©äºæ”¶æ•›ã€‚

**ä½ è®¾ç½®`eta_scal_max=2.0`çš„ä¸¤ä¸ªç†ç”±è§£æï¼š**

1. **æ”¾å¤§å­¦ä¹ ç‡ï¼Œè·³å‡ºå±€éƒ¨æå°å€¼æˆ–éç‚¹**

- å­¦ä¹ ç‡é«˜ â†’ ä¼˜åŒ–å™¨å¯ä»¥â€œè·³è¿‡â€ä¸å¤ªç†æƒ³çš„è§£
- åœ¨ç­–ç•¥å­¦ä¹ ï¼ˆå°¤å…¶æ˜¯å¼ºåŒ–å­¦ä¹ ï¼‰æˆ–éå‡¸ä¼˜åŒ–ä¸­å°¤å…¶é‡è¦
   ğŸ’¡ ä¾‹å¦‚ï¼Œåœ¨PPOä¸­policyè¢«KLçº¦æŸï¼Œå¦‚æœå­¦ä¹ ç‡è¿‡å°å®¹æ˜“é™·å…¥suboptimal policy

2. **æé«˜å¹³å‡å­¦ä¹ ç‡ï¼Œé¿å…è¿‡æ—©æ”¶æ•›åˆ°suboptimalè§£**

- ä½™å¼¦é€€ç«çš„å¹³å‡å€¼$\approx (\eta_{max} + \eta_{min}) / 2$
- å¦‚æœè®¾ä¸º$(1.0, 0.1)$ï¼Œå¹³å‡ä¸º0.55å€åˆå§‹lrï¼Œå®¹æ˜“å¤ªä½
- ä½ è®¾ä¸º$(2.0, 0.2)$ï¼Œå¹³å‡ä¸º1.1å€åˆå§‹lrï¼Œæ›´æ¥è¿‘base lrï¼Œè€Œä¸”æœ‰æ¢ç´¢æ€§

