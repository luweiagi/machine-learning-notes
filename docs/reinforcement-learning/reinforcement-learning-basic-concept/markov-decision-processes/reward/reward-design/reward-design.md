# 奖励设计

- [返回上层目录](../reward.md)



> 本文总结了多智能体空战强化学习（Multi-Agent RL for Air Combat）中的奖励设计实践与经验教训，重点讨论“何时给奖励、何时放手”，以及弱 shaping 的使用原则。这些经验同样适用于其他连续控制、对抗或稀疏奖励环境。

# 核心设计理念

## 奖励不是约束，奖励是引导

- 奖励函数的目的是引导智能体学习有用行为，而不是直接替代策略决策。
- **原则**：
  - **仅在必要的时候给奖励或惩罚**。比如：敌机导弹逼近、发射窗口存在但未发射导弹等。
  - 对于不易判断的情况，尽量让智能体自主探索。
    - 例：敌机尚未发射导弹，我机可能需要提前拉开距离，但奖励函数难以判断是否必要 → 留给智能体探索。

> 深刻直觉经验 1：
>  “只在环境会退化、策略会坍缩的地方出手，其余全部交给学习。”
>  这样既保证安全和学习效率，又不限制智能体自主策略的多样性。

## 弱 shaping 是训练辅助，不是目标

- **弱 shaping**：非常小的、局部的奖励，用于**稳定早期学习**，例如：
  - 导弹发射后的短期逼近奖励
  - 窗口内不射导弹的机会成本（短时间累计）
- 特性：
  - **稀疏且弱**，防止过度干扰策略
  - 仅用于早期训练加速收敛
  - 可随训练逐渐降低权重，最终可以完全取消
- **核心原则**：
  - Shaping 的目标是加速探索和防止训练初期策略崩溃，不应成为最终策略目标
  - 训练收敛后，只保留最终稀疏奖励（如击落敌机 / 被击落）即可

> 深刻直觉经验 2：
>  “弱 shaping 是训练辅助，不是目标。”
>  只在 early learning 中提供微弱的梯度信号，训练收敛后撤掉不会影响策略性能。

## 过程稀疏奖励 vs 稠密奖励

- **稀疏奖励**：击落敌机、被击落 → 强因果信号
- **稠密奖励**：
  - 仅在特定关键事件发生时提供：
    - 敌机导弹逼近时给出躲避奖励
    - 发射窗口存在时未发射给出轻微机会成本
    - 没有导弹威胁且距离过远时鼓励靠近敌机
- 训练初期：
  - 稀疏奖励不足以引导探索
  - 关键弱 shaping / 稀疏过程奖励有助于学习
- 收敛后：
  - 可以完全取消稠密奖励，只用稀疏奖励
  - 目的是让策略依赖 **最终任务目标**，避免过度依赖奖励函数设计

> 深刻直觉经验 3：
>  “早期奖励可以加速学习，训练收敛后应让最终目标主导策略。”

# 奖励设计实践

## 导弹发射奖励

- **窗口内合理发射**：一次性奖励 + 削弱库存无关
- **机会成本**：
  - 窗口内不射导弹 → 小的累计惩罚
  - 随剩余导弹数量衰减，后期不强迫清空
  - 设置上限，防止长时间累积过度惩罚
- **发射动作成本**：
  - 每次发射直接扣小额成本（per-shot cost）
  - 避免频繁无效发射
- **发射后确认 shaping**：
  - 仅对最近发射导弹且仍在确认时间窗内
  - 奖励极小，用于稳定 early learning
  - 可选择只奖励逼近敌机，不惩罚离开敌机

**原则总结**：

- 尽量只奖励最近发射的导弹，避免多弹同时在空中导致线性叠加
- 保留最大自由度，让智能体学习何时回避或攻击
- 弱 shaping 强度非常低，时间窗口短

## 导弹躲避奖励

- **原始写法问题**：
  - 一旦发现敌机导弹立即逃跑 → 严重限制智能体自主探索
  - 导弹可能距离远、威胁小，此时过早跑不利于攻击策略
- **改进思路**：
  - 仅在导弹距离撞击剩余时间 < N 秒时才奖励逃避
  - 允许智能体自主权衡攻击与防御策略
- **原则总结**：
  - 只在环境安全边界临近时干预
  - 给最大探索空间，同时避免策略坍塌

## 接近敌机奖励

- 仅在 **没有导弹威胁且距离足够远** 时提供奖励
- 其他靠近敌机的决策交给智能体自主学习
- 避免过度限制智能体行为

## 总结奖励设计原则

1. **必要时奖励**：只在关键事件或环境可能退化时出手
2. **弱 shaping**：
   - 仅作为 early learning 辅助
   - 强度极小，时间窗口短
   - 收敛后可撤掉
3. **稀疏奖励为主**：
   - 最终任务奖励是策略目标
   - 避免奖励函数绑死策略
4. **过程奖励可逐渐退化**：
   - 训练初期加速收敛
   - 训练中期逐步降低
   - 收敛后完全依赖稀疏奖励

> 深刻直觉经验 4：
>  “稀疏奖励是目标，弱 shaping 是训练辅助，奖励函数设计永远不应替代策略判断。”

# 训练策略与课程式学习

- **Curriculum Schedule**：
  - 从简单任务开始，逐步过渡到复杂场景
  - 例：1v1 空战 → 2v2 → 3v3
- **好处**：
  - 帮助智能体稳定探索初始策略空间
  - 避免初期复杂对抗导致探索失败
- **与奖励设计结合**：
  - 早期可增加弱 shaping
  - 随课程升级逐渐降低 shaping 权重
  - 最终只保留稀疏奖励

> 深刻直觉经验 5：
>  “从简单到复杂，奖励辅助逐渐退化，让策略主导行为。”

# 总结经验价值

- 奖励设计是一种**策略引导**，而非策略替代
- 过多或过强的奖励会**限制智能体自主探索**
- 弱 shaping 是早期训练工具，不是目标
- 稀疏奖励是最终目标，强化因果信号
- 课程式训练可以显著提升收敛速度和策略稳定性

> 核心理念总结：
>
> 1. “只在环境退化、策略坍塌的地方出手，其余交给学习。”
> 2. “弱 shaping 是训练辅助，不是目标。”
> 3. “稀疏奖励为目标，奖励函数永远不能绑死策略。”
> 4. “课程式训练 + 渐退 shaping = 稳定收敛与自主策略探索兼顾。”

# 参考资料



## 大模型

* [奖励函数设计](https://chatgpt.com/c/6938f2c6-d1f4-8325-a0e9-168c20318205)

